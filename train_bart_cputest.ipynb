{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-empty",
   "metadata": {},
   "source": [
    "## My train using Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-singapore",
   "metadata": {},
   "source": [
    "Training BART with a changed script from th GPT2 project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-replication",
   "metadata": {},
   "source": [
    "### !! PROBLEM: standard BART doesn't allow position_ids. Should we rewrite the code??\n",
    "Hell yeah we should..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elegant-apparel",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:46.737540Z",
     "iopub.status.busy": "2021-10-06T23:07:46.736857Z",
     "iopub.status.idle": "2021-10-06T23:07:46.739654Z",
     "shell.execute_reply": "2021-10-06T23:07:46.740179Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# #!pip uninstall transformers\n",
    "# !python -m venv .env\n",
    "# !source .env/bin/activate\n",
    "#!pip install transformers==4.5.1\n",
    "# !pip show transformers\n",
    "\n",
    "#!pip install torch==1.8.1\n",
    "# !pip install fairscale\n",
    "#!pip install --user --upgrade papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "found-abuse",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:46.743055Z",
     "iopub.status.busy": "2021-10-06T23:07:46.742436Z",
     "iopub.status.idle": "2021-10-06T23:07:46.744732Z",
     "shell.execute_reply": "2021-10-06T23:07:46.745210Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip show transformers\n",
    "#!python -m pip list --user\n",
    "# !pip show deepspeed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "centered-subscription",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:46.747895Z",
     "iopub.status.busy": "2021-10-06T23:07:46.747279Z",
     "iopub.status.idle": "2021-10-06T23:07:53.596818Z",
     "shell.execute_reply": "2021-10-06T23:07:53.596057Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from transformers import BartForConditionalGeneration, GPT2Tokenizer, PreTrainedTokenizerBase, DataCollatorForLanguageModeling\n",
    "from transformers import BartConfig, AdamW, get_linear_schedule_with_warmup, TrainingArguments, Trainer\n",
    "from transformers.file_utils import PaddingStrategy, logging\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tnrange, tqdm\n",
    "\n",
    "from dataset import SpectroDataset, SpectroDataCollator\n",
    "sys.path.append('data')\n",
    "from data_preprocess1 import print_args\n",
    "# from utils import add_special_tokens, generate_sample, sample_seq, set_seed, top_k_top_p_filtering, print_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-cooking",
   "metadata": {},
   "source": [
    "#### Setting basic training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decreased-money",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:54.418555Z",
     "iopub.status.busy": "2021-10-06T23:07:54.417700Z",
     "iopub.status.idle": "2021-10-06T23:07:54.575791Z",
     "shell.execute_reply": "2021-10-06T23:07:54.575082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/auto/brno6/home/ahajek/Spektro/MassGenie\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solar-athletics",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:54.583315Z",
     "iopub.status.busy": "2021-10-06T23:07:54.582456Z",
     "iopub.status.idle": "2021-10-06T23:07:54.748691Z",
     "shell.execute_reply": "2021-10-06T23:07:54.749203Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "bs = 4 #4\n",
    "gas = 16 #16\n",
    "data_type = \"1M\"\n",
    "SEQ_LEN = 200\n",
    "num_epochs = 8 # int(os.environ[\"TOTAL_EPOCHS\"]) #21 + 8 = BP\n",
    "resume_training = False # bool(int(os.environ[\"RESUME_TRAINING\"]))\n",
    "# resume_wandb_id = pass # \"\"\n",
    "\n",
    "# find the last checkpoint\n",
    "\n",
    "# models_pth = \"/storage/projects/msml/mg_neims_branch/models/bart_trial\"\n",
    "# runs = glob.glob(models_pth)\n",
    "# checkpoints =  glob.glob(runs[-1]+\"/checkpoint-*\")\n",
    "# checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "# load_checkpoint = checkpoints[-1]\n",
    "# print(f\"last checkpoint: {load_checkpoint}\")\n",
    "\n",
    "#load_checkpoint = \"../models/1GPU_run_jup2021-05-22-15_20_57/checkpoint-309504/\"\n",
    "#load_checkpoint = \"/storage/brno6/home/ahajek/Summarization/models_ext/noconotr3in3_epoch19_init\" # initial checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-maker",
   "metadata": {},
   "source": [
    "#### Setting all training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "upper-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PBS_NGPUS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adjacent-combination",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:54.770335Z",
     "iopub.status.busy": "2021-10-06T23:07:54.769438Z",
     "iopub.status.idle": "2021-10-06T23:07:54.790933Z",
     "shell.execute_reply": "2021-10-06T23:07:54.790013Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:\n",
      "  lr ........................... 5e-05\n",
      "  seed ......................... 42\n",
      "  gradient_accumulation_steps .. 16\n",
      "  batch_size ................... 4\n",
      "  warmup ....................... 500\n",
      "  weight_decay ................. 0.01\n",
      "  n_gpu ........................ 0\n",
      "  fairscale .................... \n",
      "  deepspeed .................... None\n",
      "  num_workers .................. 3\n",
      "  device ....................... cuda\n",
      "  num_train_epochs ............. 8\n",
      "  output_dir ................... /storage/projects/msml/mg_neims_branch/MassGenie/output\n",
      "  save_dir ..................... /storage/projects/msml/mg_neims_branch/MassGenie/models\n",
      "  save_name .................... bart_2022-03-30-18_33_49\n",
      "  load_checkpoint .............. \n",
      "  config_dir ................... /storage/projects/msml/mg_neims_branch/MassGenie/configs\n",
      "  fp16 ......................... True\n",
      "  max_grad_norm ................ 1.0\n",
      "  train_data_path .............. /storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/1M_bart_prepared_data_train.pkl\n",
      "  valid_data_path .............. /storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/1M_bart_prepared_data_valid.pkl\n",
      "  log_steps .................... 50\n",
      "  eval_steps ................... 7142\n",
      "  tokenizer_path ............... /storage/brno6/home/ahajek/nic\n",
      "  model_path ................... /storage/projects/msml/mg_neims_branch/MassGenie/models/NECO\n",
      "  wandb ........................ True\n",
      "  wandb_resume ................. False\n",
      "  wandb_id ..................... 3auxfgkd\n",
      "  tensorboard .................. False\n"
     ]
    }
   ],
   "source": [
    "now = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "now = now.replace(\":\",\"_\").replace(\" \", \"-\")\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--gradient-accumulation-steps\",default=gas, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch-size\",default=bs, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--warmup\",default=500, type=int,  help=\"warmup steps for learning rate\")\n",
    "parser.add_argument(\"--weight-decay\",default=0.01, type=float,  help=\"weight decay rate parameter\")\n",
    "parser.add_argument(\"--n-gpu\",default=os.environ[\"PBS_NGPUS\"], type=int, required=False, help=\"no of gpu available\")\n",
    "parser.add_argument(\"--fairscale\",default=\"\", type=str, required=False, choices=[\"simple\", \"zero_dp_2\", \"zero_dp_3\"], help=\"GPU paralellization via Fairscale, \" +\n",
    "                    \"more info in HuggingFace's Trainer docs\")\n",
    "parser.add_argument(\"--deepspeed\", default=None, type=str, required=False, help=\"GPU paralellization via Deepspeed, the value is the location of DeepSpeed json config file; \" +\n",
    "                    \"more info in HuggingFace's Trainer docs\")\n",
    "parser.add_argument(\"--num-workers\",default=os.environ[\"PBS_NCPUS\"], type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--num-train-epochs\",default=num_epochs, type=int,  help=\"number of training epochs\")\n",
    "parser.add_argument(\"--output-dir\",default='/storage/projects/msml/mg_neims_branch/MassGenie/output', type=str,  help=\"Path to save evaluation results\")\n",
    "parser.add_argument(\"--save-dir\",default='/storage/projects/msml/mg_neims_branch/MassGenie/models', type=str,  help=\"Path to save trained model\")\n",
    "parser.add_argument(\"--save-name\", type=str, default=f'bart_{now}', help=\"Name of the model, used for saves\")\n",
    "parser.add_argument(\"--load-checkpoint\", type=str, default='', help=\"Path to the checkpoint to resume training\")\n",
    "parser.add_argument(\"--config-dir\",default='/storage/projects/msml/mg_neims_branch/MassGenie/configs', type=str,  help=\"Path to save config files of models\")\n",
    "parser.add_argument(\"--fp16\",default=True, type=bool, required=False, help=\"whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument(\"--max-grad-norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--train-data-path\",default=f'/storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/{data_type}_bart_prepared_data_train.pkl', type=str, help=\"Path to jsonl train dataset\")\n",
    "parser.add_argument(\"--valid-data-path\",default=f'/storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/{data_type}_bart_prepared_data_valid.pkl', type=str, help=\"Path to jsonl validation dataset\")\n",
    "parser.add_argument(\"--log-steps\",default=50, type=int,  help=\"number of steps between logs\")\n",
    "parser.add_argument(\"--eval-steps\",default=7142, type=int,  help=\"number of steps between evaluations\")\n",
    "parser.add_argument(\"--tokenizer-path\",default='/storage/brno6/home/ahajek/nic', type=str, help=\"location of the desied tokenizer (special sep token will be added))\")\n",
    "parser.add_argument(\"--model-path\",default='/storage/projects/msml/mg_neims_branch/MassGenie/models/NECO', type=str, help=\"location of the desired model to finetune\")\n",
    "parser.add_argument(\"--wandb\", action='store_true', default=True, help=\"optinal logging via Weights&Biases\")\n",
    "parser.add_argument(\"--wandb-resume\", action='store_true', default=resume_training, help=\"resume logging via wandb, needs an valid run ID set in args.wandb-id\")\n",
    "parser.add_argument(\"--wandb-id\", type=str, default=wandb.util.generate_id(), help=\"Process unique wandb ID used for resumin the training process\")\n",
    "parser.add_argument(\"--tensorboard\", action='store_true', default=False, help=\"optinal logging via TensorBoard\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "arg_log = print_args(args)\n",
    "\n",
    "# extended outputs\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-invention",
   "metadata": {},
   "source": [
    "#### Loading data, tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extra-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART CONIGURATION\n",
    "# chytre vocab size, ale nemame tokenizer: # 500+len(tokenizer.get_ids()),\n",
    "# dal jsem vocab_size na 600, pro jistotu (581 by melo stacit)\n",
    "config = BartConfig(vocab_size = 600,\n",
    "                             max_position_embeddings = SEQ_LEN,\n",
    "                             encoder_layers = 12,\n",
    "                             encoder_ffn_dim = 4096,\n",
    "                             encoder_attention_heads = 16,\n",
    "                             decoder_layers = 12,\n",
    "                             decoder_ffn_dim = 4096,\n",
    "                             decoder_attention_heads = 16,\n",
    "                             encoder_layerdrop = 0.0,\n",
    "                             decoder_layerdrop = 0.0,\n",
    "                             activation_function = 'gelu',\n",
    "                             d_model = 1024,\n",
    "                             dropout = 0.2,\n",
    "                             attention_dropout = 0.0,\n",
    "                             activation_dropout = 0.0,\n",
    "                             init_std = 0.02,\n",
    "                             classifier_dropout = 0.0,\n",
    "                             scale_embedding = False,\n",
    "                             use_cache = True,\n",
    "                             num_labels = 3,\n",
    "                             pad_token_id = 501,\n",
    "                             bos_token_id = 503,\n",
    "                             eos_token_id = 502,\n",
    "                             is_encoder_decoder = True,\n",
    "                             decoder_start_token_id = 502,\n",
    "                             forced_eos_token_id = 502,\n",
    "                             static_position_embeddings=True\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continuing-latest",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:07:54.795676Z",
     "iopub.status.busy": "2021-10-06T23:07:54.795243Z",
     "iopub.status.idle": "2021-10-06T23:08:20.068457Z",
     "shell.execute_reply": "2021-10-06T23:08:20.067778Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kratkej vypis :D\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "train_data = SpectroDataset(args.train_data_path)\n",
    "valid_data = SpectroDataset(args.valid_data_path)\n",
    "\n",
    "# clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# TOKENIZER\n",
    "# tokenizer = add_special_tokens(args.tokenizer_path)\n",
    "\n",
    "# MODEL\n",
    "model = BartForConditionalGeneration(config)\n",
    "# model = BartForConditionalGeneration.from_pretrained(args.model_path)\n",
    "\n",
    "# model.to(args.device)\n",
    "model.to(\"cpu\")\n",
    "print(\"kratkej vypis :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collective-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 1M dataset\n",
    "train_data.data.rename(columns={\"lm_labels\":\"labels\"}, inplace=True)\n",
    "valid_data.data.rename(columns={\"lm_labels\":\"labels\"}, inplace=True)\n",
    "train_data.data[\"position_ids\"] = range(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "romantic-million",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destereo_smiles</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>decoder_input_ids</th>\n",
       "      <th>encoder_attention_mask</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>position_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>365667</th>\n",
       "      <td>CCN(C(=O)CNC1CN(CC1c1cncn1C)C(=O)C(=O)N)C1CC1</td>\n",
       "      <td>[30, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 4...</td>\n",
       "      <td>[500, 531, 531, 541, 506, 531, 506, 510, 542, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 531, 531, 541, 506, 531, 506, 510, 542,...</td>\n",
       "      <td>[4, 1, 6, 5, 8, 9, 8, 9, 7, 6, 5, 3, 2, 2, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392866</th>\n",
       "      <td>O=C(CN1CCNC(=O)C1)NCCC1CN(CC1C)Cc1nncn1C</td>\n",
       "      <td>[33, 36, 39, 40, 41, 42, 43, 44, 46, 51, 52, 5...</td>\n",
       "      <td>[500, 542, 510, 531, 506, 531, 541, 520, 531, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 542, 510, 531, 506, 531, 541, 520, 531,...</td>\n",
       "      <td>[1, 4, 1, 2, 8, 9, 6, 8, -9223372036854775799,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243721</th>\n",
       "      <td>COCn1nccc1S(=O)(=O)N1CCCC1C(=O)NCC(=O)OC</td>\n",
       "      <td>[33, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 5...</td>\n",
       "      <td>[500, 531, 542, 531, 565, 520, 565, 555, 555, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 531, 542, 531, 565, 520, 565, 555, 555,...</td>\n",
       "      <td>[6, 6, 3, 9, 8, 8, 8, 9, 7, 5, 6, 6, 8, 8, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102237</th>\n",
       "      <td>CCn1nncc1C(=O)N1CCOC2C1CN(C2)C(=O)c1nnn(n1)C</td>\n",
       "      <td>[28, 29, 33, 34, 36, 37, 38, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[500, 531, 531, 565, 520, 565, 565, 555, 555, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 531, 531, 565, 520, 565, 565, 555, 555,...</td>\n",
       "      <td>[1, -9223372036854775799, 5, 2, 6, 5, 5, 7, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247067</th>\n",
       "      <td>CCn1ccc(n1)C(=O)NC1CN(CC1c1ncn(c1)C)C(=O)C(=O)N</td>\n",
       "      <td>[33, 36, 38, 39, 40, 41, 42, 43, 44, 45, 49, 5...</td>\n",
       "      <td>[500, 531, 531, 565, 520, 555, 555, 555, 506, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 531, 531, 565, 520, 555, 555, 555, 506,...</td>\n",
       "      <td>[1, 1, 3, 7, 6, 8, 8, 6, 8, 4, 2, 4, 5, 6, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92540</th>\n",
       "      <td>OC1CN(CC1NC(=O)C(c1cnc[nH]1)(C)C)C(=O)c1c[nH]c...</td>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 45, 46, 51, 52, 53, 5...</td>\n",
       "      <td>[500, 542, 531, 520, 531, 541, 506, 531, 531, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 542, 531, 520, 531, 541, 506, 531, 531,...</td>\n",
       "      <td>[5, 4, 9, 8, 8, 8, 5, -9223372036854775799, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238810</th>\n",
       "      <td>OC(CN(C(=O)Cn1ccc(cc1=O)C)C)CNC(=O)C1CC1C(=O)N</td>\n",
       "      <td>[30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
       "      <td>[500, 542, 531, 506, 531, 541, 506, 531, 506, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 542, 531, 506, 531, 541, 506, 531, 506,...</td>\n",
       "      <td>[5, 2, 1, 2, 8, 6, 9, 8, 8, 9, 7, 5, 5, 7, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311793</th>\n",
       "      <td>O=C(Nc1ccc(c(c1)S(=O)(=O)N)C)CNS(=O)(=O)N(C)C</td>\n",
       "      <td>[16, 27, 28, 30, 31, 33, 34, 36, 39, 40, 41, 4...</td>\n",
       "      <td>[500, 542, 510, 531, 506, 541, 555, 520, 555, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 542, 510, 531, 506, 541, 555, 520, 555,...</td>\n",
       "      <td>[1, 3, 2, 6, 3, 2, 1, 3, 7, 5, 6, 8, 6, 9, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131110</th>\n",
       "      <td>NCC(C(=O)NC1CC2CCC(C1)N2C(=O)c1cc(=O)n2c(n1)nn...</td>\n",
       "      <td>[30, 33, 39, 40, 41, 42, 43, 44, 45, 46, 49, 5...</td>\n",
       "      <td>[500, 541, 531, 531, 506, 531, 506, 510, 542, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 541, 531, 531, 506, 531, 506, 510, 542,...</td>\n",
       "      <td>[4, -9223372036854775799, 7, 5, 9, 8, 8, 8, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475096</th>\n",
       "      <td>CNC(=O)NCCC(=O)N1CC(C(C1)NC(=O)c1ccn(c(=O)c1)C)O</td>\n",
       "      <td>[39, 40, 41, 42, 43, 44, 45, 51, 52, 53, 54, 5...</td>\n",
       "      <td>[500, 531, 541, 531, 506, 510, 542, 507, 541, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 531, 541, 531, 506, 510, 542, 507, 541,...</td>\n",
       "      <td>[5, 1, 8, 9, 7, 9, 6, 3, 3, 4, 7, 8, 9, 9, 8, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354766 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          destereo_smiles  \\\n",
       "365667      CCN(C(=O)CNC1CN(CC1c1cncn1C)C(=O)C(=O)N)C1CC1   \n",
       "392866           O=C(CN1CCNC(=O)C1)NCCC1CN(CC1C)Cc1nncn1C   \n",
       "243721           COCn1nccc1S(=O)(=O)N1CCCC1C(=O)NCC(=O)OC   \n",
       "102237       CCn1nncc1C(=O)N1CCOC2C1CN(C2)C(=O)c1nnn(n1)C   \n",
       "247067    CCn1ccc(n1)C(=O)NC1CN(CC1c1ncn(c1)C)C(=O)C(=O)N   \n",
       "...                                                   ...   \n",
       "92540   OC1CN(CC1NC(=O)C(c1cnc[nH]1)(C)C)C(=O)c1c[nH]c...   \n",
       "238810     OC(CN(C(=O)Cn1ccc(cc1=O)C)C)CNC(=O)C1CC1C(=O)N   \n",
       "311793      O=C(Nc1ccc(c(c1)S(=O)(=O)N)C)CNS(=O)(=O)N(C)C   \n",
       "131110  NCC(C(=O)NC1CC2CCC(C1)N2C(=O)c1cc(=O)n2c(n1)nn...   \n",
       "475096   CNC(=O)NCCC(=O)N1CC(C(C1)NC(=O)c1ccn(c(=O)c1)C)O   \n",
       "\n",
       "                                                input_ids  \\\n",
       "365667  [30, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 4...   \n",
       "392866  [33, 36, 39, 40, 41, 42, 43, 44, 46, 51, 52, 5...   \n",
       "243721  [33, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 5...   \n",
       "102237  [28, 29, 33, 34, 36, 37, 38, 39, 40, 41, 42, 4...   \n",
       "247067  [33, 36, 38, 39, 40, 41, 42, 43, 44, 45, 49, 5...   \n",
       "...                                                   ...   \n",
       "92540   [39, 40, 41, 42, 43, 44, 45, 46, 51, 52, 53, 5...   \n",
       "238810  [30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
       "311793  [16, 27, 28, 30, 31, 33, 34, 36, 39, 40, 41, 4...   \n",
       "131110  [30, 33, 39, 40, 41, 42, 43, 44, 45, 46, 49, 5...   \n",
       "475096  [39, 40, 41, 42, 43, 44, 45, 51, 52, 53, 54, 5...   \n",
       "\n",
       "                                        decoder_input_ids  \\\n",
       "365667  [500, 531, 531, 541, 506, 531, 506, 510, 542, ...   \n",
       "392866  [500, 542, 510, 531, 506, 531, 541, 520, 531, ...   \n",
       "243721  [500, 531, 542, 531, 565, 520, 565, 555, 555, ...   \n",
       "102237  [500, 531, 531, 565, 520, 565, 565, 555, 555, ...   \n",
       "247067  [500, 531, 531, 565, 520, 555, 555, 555, 506, ...   \n",
       "...                                                   ...   \n",
       "92540   [500, 542, 531, 520, 531, 541, 506, 531, 531, ...   \n",
       "238810  [500, 542, 531, 506, 531, 541, 506, 531, 506, ...   \n",
       "311793  [500, 542, 510, 531, 506, 541, 555, 520, 555, ...   \n",
       "131110  [500, 541, 531, 531, 506, 531, 506, 510, 542, ...   \n",
       "475096  [500, 531, 541, 531, 506, 510, 542, 507, 541, ...   \n",
       "\n",
       "                                   encoder_attention_mask  \\\n",
       "365667  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392866  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "243721  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "102237  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "247067  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "92540   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "238810  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "311793  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "131110  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "475096  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                   decoder_attention_mask  \\\n",
       "365667  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "392866  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "243721  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "102237  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "247067  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "92540   [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "238810  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "311793  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "131110  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "475096  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  \\\n",
       "365667  [-100, 531, 531, 541, 506, 531, 506, 510, 542,...   \n",
       "392866  [-100, 542, 510, 531, 506, 531, 541, 520, 531,...   \n",
       "243721  [-100, 531, 542, 531, 565, 520, 565, 555, 555,...   \n",
       "102237  [-100, 531, 531, 565, 520, 565, 565, 555, 555,...   \n",
       "247067  [-100, 531, 531, 565, 520, 555, 555, 555, 506,...   \n",
       "...                                                   ...   \n",
       "92540   [-100, 542, 531, 520, 531, 541, 506, 531, 531,...   \n",
       "238810  [-100, 542, 531, 506, 531, 541, 506, 531, 506,...   \n",
       "311793  [-100, 542, 510, 531, 506, 541, 555, 520, 555,...   \n",
       "131110  [-100, 541, 531, 531, 506, 531, 506, 510, 542,...   \n",
       "475096  [-100, 531, 541, 531, 506, 510, 542, 507, 541,...   \n",
       "\n",
       "                                             position_ids  \n",
       "365667  [4, 1, 6, 5, 8, 9, 8, 9, 7, 6, 5, 3, 2, 2, 5, ...  \n",
       "392866  [1, 4, 1, 2, 8, 9, 6, 8, -9223372036854775799,...  \n",
       "243721  [6, 6, 3, 9, 8, 8, 8, 9, 7, 5, 6, 6, 8, 8, 9, ...  \n",
       "102237  [1, -9223372036854775799, 5, 2, 6, 5, 5, 7, 7,...  \n",
       "247067  [1, 1, 3, 7, 6, 8, 8, 6, 8, 4, 2, 4, 5, 6, 7, ...  \n",
       "...                                                   ...  \n",
       "92540   [5, 4, 9, 8, 8, 8, 5, -9223372036854775799, 4,...  \n",
       "238810  [5, 2, 1, 2, 8, 6, 9, 8, 8, 9, 7, 5, 5, 7, 8, ...  \n",
       "311793  [1, 3, 2, 6, 3, 2, 1, 3, 7, 5, 6, 8, 6, 9, 7, ...  \n",
       "131110  [4, -9223372036854775799, 7, 5, 9, 8, 8, 8, 5,...  \n",
       "475096  [5, 1, 8, 9, 7, 9, 6, 3, 3, 4, 7, 8, 9, 9, 8, ...  \n",
       "\n",
       "[354766 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.input_ids])\n",
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.decoder_input_ids])\n",
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.encoder_attention_mask])\n",
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.decoder_attention_mask])\n",
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.labels])\n",
    "assert 0 == sum([len(x) != SEQ_LEN for x in train_data.data.position_ids])\n",
    "train_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indian-harbor",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:08:20.072079Z",
     "iopub.status.busy": "2021-10-06T23:08:20.071355Z",
     "iopub.status.idle": "2021-10-06T23:08:20.073798Z",
     "shell.execute_reply": "2021-10-06T23:08:20.073140Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 353746944\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-screen",
   "metadata": {},
   "source": [
    "#### Setting run resuming and WandB logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "environmental-simulation",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:08:20.081323Z",
     "iopub.status.busy": "2021-10-06T23:08:20.080892Z",
     "iopub.status.idle": "2021-10-06T23:08:25.559006Z",
     "shell.execute_reply": "2021-10-06T23:08:25.559590Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhajekad\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "env: WANDB_PROJECT=BART_for_gcms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhajekad\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">upbeat-wave-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hajekad/BART_for_gcms\" target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hajekad/BART_for_gcms/runs/2tir5267\" target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms/runs/2tir5267</a><br/>\n",
       "                Run data is saved locally in <code>/mnt/storage-brno6/home/ahajek/Spektro/MassGenie/wandb/run-20220330_102430-2tir5267</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resume training\n",
    "if resume_training:\n",
    "    args.load_checkpoint = load_checkpoint\n",
    "    if args.wandb_resume:\n",
    "        args.wandb_id = resume_wandb_id\n",
    "\n",
    "# Init wandb\n",
    "if args.wandb:\n",
    "    ! wandb login\n",
    "    %env WANDB_PROJECT = BART_for_gcms\n",
    "    wandb.init(id=args.wandb_id, resume=\"allow\")\n",
    "    wandb.run.name = args.save_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-brass",
   "metadata": {},
   "source": [
    "#### Setting training arguments (according to args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "restricted-tactics",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:08:25.568722Z",
     "iopub.status.busy": "2021-10-06T23:08:25.568091Z",
     "iopub.status.idle": "2021-10-06T23:08:26.169291Z",
     "shell.execute_reply": "2021-10-06T23:08:26.169886Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=args.save_dir+\"/\"+args.save_name,                            # output directory\n",
    "    num_train_epochs=args.num_train_epochs,              # total # of training epochs\n",
    "    per_device_train_batch_size=args.batch_size,         # batch size per device during training\n",
    "    per_device_eval_batch_size=args.batch_size,          # batch size for evaluation\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    warmup_steps=args.warmup,                                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=args.weight_decay,                                   # strength of weight decay\n",
    "    logging_dir=args.save_dir + './logs',                # directory for storing logs\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=args.save_name,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "#     eval_steps=args.eval_steps,\n",
    "    logging_steps=2, # args.log_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "#     save_steps=1000,\n",
    "    fp16=args.fp16,\n",
    "    dataloader_drop_last=True,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=args.num_workers,\n",
    "    sharded_ddp=args.fairscale,\n",
    "    deepspeed=args.deepspeed \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,            # training dataset\n",
    "    eval_dataset=valid_data,             # evaluation dataset\n",
    "    data_collator = SpectroDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-mortality",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-perfume",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-06T23:08:26.173100Z",
     "iopub.status.busy": "2021-10-06T23:08:26.172359Z",
     "iopub.status.idle": "2021-10-07T19:20:14.222372Z",
     "shell.execute_reply": "2021-10-07T19:20:14.223148Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/brno6/home/ahajek/.local-Pytorch-21.SIF/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 354766\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 44344\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:752: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:787: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5784' max='44344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5784/44344 2:33:39 < 17:04:46, 0.63 it/s, Epoch 1.04/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 44346\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /storage/projects/msml/mg_neims_branch/MassGenie/models/bart_2022-03-30-10_22_16/checkpoint-5543\n",
      "Configuration saved in /storage/projects/msml/mg_neims_branch/MassGenie/models/bart_2022-03-30-10_22_16/checkpoint-5543/config.json\n",
      "Model weights saved in /storage/projects/msml/mg_neims_branch/MassGenie/models/bart_2022-03-30-10_22_16/checkpoint-5543/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# log args to \"args\" file in args.save_dir (right before the training starts, to be the latest)\n",
    "Path(f'{args.save_dir}/{args.save_name}').mkdir(exist_ok=True)\n",
    "with open(f'{args.save_dir}/{args.save_name}/args', 'w+') as output_file:\n",
    "    output_file.write(arg_log)\n",
    "\n",
    "if args.load_checkpoint:\n",
    "    trainer.train(args.load_checkpoint)\n",
    "else:\n",
    "    #trainer.evaluate()\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-conversation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
