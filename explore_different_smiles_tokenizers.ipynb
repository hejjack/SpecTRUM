{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29ccd0b",
   "metadata": {},
   "source": [
    "# Explore alternative tokenizers\n",
    "Exploring possibilities of different NLP tokenizers for the BART model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb6efbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa6bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"./data/trial_set/1K.smi\", delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c1f548f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)[C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)[C..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c14a2",
   "metadata": {},
   "source": [
    "## WordPiece tokenizer\n",
    "This tokenizer from the deepchem library looks to be better for anorganic molecules, has a lot of structures that are not that common in our SMILES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a11b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'O', 'c', '1', 'c', 'c', '2', 'c', 'c', 'c', '(', '=', 'O', ')', 'o', 'c', '2', 'c', '(', 'O', '[C@@H]', '2', 'O', '[C@@H]', '(', 'C', 'O', ')', '[C@H]', '(', 'O', ')', '[C@@H]', '(', 'O', ')', '[C@@H]', '2', 'O', ')', 'c', '1', 'O', '1', '6', '2', '1', '5', '1', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "import os\n",
    "\n",
    "vocab_path = 'wp_tokenizer/vocab.txt'\n",
    "tokenizer = SmilesTokenizer(vocab_path)\n",
    "print(tokenizer.tokenize(df.iloc[90][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29cf9884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage-brno6/home/ahajek/Spektro/MassGenie\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff876da",
   "metadata": {},
   "source": [
    "## BBPE Tokenizer\n",
    "Hopefully will learn to tokenize the SMILES into longer squences.\n",
    "\n",
    "Let's train one.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e1d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SMILES file without the zinc bullshit\n",
    "with open(\"./tokenizer/training_data/1K.txt\", \"w+\") as f:\n",
    "    for smiles in df.smiles:\n",
    "        f.write(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37eb2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self, vocab_size=50257, min_frequency=10):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, path):\n",
    "        trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, show_progress=True, initial_alphabet=ByteLevel.alphabet(), \n",
    "                             special_tokens=[\"<eos>\",\n",
    "                                             \"<ukn>\",\n",
    "                                             \"<pad>\",\n",
    "                                             \"<bos>\",\n",
    "                                             \"<neims>\",\n",
    "                                             \"<nist>\",\n",
    "                                             \"<rassp>\",\n",
    "                                             \"<trafo>\",\n",
    "                                             \"<source1>\",\n",
    "                                             \"<source2>\",\n",
    "                                             \"<source3>\",])\n",
    "        self.tokenizer.train(path, trainer)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470782a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf500\n",
      "- max vocab_size 50257\n",
      "- min_frequency 500\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf1000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 1000\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf2000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 2000\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf3000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 3000\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf4000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 4000\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf5000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 5000\n",
      "- final vocab size 801 (including 11 special tokens)\n",
      "\n",
      "\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf6000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 6000\n",
      "- final vocab size 801 (including 11 special tokens)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "path = [\"tokenizer/training_data/1M.txt\"]\n",
    "min_frequency = 300\n",
    "\n",
    "for min_frequency in [500, 1000, 2000, 3000, 4000, 5000, 6000]:\n",
    "    tokenizer = BPE_token(min_frequency=min_frequency)\n",
    "\n",
    "    # train the tokenizer model\n",
    "    tokenizer.bpe_train(path)\n",
    "\n",
    "    # saving the tokenized data in our specified folder\n",
    "    save_path = 'tokenizer/bbpe_tokenizer/' ####\n",
    "    tokenizer.save_tokenizer(save_path)\n",
    "    tokenizer.tokenizer.save(save_path + f\"/bart_bbpe_tokenizer_1M_mf{min_frequency}.model\")    #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d807955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_bbpe_tokenizer_1M_mf500\n",
      "- max vocab_size 50257\n",
      "- min_frequency 500\n",
      "- final vocab size 506 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf1000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 1000\n",
      "- final vocab size 454 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf2000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 2000\n",
      "- final vocab size 422 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf3000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 3000\n",
      "- final vocab size 401 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf4000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 4000\n",
      "- final vocab size 390 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf5000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 5000\n",
      "- final vocab size 376 (including 11 special tokens)\n",
      "bart_bbpe_tokenizer_1M_mf6000\n",
      "- max vocab_size 50257\n",
      "- min_frequency 6000\n",
      "- final vocab size 365 (including 11 special tokens)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "save_path = 'tokenizer/bbpe_tokenizer/' ####\n",
    "for min_frequency in [500, 1000, 2000, 3000, 4000, 5000, 6000]:\n",
    "    # loading the saved tokenizer\n",
    "    tokenizer = Tokenizer.from_file(save_path + f\"/bart_bbpe_tokenizer_1M_mf{min_frequency}.model\")\n",
    "\n",
    "    print(f\"bart_bbpe_tokenizer_1M_mf{min_frequency}\\n\" +\n",
    "          \"- max vocab_size 50257\\n\" + \n",
    "          f\"- min_frequency {min_frequency}\\n\" +\n",
    "          f\"- final vocab size {len(tokenizer.get_vocab())} (including 11 special tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e29bb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2378752e",
   "metadata": {},
   "source": [
    "### Try the BBPE out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0e31df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# vocab = \"./tokenizer/bbpe_tokenizer/vocab.json\"\n",
    "# merges = \"./tokenizer/bbpe_tokenizer//merges.txt\"\n",
    "tok = \"./tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model\"\n",
    "tokenizer = Tokenizer.from_file(tok)\n",
    "# special_tokens_dict = {\"bos_token\": \"<bos>\", \"unk_token\": \"<unk>\", \"eos_token\": \"<eos>\", \"sep_token\": \"<sep>\"}\n",
    "# special_tokens_dict = [\"<sep>\"]\n",
    "\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# And then encode:\n",
    "encoded = tokenizer.encode(\"Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ac54d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġ', 'CS', '(=', 'O', ')(=', 'O', ')', 'NCC', '(=', 'O', ')', 'N', '[', 'C', '@', 'H', ']', '1', 'COCC', '[', 'C', '@@', 'H', ']', '1', 'Oc', '1', 'ccc', '(', 'C', '(', 'N', ')=', 'O', ')', 'cc', '1']\n"
     ]
    }
   ],
   "source": [
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4276e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62814d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = tokenizer.token_to_id(\"<bos>\")\n",
    "et = tokenizer.token_to_id(\"<eos>\")\n",
    "tok_smiles = [bt] + tokenizer.encode(df.smiles[0]).ids #+ [et] + (200-2-len(tokenizer.encode(df.smiles[0]))) * [pt]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dcd4fb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)[C@H]2C1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[224,\n",
       " 278,\n",
       " 20,\n",
       " 282,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 70,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 70,\n",
       " 20,\n",
       " 54,\n",
       " 260,\n",
       " 50,\n",
       " 270,\n",
       " 50,\n",
       " 12,\n",
       " 49,\n",
       " 20,\n",
       " 267,\n",
       " 21,\n",
       " 38,\n",
       " 260,\n",
       " 50,\n",
       " 12,\n",
       " 266,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 38,\n",
       " 260,\n",
       " 50,\n",
       " 263,\n",
       " 38,\n",
       " 35,\n",
       " 43,\n",
       " 64,\n",
       " 21,\n",
       " 38,\n",
       " 20]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.smiles[0])\n",
    "tokenizer.encode(df.smiles[0]).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0147c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destereo_smiles</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>decoder_input_ids</th>\n",
       "      <th>encoder_attention_mask</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>position_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...</td>\n",
       "      <td>[15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...</td>\n",
       "      <td>[3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1</td>\n",
       "      <td>[17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...</td>\n",
       "      <td>[1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...</td>\n",
       "      <td>[30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
       "      <td>[3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...</td>\n",
       "      <td>[2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C</td>\n",
       "      <td>[33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...</td>\n",
       "      <td>[2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C</td>\n",
       "      <td>[33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...</td>\n",
       "      <td>[4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>OCCOCCNC(=O)C1CC(C(C1)O)NC(=O)c1nsnc1C</td>\n",
       "      <td>[19, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...</td>\n",
       "      <td>[3, 4, 3, 3, 5, 3, 4, 7, 6, 9, 9, 9, 9, 9, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>COc1ccc(cc1C(=O)NC1COC2C1OCC2O)S(=O)(=O)N</td>\n",
       "      <td>[26, 31, 33, 36, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...</td>\n",
       "      <td>[0, 7, 2, 4, 1, 7, 4, 8, 8, 9, 8, 8, 7, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>O=C(Cn1ncn(c1=O)C)OCCCOC(=O)Cn1ncn(c1=O)C</td>\n",
       "      <td>[29, 30, 31, 39, 40, 41, 42, 43, 44, 45, 52, 5...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...</td>\n",
       "      <td>[3, 1, 3, 4, 6, 4, 8, 5, 6, 0, 2, 6, 6, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>OC(=O)CN(C(=O)C)CC1OCCN(C1)C(=O)CN1CSCC1=O</td>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 4...</td>\n",
       "      <td>[3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...</td>\n",
       "      <td>[3, 0, 0, 3, 2, 1, 4, 1, 6, 6, 8, 9, 9, 9, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>O=C(N1CCN(C2C1CS(=O)(=O)C2)S(=O)(=O)C)Cc1cccnc1</td>\n",
       "      <td>[27, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...</td>\n",
       "      <td>[4, 3, 1, 1, 3, 8, 6, 8, 8, 7, 5, 1, 4, 6, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       destereo_smiles  \\\n",
       "629  COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...   \n",
       "338       O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1   \n",
       "620  CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...   \n",
       "396   Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C   \n",
       "251            O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C   \n",
       "..                                                 ...   \n",
       "308             OCCOCCNC(=O)C1CC(C(C1)O)NC(=O)c1nsnc1C   \n",
       "674          COc1ccc(cc1C(=O)NC1COC2C1OCC2O)S(=O)(=O)N   \n",
       "631          O=C(Cn1ncn(c1=O)C)OCCCOC(=O)Cn1ncn(c1=O)C   \n",
       "526         OC(=O)CN(C(=O)C)CC1OCCN(C1)C(=O)CN1CSCC1=O   \n",
       "121    O=C(N1CCN(C2C1CS(=O)(=O)C2)S(=O)(=O)C)Cc1cccnc1   \n",
       "\n",
       "                                             input_ids  \\\n",
       "629  [15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...   \n",
       "338  [17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   \n",
       "620  [30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
       "396  [33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "251  [33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...   \n",
       "..                                                 ...   \n",
       "308  [19, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 4...   \n",
       "674  [26, 31, 33, 36, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "631  [29, 30, 31, 39, 40, 41, 42, 43, 44, 45, 52, 5...   \n",
       "526  [30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 4...   \n",
       "121  [27, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "\n",
       "                                     decoder_input_ids  \\\n",
       "629  [3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...   \n",
       "338  [3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...   \n",
       "620  [3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...   \n",
       "396  [3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...   \n",
       "251  [3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...   \n",
       "..                                                 ...   \n",
       "308  [3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...   \n",
       "674  [3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...   \n",
       "631  [3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...   \n",
       "526  [3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...   \n",
       "121  [3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...   \n",
       "\n",
       "                                encoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "308  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "674  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "631  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "526  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                decoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "308  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "674  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "631  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "526  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \\\n",
       "629  [3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...   \n",
       "338  [3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...   \n",
       "620  [3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...   \n",
       "396  [3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...   \n",
       "251  [3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...   \n",
       "..                                                 ...   \n",
       "308  [3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...   \n",
       "674  [3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...   \n",
       "631  [3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...   \n",
       "526  [3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...   \n",
       "121  [3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...   \n",
       "\n",
       "                                          position_ids  \n",
       "629  [3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...  \n",
       "338  [1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...  \n",
       "620  [2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...  \n",
       "396  [2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...  \n",
       "251  [4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...  \n",
       "..                                                 ...  \n",
       "308  [3, 4, 3, 3, 5, 3, 4, 7, 6, 9, 9, 9, 9, 9, 7, ...  \n",
       "674  [0, 7, 2, 4, 1, 7, 4, 8, 8, 9, 8, 8, 7, 7, 6, ...  \n",
       "631  [3, 1, 3, 4, 6, 4, 8, 5, 6, 0, 2, 6, 6, 8, 8, ...  \n",
       "526  [3, 0, 0, 3, 2, 1, 4, 1, 6, 6, 8, 9, 9, 9, 8, ...  \n",
       "121  [4, 3, 1, 1, 3, 8, 6, 8, 8, 7, 5, 1, 4, 6, 2, ...  \n",
       "\n",
       "[469 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"data/trial_set/1K_bbpe_bart_prepared_data_train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc81557",
   "metadata": {},
   "source": [
    "### Inspect the tokenizers a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d22750ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "def get_num_tokens(tokenizer):\n",
    "    return len(tokenizer.get_vocab())\n",
    "\n",
    "def get_mean_vocab_len(tokenizer):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    mean_len = sum(map(lambda x: len(x), vocab.keys())) / len(vocab)\n",
    "    return mean_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24869e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3 = Tokenizer.from_file(\"tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model\")\n",
    "tokenizer30 = Tokenizer.from_file(\"tokenizer/bbpe_tokenizer/bart_bbpe_tokenizer_1M_mf30.model\")\n",
    "tokenizer3000 = Tokenizer.from_file(\"tokenizer/bbpe_tokenizer/bart_bbpe_tokenizer_1M_mf3000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffaf22ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240 4.661290322580645\n",
      "756 3.427248677248677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(401, 1.940149625935162)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_num_tokens(tokenizer3), get_mean_vocab_len(tokenizer3))\n",
    "print(get_num_tokens(tokenizer30), get_mean_vocab_len(tokenizer30))\n",
    "get_num_tokens(tokenizer3000), get_mean_vocab_len(tokenizer3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7ff62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
