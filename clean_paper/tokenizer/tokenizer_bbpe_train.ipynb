{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff876da",
   "metadata": {},
   "source": [
    "# BBPE SMILES Tokenizer\n",
    "The goal of this notebook is to prepare training data and train a suitable SMILES tokenizer. We get inspired by the generative NLP approaches and try to train a BBPE tokenizer. The main advantage of this tokenization technique is, that since the training starts with a vocabulary with all on-byte ASCII char, we can be sure to tokenize any SMILES string without encountering an unknown token (for compatibility reasons we add it anyway though).\n",
    "\n",
    "## Data preparation\n",
    "As training data we take a 1M random slice from the 30M dataset we scraped from ZINC20 database. The SMILES are already deduplicated and canonical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fd224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data_path = \"../data/zinc/30M/30M.smi\"\n",
    "slice_save_dir = \"training_data\"\n",
    "Path(slice_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def random_slice(size, data_path, slice_save_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = np.array(f.read().splitlines())\n",
    "        choice = np.random.choice(data, size, replace=False)\n",
    "\n",
    "    with open(slice_save_path, 'w') as f:\n",
    "        for item in choice:\n",
    "            f.write(item + \" \")\n",
    "    \n",
    "random_slice(1000000, data_path, slice_save_dir + \"/1M.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d6698",
   "metadata": {},
   "source": [
    "# BBPE tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37eb2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self, vocab_size=100000, min_frequency=10):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()   # normalization of unicode characters (technicality)\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, path):\n",
    "        trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, show_progress=True, initial_alphabet=ByteLevel.alphabet(), \n",
    "                             special_tokens=[\"<eos>\",\n",
    "                                             \"<unk>\",\n",
    "                                             \"<pad>\",\n",
    "                                             \"<bos>\",\n",
    "                                             \"<neims>\",\n",
    "                                             \"<nist>\",\n",
    "                                             \"<rassp>\",\n",
    "                                             \"<trafo>\",\n",
    "                                             \"<source1>\",\n",
    "                                             \"<source2>\",\n",
    "                                             \"<source3>\",])\n",
    "        self.tokenizer.train(path, trainer)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "470782a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "train_data_path = slice_save_dir + \"/1M.txt\"\n",
    "mfs = [10, 100, 10000, 10000000]\n",
    "\n",
    "for min_frequency in mfs:\n",
    "    tokenizer = BPE_token(min_frequency=min_frequency)\n",
    "\n",
    "    # train the tokenizer model\n",
    "    tokenizer.bpe_train([train_data_path])\n",
    "\n",
    "    # saving the tokenized data in our specified folder\n",
    "    save_path = '' ####\n",
    "    tokenizer.save_tokenizer(save_path)\n",
    "    tokenizer.tokenizer.save(save_path + f\"/test_tokenizer_mf{min_frequency}.model\")    #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c34d6",
   "metadata": {},
   "source": [
    "## Check the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d807955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_1M_mf10\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10\n",
      "- final vocab size 1286 (including 11 special tokens)\n",
      "\n",
      "tokenizer_1M_mf100\n",
      "- max vocab_size 100000\n",
      "- min_frequency 100\n",
      "- final vocab size 780 (including 11 special tokens)\n",
      "\n",
      "tokenizer_1M_mf10K\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10K\n",
      "- final vocab size 367 (including 11 special tokens)\n",
      "\n",
      "tokenizer_1M_mf10M\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10M\n",
      "- final vocab size 267 (including 11 special tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "save_path = ''\n",
    "all_mfs = ['10', '100', '10K', '10M']\n",
    "\n",
    "for min_frequency in all_mfs:\n",
    "    # loading the saved tokenizer\n",
    "    tokenizer = Tokenizer.from_file(save_path + f\"tokenizer_mf{min_frequency}.model\")\n",
    "\n",
    "    print(f\"tokenizer_1M_mf{min_frequency}\\n\" +\n",
    "          \"- max vocab_size 100000\\n\" + \n",
    "          f\"- min_frequency {min_frequency}\\n\" +\n",
    "          f\"- final vocab size {len(tokenizer.get_vocab())} (including 11 special tokens)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378752e",
   "metadata": {},
   "source": [
    "## Try the BBPE out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0e31df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä Cc', '1', 'nn', '(', 'C', ')', 'c', '(', 'C', ')', 'c', '1', 'S', '(=', 'O', ')(=', 'O', ')', 'N', '1', 'CCN', '2', 'C', '(=', 'O', ')', 'CN', '(', 'C', ')', 'C', '(=', 'O', ')']\n",
      "[318, 27, 289, 18, 45, 19, 77, 18, 45, 19, 77, 27, 61, 269, 57, 294, 57, 19, 56, 27, 279, 28, 45, 269, 57, 19, 280, 18, 45, 19, 45, 269, 57, 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# vocab = \"./tokenizer/bbpe_tokenizer/vocab.json\"\n",
    "# merges = \"./tokenizer/bbpe_tokenizer//merges.txt\"\n",
    "tok = \"tokenizer_mf10.model\"\n",
    "tokenizer = Tokenizer.from_file(tok)\n",
    "# special_tokens_dict = {\"bos_token\": \"<bos>\", \"unk_token\": \"<unk>\", \"eos_token\": \"<eos>\", \"sep_token\": \"<sep>\"}\n",
    "# special_tokens_dict = [\"<sep>\"]\n",
    "\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# And then encode:\n",
    "encoded = tokenizer.encode(\"Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)\")\n",
    "\n",
    "print(encoded.tokens), print(encoded.ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
