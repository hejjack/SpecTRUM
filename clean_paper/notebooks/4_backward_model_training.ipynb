{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward model training\n",
    "\n",
    "In this notebook we take the prepared experimental (NIST) and synthetic (NEIMS-gen, RASSP-gen) datasets and train our final backward model. \n",
    "\n",
    "The model is an adapted encoder-decoder transformer architecture that takes m/z values and intensities as an input and predicts the SMILES string of a molecule as an output. The architecture is based on Huggingface implementation of the [BART model](https://huggingface.co/docs/transformers/model_doc/bart) and can be found in `{PROJECT_ROOT}/bart_spektro/modeling_bart_spektro.py`.\n",
    "\n",
    "The training phase is divided into two parts:\n",
    "1. Pretraining on the synthetic dataset (NEIMS-gen, RASSP-gen)\n",
    "2. Fine-tuning on the experimental dataset (NIST)\n",
    "\n",
    "In order to train the best model possible, we implemented several experiments seeking for the best architecture and hyperparameters. First we show how to train the final model, and then we show how to reproduce the experiments. \n",
    "\n",
    "All the various configurations of the runs are stored in the `{PROJECT_ROOT}/configs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model training\n",
    "\n",
    "TODO: \n",
    "- edit data paths in configs!!\n",
    "\n",
    "The final model's configuration can be found in `{PROJECT_ROOT}/configs/pretrain_final.yaml` and `{PROJECT_ROOT}/configs/finetune_final.yaml`. The run commands are as follows:\n",
    "\n",
    "### Pretraining\n",
    "\n",
    "```bash\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_final.yaml \\\n",
    "                     --additional-info _final \\\n",
    "                     --additional-tags scratch:rassp1:neims1:final \\\n",
    "                     --wandb-group pretrain\n",
    "```\n",
    "\n",
    "### Finetuning\n",
    "```bash\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_final.yaml \\\n",
    "                     --checkpoint checkpoints/pretrain/{CHECKPOINT_NAME}/checkpoint-224000 \\\n",
    "                     --additional-info final \\\n",
    "                     --additional-tags final \\\n",
    "                     --wandb-group finetune \n",
    "```\n",
    "\n",
    "NOTES:\n",
    "\n",
    "The training process is logged to [Weights & Biases](https://wandb.ai/home). We encourage you to create a profile to track the training. When using `wandb` you can also resume a failed run easily.  \n",
    "\n",
    "The final model is saved in the `{PROJECT_ROOT}/checkpoints/{wandb-group}/{wandb-run-id}` folder. More information on the `train_bart.py` script arguments can be found by running `python train_bart.py --help`.\n",
    "\n",
    "If you have more GPUs available, specify the list of the chosen IDs in CUDA_VISIBLE_DEVICES env variable right before the `python` call at the beginning of the line. \n",
    "\n",
    "If your GPU memory is smaller than 40GB, you might need to adjust the batch size in the config files: set the auto_bs to False and specify the batch size manually (setting values for `per_device_train_batch_size`, `per_device_eval_batch_size`, `gradient_accumulation_steps`, and `eval_accumulation_steps` in `hf_training_args`). Beware that the batch size is an important hyperparameter and can significantly affect the moodel's performance.\n",
    "\n",
    "Training on CPU (debug mode) is currently not supported due to dependency issues.\n",
    "\n",
    "Since the training process is quite long (60h + 24h on H100 GPU), we recommend running it in a `tmux` session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Positional embeddings\n",
    "In this experiment we tested whether we can effectively input two channels of information (m/z and intensities) in the form of two sets of trained embeddings that are added together. A model given only m/z values and static positional encoding is compared to a model where trained binned intensity embeddings replace positional encoding.\n",
    "\n",
    "To save time and resources, for this experiment we conducted only the finetuning step.\n",
    "\n",
    "#### Training (Finetuning)\n",
    "```bash\n",
    "# m/z + static positional encoding\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp1_pos_emb.yaml \\\n",
    "                     --additional-info _exp1_pos_emb \\\n",
    "                     --additional-tags exp1:pos_emb:from_scratch \\\n",
    "                     --wandb-group finetune \n",
    "\n",
    "# m/z + intensity embeddings\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp1_int_emb.yaml \\\n",
    "                     --additional-info exp1_int_emb \\\n",
    "                     --additional-tags exp1:int_emb:from_scratch \\\n",
    "                     --wandb-group finetune \n",
    "```\n",
    "\n",
    "In this experiment we showed that the model can effectively learn to use the intensity embeddings as a positional encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Intensity binning\n",
    "This experiment  aims to validate our intuition regarding logarithmic intensity binning and try to find an optimal combination of the number of bins *s + 1* and the logarithm base *b*.\n",
    "\n",
    "We investigated two variants of linear binning, rounding the intensities to two and three decimal places, resulting in 100 and 1000 trainable bins, respectively. Additionally, we experiment with four different variants of logarithmic binning parameters (s, b), specifically (9, 2.2), (20, 1.43), (29, 1.28), and (39, 1.2). The arbitrarily looking pairs of numbers are hand-crafted to create a uniform-like distribution without unnecessary empty bins, on the NIST train dataset.\n",
    "\n",
    "Again, we only conducted the finetuning step for this experiment.\n",
    "\n",
    "#### Training (Finetuning)\n",
    "```bash\n",
    "# linear binning 100 bins\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_lin_100.yaml \\\n",
    "                     --additional-info _exp2_lin_100 \\\n",
    "                     --additional-tags exp2:lin_100:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# linear binning 1000 bins\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_lin_1000.yaml \\\n",
    "                     --additional-info _exp2_lin_1000 \\\n",
    "                     --additional-tags exp2:lin_1000:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# log binning 10 bins, base 2.2\n",
    "# !!! This is the same run as [exp1: m/z + intensity embeddings]\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_log_9_2.2.yaml \\\n",
    "                     --additional-info _exp2_log_9_2.2 \\\n",
    "                     --additional-tags exp2:log_9_2.2:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# log binning 21 bins, base 1.43\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_log_20_1.43.yaml \\\n",
    "                     --additional-info _exp2_log_20_1.43 \\\n",
    "                     --additional-tags exp2:log_20_1.43:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# log binning 30 bins, base 1.28\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_log_29_1.28.yaml \\\n",
    "                     --additional-info _exp2_log_29_1.28 \\\n",
    "                     --additional-tags exp2:log_29_1.28:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# log binning 40 bins, base 1.2\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp2_log_39_1.2.yaml \\\n",
    "                     --additional-info _exp2_log_39_1.2 \\\n",
    "                     --additional-tags exp2:log_39_1.2:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "```\n",
    "\n",
    "In this experiment we showed that the logarithmic binning can be used as an effective intensity encoding. There was an significant improvement for models using more than 10 bins, but the differences for 30 vs. 40 bins were negligible in our runs. Linear binning wit 1000 bins performed reasonably well, lin_100 did not. The best results were achieved with 30 bins and log base 1.28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Molecular representation & Tokenization\n",
    "Another decision to make was choosing the optimal encoding for the model’s decoder subcomponent. We  initially questioned whether the BBPE tokenization of SMILES was a suitable approach and, if so, what\n",
    "properties the tokenizer should have. Additionally, we questioned the choice of SMILES as a molecular representation and compared the results with the recently proposed SELFIES, a representation designed\n",
    "specifically for generative neural networks.\n",
    "\n",
    "We decided to test four values of `min_frequency` parameter – 10, 100, 10 000, and 10 million, resulting in vocabulary sizes of 1286, 780, 367 and 267 tokens, respectively. The last tokenizer (10M) splits strings simply on the level of characters, the others also use longer tokens. Besides, we included a SELFIES tokenizer that we created by wrapping the Hugging face PreTrainedTokenizer class around the [SELFIES tokenization algorithm](https://pypi.org/project/selfies/).\n",
    "\n",
    "Also here, we only conducted the finetuning step.\n",
    "\n",
    "#### Training (Finetuning)\n",
    "```bash\n",
    "# BBPE with minimal token frequency of 10\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf10.yaml \\\n",
    "                     --additional-info _exp3_mf10 \\\n",
    "                     --additional-tags exp3:mf10:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# BBPE with minimal token frequency of 100\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf100.yaml \\\n",
    "                     --additional-info _exp3_mf100 \\\n",
    "                     --additional-tags exp3:mf100:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# BBPE with minimal token frequency of 10K\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf10K.yaml \\\n",
    "                     --additional-info _exp3_mf10K \\\n",
    "                     --additional-tags exp3:mf10K:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# BBPE with minimal token frequency of 10M\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf10M.yaml \\\n",
    "                     --additional-info _exp3_mf10M \\\n",
    "                     --additional-tags exp3:mf10M:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# SELFIES tokenizer\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_selfies.yaml \\\n",
    "                     --additional-info _exp3_selfies \\\n",
    "                     --additional-tags exp3:selfies:from_scratch \\\n",
    "                     --wandb-group finetune\n",
    "```\n",
    "\n",
    "In this experiment we showed that the BBPE tokenization with a minimal token frequency of 10M (character-level tokenization) is the best choice for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Pretraining datasets mixing\n",
    "In this experiment, we tested whether pretraining on synthetic data positively affects the model’s results. We also tried to find a good mixture of pretraining datasets to optimize the performance.\n",
    "\n",
    "With the use of specific source tokens at the beinning of every SMILES string determining the origin dataset of each spectrum (<rassp>, <neims>, <rassp>) we give our model a chance to adapt to the differences between the datasets. We tested the following pretraining combinations of datasets:\n",
    "\n",
    "- (no pretraining - baseline trained in the previous experiment - exp3_mf10M) \n",
    "- RASSP-gen only\n",
    "- NEIMS-gen only\n",
    "- RASSP-gen + NEIMS-gen (mixing 1:1)\n",
    "- RASSP-gen + NEIMS-gen + NIST (mixing 1:1:0.1)\n",
    "\n",
    "All the pretrained models were finetuned on the NIST dataset and the performance was measured on the NIST validation set.\n",
    "\n",
    "#### Pretraining\n",
    "```bash\n",
    "# RASSP-gen only\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_exp4_rassp.yaml \\\n",
    "                     --additional-info _exp4_rassp \\\n",
    "                     --additional-tags exp4:rassp:from_scratch \\\n",
    "                     --wandb-group pretrain\n",
    "\n",
    "# NEIMS-gen only\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_exp4_neims.yaml \\\n",
    "                     --additional-info _exp4_neims \\\n",
    "                     --additional-tags exp4:neims:from_scratch \\\n",
    "                     --wandb-group pretrain\n",
    "\n",
    "# RASSP-gen + NEIMS-gen\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_exp4_rassp_neims.yaml \\\n",
    "                     --additional-info _exp4_rassp_neims \\\n",
    "                     --additional-tags exp4:rassp:neims:from_scratch \\\n",
    "                     --wandb-group pretrain\n",
    "\n",
    "# RASSP-gen + NEIMS-gen + NIST\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_exp4_rassp_neims_nist.yaml \\\n",
    "                     --additional-info _exp4_rassp_neims_nist \\\n",
    "                     --additional-tags exp4:rassp:neims:nist:from_scratch \\\n",
    "                     --wandb-group pretrain\n",
    "```\n",
    "\n",
    "#### Finetuning\n",
    "The finetuning step is the same as in the previous experiment - just replace the `--checkpoint` argument with the path to the pretrained model.\n",
    "\n",
    "```bash\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf10M.yaml \\\n",
    "                     --checkpoint checkpoints/pretrain/{CHECKPOINT_NAME}/checkpoint-112000 \\ # TODO: add correct path\n",
    "                     --additional-info _exp4_{MIXTURE_NAME} \\   # TODO: add correct name\n",
    "                     --additional-tags exp4:from_pretrained \\   # TODO: add correct tags\n",
    "                     --wandb-group finetune\n",
    "```\n",
    "\n",
    "In this experiment we showed that the model benefits from pretraining on synthetic data. The best results were achieved with the mixture of RASSP-gen and NEIMS-gen datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Source token vindication\n",
    "In this experiment, we tested the effect of the source tokens on the model's performance. We compared the model trained with three different source tokens to the model trained with the same source token for all datasets.\n",
    "\n",
    "#### Pretraining\n",
    "```bash\n",
    "# one source token for all datasets (we chose <nist> for convenience)\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/pretrain_exp5_one_src_token.yaml \\\n",
    "                     --additional-info _exp5_one_src_token \\\n",
    "                     --additional-tags exp5:one_src_token:from_scratch \\\n",
    "                     --wandb-group pretrain\n",
    "```\n",
    "\n",
    "#### Finetuning\n",
    "The finetuning step is the same as in the previous experiment - just replace the `--checkpoint` argument with the path to the pretrained model.\n",
    "\n",
    "```bash\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp3_mf10M.yaml \\\n",
    "                     --checkpoint checkpoints/pretrain/{CHECKPOINT_NAME}/checkpoint-112000 \\ # TODO: add correct path\n",
    "                     --additional-info _exp5_one_src_token \\ \n",
    "                     --additional-tags exp5:from_pretrained:one_src_token \\\n",
    "                     --wandb-group finetune\n",
    "```\n",
    "\n",
    "In this experiment we showed that the model does not benefit from the source tokens. The results were comparable to the model trained with the same source token for all datasets, thus we suggest that the model learned to ignore the source tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Layer freezing\n",
    "In this next experiment, we investigated whether we could speed up the finetuning process or even enhance the model’s performance using layer freezing. Layer freezing is commonly used as a regularization method that, given a considerable level of overfitting seen on model's loss values, might be benefitial. We tested the following layer freezing strategies:\n",
    "\n",
    "- no freezing (here we used the best model from the previous experiments - unfrozen RASSP:NIST model finetuned on the NIST dataset)\n",
    "- train *fc1* and *decoder embedding* layers (72% frozen)\n",
    "- train *fc1*, *cross-attention*, *decoder self-attention* and *decoder embedding* layers (45% frozen)\n",
    "\n",
    "#### Finetuning\n",
    "```bash\n",
    "# train fc1 and decoder embedding layers\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp6_72perc_forzen.yaml \\\n",
    "                     --checkpoint checkpoints/pretrain/{NEIMS_RASSP_CHECKPOINT_NAME}/checkpoint-112000 \\ # TODO: add correct path\n",
    "                     --additional-info _exp6_72perc_forzen \\\n",
    "                     --additional-tags exp6:72perc_forzen:from_pretrained \\\n",
    "                     --wandb-group finetune\n",
    "\n",
    "# train fc1, cross-attention, decoder self-attention and decoder embedding layers\n",
    "cd {PROJECT_ROOT}\n",
    "CUDA_VISIBLE_DEVICES=0 python train_bart.py --config-file configs/finetune_exp6_45perc_forzen.yaml \\\n",
    "                     --checkpoint checkpoints/pretrain/{NEIMS_RASSP_CHECKPOINT_NAME}/checkpoint-112000 \\ # TODO: add correct path\n",
    "                     --additional-info _exp6_45perc_forzen \\\n",
    "                     --additional-tags exp6:45perc_forzen:from_pretrained \\\n",
    "                     --wandb-group finetune\n",
    "```\n",
    "\n",
    "In this experiment we showed that the model does not benefit from layer freezing. The results were significantly worse than the model trained without freezing. The more layers were frozen, the worse the results were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Training length\n",
    "We identified the best combination of hyperparameters from those we tested. Since the training didn’t seem to fully converge yet, we doubled the number of epochs for both the pretraining and finetuning phases.\n",
    "\n",
    "This experiment results in the final models, the training of which is described at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
