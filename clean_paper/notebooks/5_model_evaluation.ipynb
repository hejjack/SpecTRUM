{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "This notebook is a simple example of how to evaluate our model on our tet split of the NIST20 dataset.\n",
    "\n",
    "We will show you how to perform **Simple evaluation**, **Evaluation in comparison with DB search** and **Visualization of predictions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple evaluation\n",
    "### Generate predictions\n",
    "\n",
    "To generate predictions we need to specify a configuration file and a path to the trained model. The configuration file for NIST dataset is provided in the `config` directory as [predict_nist.yaml](../configs/predict_nist.yaml). \n",
    "\n",
    "The script [predict.py](../predict.py) is used to generate predictions. It takes the prepared `jsonl` file and outputs a `jsonl` file where each line is a `json` with keys being the generated SMILES strings and their values are candidates' probabilities generated by our model. We use these probabilities to sort candidates according to \"the model's view\".\n",
    "\n",
    "Statistics and all metadata about the run are stored in `log_file.yaml` in the same directory as the predictions.\n",
    "\n",
    "Running `predict.py` could look like this:\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0 python ../predict.py --checkpoint ../checkpoints/finetune/fearless-wildflower-490_rassp1_neims1_224kPretrain_148k/checkpoint-147476 \\\n",
    "                                            --output-folder predictions \\\n",
    "                                            --config-file configs/predict_nist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictions\n",
    "Evaluation of predictions is done with the [evaluate_predictions.py](../evaluate_predictions.py) script. It takes a path to the predictions file, a path to the ground truth file and a path to a [config file](../configs/evaluate_nist.yaml) as input and appends all the evaluation metrics to the corresponding `log_file.yaml`. The script also generates several plots and saves them in the same directory as the predictions.\n",
    "\n",
    "Running `evaluate_predictions.py` could look like this:\n",
    "\n",
    "```bash\n",
    "python evaluate_predictions.py --predictions-path {path-to-predictions.jsonl} \\\n",
    "                               --labels-path data/nist/train.jsonl \\\n",
    "                               --config-file configs/evaluate_nist.yaml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation in comparison with database search\n",
    "To find out how is the model doing against baseline you can also run the evluation in comparison with a standard database search in the de novo scenario. As a reference library we use the NIST train set with 232 025 experimentally measured spectra, and query library is the NIST test set. \n",
    "\n",
    "For this scenario to work we need to precompute the highest cosine similarity of the query spectra within the reference library and their respective SMILES (fingerprint) similarity. This fingerprint similarity is then compared to the similarity of our model's candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute similarity index for NIST test\n",
    "\n",
    "```bash\n",
    "SPLIT_NAME=test\n",
    "FP_TYPE=morgan\n",
    "SIMIL_FUN=tanimoto\n",
    "\n",
    "python precompute_db_index.py \\\n",
    "           --reference data/nist/train.jsonl \\\n",
    "           --query data/nist/test.jsonl \\\n",
    "           --outfile data/nist/test_with_db_index.jsonl \\\n",
    "           --num_processes 32 \\\n",
    "           --fingerprint_type ${FP_TYPE} \\\n",
    "           --fp_simil_function ${SIMIL_FUN}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate including database search comparison\n",
    "For this you have to set `do_db_search` in [evaluate_nist.yaml](../configs/evaluate_nist.yaml) to `True` and change the labels to the ones enriched with db index.\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python evaluate_predictions.py --predictions-path {path-to-predicitons.jsonl} \\\n",
    "                               --labels-path data/nist/test_with_db_index.jsonl \\\n",
    "                               --config-file configs/evaluate_nist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on the satistics' interpretation\n",
    "The statistics are stored in the `log_file.yaml` in the same directory as the predictions. Let's see what they mean:\n",
    "\n",
    "`similsort` - predictions are sorted by the fingerprint similarity of the candidates to the query spectrum. It shows the upper bound on the model's performance when generating multiple candidates. This scenario can also mimic the situation with a domain expert that can pick the correct candiddate every time. \n",
    "\n",
    "`probsort` - predictions are sorted by the model's probabilities. It shows the model's performance when generating multiple candidates without any other information on their quality.\n",
    "\n",
    "\n",
    "### Example\n",
    "```yaml\n",
    "evaluation_0:\n",
    "    average_num_of_predictions: '9.52'            # mean number of UNIQUE VALID candidates generated per \n",
    "                                                  # query\n",
    "    db_search:\n",
    "        mean_db_score: '0.22'                     # mean fingerprint similarity of the best db candidate\n",
    "        mean_fpsd_score_probsort: '0.05'          # mean difference between the best candidates of db and\n",
    "                                                  # the model (probsort)\n",
    "        mean_fpsd_score_similsort: '0.14'         # mean difference between the best candidates of db and\n",
    "                                                  # the model (similsort)\n",
    "        percentage_of_BART_wins_probsort: '0.63'  # percentage of queries where the model's best \n",
    "                                                  # candidate is better than the best db candidate \n",
    "                                                  # (probsort)\n",
    "        percentage_of_BART_wins_similsort: '0.84' # percentage of queries where the model's best \n",
    "                                                  # candidate is better than the best db candidate \n",
    "                                                  # (similsort)\n",
    "        percentage_of_ties_probsort: '0.14'       # percentage of queries where the model's best \n",
    "                                                  # candidate is equal to the best db candidate (probsort)\n",
    "        percentage_of_ties_similsort: '0.06'      # percentage of queries where the model's best \n",
    "                                                  # candidate is equal to the best db candidate (similsort)\n",
    "        ties:                                     # detailed statistics on ties\n",
    "            mean_tie_simils_probsort: '0.38'\n",
    "            mean_tie_simils_similsort: '0.60'\n",
    "            num_of_ties_probsort: '35'\n",
    "            num_of_ties_simils_equal_to_1_probsort: '4'  # number of ties with similarity equal to 1 (probsort)\n",
    "            num_of_ties_simils_equal_to_1_similsort: '6' # number of ties with similarity equal to 1 (similsort)\n",
    "            num_of_ties_similsort: '16'\n",
    "            percentage_of_ties_simils_equal_to_1_probsort: '0.11'\n",
    "            percentage_of_ties_simils_equal_to_1_similsort: '0.37'\n",
    "    eval_config:                                 # log of configuration used for evaluation\n",
    "        do_db_search: true\n",
    "        filtering_args:\n",
    "            max_mol_repr_len: 100\n",
    "            max_mz: 500\n",
    "            max_num_peaks: 300\n",
    "            mol_repr: smiles\n",
    "        fingerprint_type: morgan\n",
    "        on_the_fly: true\n",
    "        save_best_predictions: true\n",
    "        fp_simil_function: tanimoto\n",
    "        threshold: 0.85\n",
    "    eval_time: 00:00:02\n",
    "    formula_stats:                               # statistics on molecular formulas      \n",
    "        num_all_correct_formulas: 154 / 2324        # from all generated candidates\n",
    "        num_at_least_one_correct_formula: '38'      # at least one correct formula was generated for the query\n",
    "        num_correct_formulas_at_best_prob: '24'     # correct formula was returned as the best candidate (probsort)\n",
    "        num_correct_formulas_at_best_simil: '29'    # correct formula was returned as the best candidate (similsort)\n",
    "        percentage_of_all_correct_formulas: '0.06'        \n",
    "        percentage_of_at_least_one_correct_formula: '0.15'\n",
    "        percentage_of_correct_formulas_at_best_prob: '0.09'\n",
    "        percentage_of_correct_formulas_at_best_simil: '0.11'\n",
    "    hit_at_k_prob: '[(1, 0.02), (2, 0.03), (3, 0.04),\n",
    "        (4, 0.06), (5, 0.08)]'                      # percentage of correct formulas returned in top k \n",
    "                                                    # candidates (probsort) \n",
    "                                                    # note: this metric for similsort would not make sense\n",
    "    labels_path: data/mace/MACE_r05_with_db_index.jsonl\n",
    "    num_datapoints_tested: '244'\n",
    "    num_empty_preds: '0'                            # number of queries with no valid candidates\n",
    "    num_predictions_at_k_counter: '[244, 244, 243, 243, 242, 242, 241, 235, 222, 168]'\n",
    "    precise_preds_stats:      # statistics on precise returned best candidates (exactly the same canonical SMILES)\n",
    "        num_precise_preds_probsort: '4'\n",
    "        num_precise_preds_similsort: '20'\n",
    "        percentage_of_precise_preds_probsort: '0.01'\n",
    "        percentage_of_precise_preds_similsort: '0.08'\n",
    "    simil_1_hits:                              # statistics on returned best candidates with similarity 1\n",
    "        counter_multiple_hits: dict_items([(3, 2), (1, 13), (2, 5)]) # monitors situations where more than one \n",
    "                                                                     # candidate has prob equal to 1. This can \n",
    "                                                                     # happen bcs of imperfection of fp similarity. \n",
    "                                                                     # Format: (num of hits, num of occurences)\n",
    "        num_1_hits_as_first_probsort: '5'\n",
    "        num_1_hits_as_first_similsort: '20'\n",
    "        num_fp_simil_fail_prob: '1'                                  # number of queries where the best candidate \n",
    "                                                                     # has similarity 1 but is not the correct one\n",
    "        num_fp_simil_fail_simil: '0'\n",
    "        percentage_of_1_hits_as_first_probsort: '0.02'\n",
    "        percentage_of_1_hits_as_first_similsort: '0.08'\n",
    "    start_time_utc: 01/09/2024 11:19:01\n",
    "    threshold_stats:            # statistics using a threshold (not backed by any theoretical\n",
    "                                # or empirical reasoning, just to have a clue about \"relatively good\" candidates)\n",
    "        num_better_than_threshold_probsort: '7'\n",
    "        num_better_than_threshold_similsort: '22'\n",
    "        percentage_of_better_than_threshold_probsort: '0.02'\n",
    "        percentage_of_better_than_threshold_similsort: '0.09'\n",
    "        threshold: '0.85'\n",
    "    topk_probsort: '[0.28, 0.27, 0.27, 0.26, 0.26, 0.25, 0.24,\n",
    "        0.23, 0.23, 0.22]'          # mean similarities on k-th position (probsort) \n",
    "    topk_similsort: '[0.37, 0.32, 0.29, 0.27, 0.25, 0.23, 0.22,\n",
    "        0.19, 0.18, 0.16]'          # mean similarities on k-th position (similsort) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions\n",
    "To see what the model's predictions look like we prepared a little visualization script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "from utils.eval_utils import load_labels_to_datapipe\n",
    "\n",
    "# load labels and predictions\n",
    "predictions_path = \"path/to/predictions.jsonl\" # CHANGE for the actual path\n",
    "labels_path = \"data/nist/test_with_db_index.jsonl\" \n",
    "\n",
    "\n",
    "labels, _ = load_labels_to_datapipe(Path(labels_path))\n",
    "labels = list(labels)\n",
    "str_predictions = open(predictions_path).readlines()\n",
    "dict_predictions = [json.loads(p) for p in str_predictions]\n",
    "sorted_predictions = [sorted([k for k, _ in sorted(pred.items(), key=lambda x: x[1])]) for pred in dict_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_to_viz = list(zip(labels, sorted_predictions))[142]  # TODO: change this to visualize different pairs\n",
    "best_n_to_viz = 5                                          # TODO: change this to visualize more/less predictions\n",
    "\n",
    "if isinstance(pairs_to_viz, tuple):\n",
    "    pairs_to_viz = [pairs_to_viz]\n",
    "\n",
    "for gt_smiles, preds in pairs_to_viz:\n",
    "    print(\"\\n##################\")\n",
    "    print(\"GT smiles:\", gt_smiles)\n",
    "    gt_mol = Chem.MolFromSmiles(gt_smiles)\n",
    "    display(gt_mol)\n",
    "    for i, pred_smiles in enumerate(preds[:best_n_to_viz]):\n",
    "        pred_mol = Chem.MolFromSmiles(pred_smiles)\n",
    "        print(f\"Prediction {i}: {pred_smiles}, similarity: {DataStructs.FingerprintSimilarity(Chem.RDKFingerprint(gt_mol), Chem.RDKFingerprint(pred_mol))}\")\n",
    "        display(pred_mol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BARTtrainH100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
