{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "In this notebook we describe and demonstrate the process of data acquisition, synthetic spectra generation and dataset composition. Training of our model requires three datasets - experimentally measured commercial NIST GS-EI-MS library (we use NIST20) and two much bigger synthetic datasets generated by open source models NEIMS and RASSP. The NIST dataset can be purchased from distributors listed [here](https://chemdata.nist.gov/dokuwiki/doku.php?id=chemdata:distributors), the synthetic datasets can be either generated with this notebook or downloaded from us (see **Precomputed datasets** below).\n",
    "\n",
    "The only information about each molecule we need for training our model is:\n",
    "    \n",
    "```text\n",
    "- SMILES:              \"CCCCCCCCC(CCCC)O[Si]1(C)CCCCC1\"\n",
    "- list of m/z values:  [26, 27, 28, 31, ..., 200, 201, 255, 256, 257]\n",
    "- list of intensities: [21.98, 335.7, 49.95, 737.34, ..., 23.98, 5.99, 78.93, 20.98, 5.0]\n",
    "```\n",
    "\n",
    "### 1) NIST dataset\n",
    "This notebook first creates training, test and validation splits for NIST and converts them into `.jsonl` files. Each line of these files is a `json` with SMILES, m/z and intensities as keys representing a single molecule.\n",
    "\n",
    "### 2) SMILES acquisition from ZINC\n",
    "Secondly we download a subset of ZINC library (query *2d-standard-annotated-druglike*) that contains SMILES strings but no m/z nor intensities. We further filter the SMILES randomly to 30M, which creates a base for synthetic generation. \n",
    "\n",
    "### 3) RASSP synthetic spectra generation\n",
    "Thirdly, we generate synthetic spectra using RASSP model. This model's molecular restrictions reduce the number of generated spectra to ???4.8M???. These spectra are split into training, validation and test sets.\n",
    "\n",
    "### 4) NEIMS synthetic spectra generation\n",
    "In the fourth step we use NEIMS model to generate synthetic spectra from the ???4.8M??? molecules permitted by RASSP (NEIMS does not have as strict molecular restrictions). These spectra are divided into **the same** training, validation and test splits as RASSP.\n",
    "\n",
    "### 5) Dataleaks elimination\n",
    "Finally we ensure there are no data leaks between NIST valid + test set (which will serve as the primary evaluation sets) and all the training data (RASSP train set, NEIMS train set and synthetic train sets). \n",
    "\n",
    "### Precomputed datasets\n",
    "Synthetic spectra generation in full scale is very computationally intensive and requires a lot of resources. Therefore, we provide the prepared synthetic dataset in the form of a zip file that can be downloaded from the following link: ....\n",
    "\n",
    "If the user chooses to use the precomputed datasets for training, he can perform step 1 with his o, skip the steps 2-4 and proceed directly to the dataleaks elimination step 5. \n",
    "If the user chooses to generate the synthetic datasets himself, he can go through all the steps and choose to process the whole dataset instead of just the toy example presented.\n",
    "\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) NIST dataset\n",
    "\n",
    "### 1.1) NIST cleaning and splitting\n",
    "The core part of this section is altered from a [notebook](https://github.com/Jozefov/mass-spectra-prediction-GCN/blob/master/Notebooks/data_preprocessing.ipynb) created by Filip Jozefov.\n",
    "\n",
    "- in this subbsection we inspect the missing identifiers in NISt dataset\n",
    "- we dorp ~60k of NIST spectra that don't have any form of a proper identifier (smiles, inchikey)\n",
    "- we canonize the smiles strings and remove stereochemistry information\n",
    "- we split the remaining data in the 0.8:0.1:0.1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from matchms.importing import load_from_msp\n",
    "from matchms.exporting import save_as_msp\n",
    "from matchms import Spectrum\n",
    "import matchms\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spectra_process_utils import remove_stereochemistry_and_canonicalize\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/xhajek9/gc-ms_bart/clean_paper' # TODO: change this to the path of your project root\n",
    "NIST_PATH = '../data/datasets/NIST/NIST20/20210925_NIST_EI_MS_cleaned.msp' # TODO: change this to the path of your NIST20 dataset\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_dataset = list(load_from_msp(NIST_PATH, metadata_harmonization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of missing identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count examined occurrences of specific data missing in our dataset\n",
    "def count_all(dataset):\n",
    "    \n",
    "    all_data = 0\n",
    "\n",
    "    no_smiles = 0\n",
    "    no_inchikey = 0\n",
    "    no_inchi = 0\n",
    "\n",
    "    no_smile_only = 0\n",
    "    no_inchikey_only = 0\n",
    "    both_missing_counter = 0\n",
    "\n",
    "    all_identifier_missing = 0\n",
    "\n",
    "    for obj in dataset:\n",
    "        if obj.get('smiles') == None:\n",
    "            no_smiles += 1\n",
    "        if obj.get('inchikey') == None:\n",
    "            no_inchikey += 1\n",
    "        if obj.get('inchi') == None:\n",
    "            no_inchi += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchi') != None:\n",
    "            no_smile_only += 1\n",
    "        if obj.get('smiles') != None and obj.get('inchi') == None:\n",
    "            no_inchikey_only += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchikey') != None:\n",
    "            both_missing_counter += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchikey') == None and obj.get('inchi') == None:\n",
    "            all_identifier_missing += 1\n",
    "        all_data += 1\n",
    "    return (all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\n",
    "            both_missing_counter, all_identifier_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing = count_all(nist_dataset)\n",
    "\n",
    "unique_smiles = set([obj.get('smiles') for obj in nist_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We are currently working with {all_data - no_smiles} smiles, from which {len(unique_smiles)} are unique\\n\")\n",
    "print(f\"We are currently working with {all_data - no_inchikey} inchikeys\\n\")\n",
    "print(f\"We are currently working with {all_data - no_inchi} inchi\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS\n",
    "data_missing = {\n",
    "    'All data': [all_data],\n",
    "    'No smiles': [no_smiles],\n",
    "    'No inchikey': [no_inchikey],     \n",
    "    'No inchi': [no_inchi],\n",
    "    'Smiles Only Missing': [no_smile_only],\n",
    "    'Inchi Only Missing': [no_inchikey_only],\n",
    "    'Both Missing': [both_missing_counter],\n",
    "    'All tree missing': [all_identifier_missing],    \n",
    "}\n",
    "missing_df = pd.DataFrame(data_missing)\n",
    "\n",
    "missing_df = missing_df.T\n",
    "missing_df.columns = [\"Count\"]\n",
    "missing_df[\"average\"] = missing_df.apply(lambda row: row.Count / all_data, axis = 1)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS VISUALIZATION\n",
    "\n",
    "# x-coordinates of left sides of bars \n",
    "parameters_missing = [i for i in range(len(missing_df))]\n",
    "  \n",
    "# heights of bars\n",
    "height = [i for i in missing_df.Count]\n",
    "  \n",
    "# labels for bars\n",
    "tick_label = ['All data', 'No smiles', 'No inchikey', 'No inchi', 'Smiles Only Missing',\n",
    "              'Inchi Only Missing', 'Both Missing', 'All tree missing']\n",
    "  \n",
    "\n",
    "plt.bar(parameters_missing, height, tick_label = tick_label,\n",
    "        width = 0.8)\n",
    "  \n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Number of missing data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifier reconstruction\n",
    "\n",
    "We have approximately 60k data that we are unable to work with. They do not include any identifiers (inchi, inchikey, or SMILES) so we are dropping them. SMILES is crucial for us, so for molecules that don't include a valid SMILES string but include another identifier we will try to reconstruct it. If that fails, we will drop such molecule too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and try to restore corupted smiles\n",
    "# both tools are used, with help of rdkit and matchms as well\n",
    "# e add the smiles destereo and canonization\n",
    "\n",
    "def reconstruct_information(dataset):\n",
    "    updated_dataset = []\n",
    "    for spectrum in tqdm(dataset):\n",
    "        smiles = spectrum.get('smiles')\n",
    "        # all missing\n",
    "        if smiles == None and spectrum.get('inchikey') == None and spectrum.get('inchi') == None:\n",
    "            continue\n",
    "            \n",
    "        #check weather smiles is syntactically valid or molecule is chemically reasonable\n",
    "        if (smiles == None or \\\n",
    "            Chem.MolFromSmiles(smiles, sanitize=False) == None or\\\n",
    "            Chem.MolFromSmiles(smiles) == None) and\\\n",
    "            spectrum.get('inchi') != None:\n",
    "            \n",
    "            # try to convert from inchi\n",
    "            tmp = Chem.inchi.MolFromInchi(spectrum.get('inchi'))\n",
    "            if tmp != None:\n",
    "                spectrum.set('smiles', Chem.MolToSmiles(tmp))\n",
    "                smiles = spectrum.get('smiles')\n",
    "        \n",
    "        # try with matchms\n",
    "        if smiles == None and spectrum.get('inchi') != None:\n",
    "            spectrum = matchms.filtering.derive_smiles_from_inchi(spectrum)\n",
    "            smiles = spectrum.get('smiles')\n",
    "\n",
    "        if smiles == None:\n",
    "            continue\n",
    "            \n",
    "        updated_dataset.append(spectrum)\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = reconstruct_information(nist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In the dataset there remains {len(reconstructed_dataset)} / {len(nist_dataset)} molecules and all have now SMILES strings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stereochemistry and canonicalize smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stereochemistry_and_canonicalize_whole_dataset(dataset):\n",
    "    updated_dataset = []\n",
    "    counter_smiles_changed = 0\n",
    "    for i, spectrum in enumerate(dataset):\n",
    "        smiles = spectrum.get('smiles')\n",
    "        if smiles is None:\n",
    "            raise ValueError(\"Smiles is None, reconstruction and filtering poorly done.\")\n",
    "        new_smiles = remove_stereochemistry_and_canonicalize(smiles)\n",
    "        if new_smiles is None:\n",
    "            continue\n",
    "        spectrum.set('smiles', new_smiles)\n",
    "        if new_smiles != smiles:\n",
    "            counter_smiles_changed += 1\n",
    "        updated_dataset.append(spectrum)\n",
    "    print(f\"Number of smiles canonicalized or destereochemicalized: {counter_smiles_changed}\")\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalized_dataset = remove_stereochemistry_and_canonicalize_whole_dataset(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_smiles = set([obj.get('smiles') for obj in canonicalized_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In the dataset there remains {len(canonicalized_dataset)} / {len(nist_dataset)} molecules and all have now canonical SMILES strings\")\n",
    "print(f\"\\nFrom the remaining there are {len(unique_smiles)} unique SMILES strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "TRAIN_INDEX = 0\n",
    "VALID_INDEX = 1\n",
    "TEST_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each spectrum to its smiles\n",
    "def unique_mapping(dataset):\n",
    "    \n",
    "    smiles_dict = dict()\n",
    "    counter_none = 0\n",
    "    \n",
    "    for spectrum in dataset:\n",
    "        if \"smiles\" not in spectrum.metadata or spectrum.get(\"smiles\") == None:\n",
    "            counter_none += 1\n",
    "            continue\n",
    "        if spectrum.get(\"smiles\") not in smiles_dict:\n",
    "            smiles_dict[spectrum.get(\"smiles\")] = [spectrum]\n",
    "        else:\n",
    "            smiles_dict[spectrum.get(\"smiles\")].append(spectrum)\n",
    "\n",
    "    print(f\"Missing smiles identifier in {counter_none} cases\")\n",
    "    return smiles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shuffled indices for train, valid and test \n",
    "def generate_index(dataset, train_ratio, valid_ratio, test_ratio):\n",
    "    dataset_length = len(dataset)\n",
    "    \n",
    "    train_idx = np.full(int(dataset_length * train_ratio), 0, dtype=int)\n",
    "    valid_idx = np.full(int(dataset_length * valid_ratio), 1, dtype=int)\n",
    "    test_idx = np.full(int(dataset_length * test_ratio), 2, dtype=int)\n",
    "    \n",
    "    concatenate_array = np.concatenate((train_idx, valid_idx, test_idx))\n",
    "    \n",
    "    np.random.shuffle(concatenate_array)\n",
    "    \n",
    "    return concatenate_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build list dateset for training, valid and test\n",
    "# we iterate over all cases with same value and append them to final list in way\n",
    "# that all train, valid and test does not overlap with duplicities \n",
    "# at the end, lists are shuffled to avoid continuous stream of same data\n",
    "def generate_train_test_dataset(dataset, indices):\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    " \n",
    "    for i, spectrums in zip(indices, dataset):\n",
    "        if i == TRAIN_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                train.append(spectrum)\n",
    "        elif i == VALID_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                valid.append(spectrum)\n",
    "        elif i == TEST_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                test.append(spectrum)\n",
    "                \n",
    "    random.shuffle(train)\n",
    "    random.shuffle(valid)\n",
    "    random.shuffle(test)\n",
    "    return (train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving list in msp format\n",
    "def save_dataset(dataset, path, name):            \n",
    "    # makes all intermediate-level directories needed to contain the leaf directory\n",
    "    os.makedirs(path, mode=0o777, exist_ok=True)\n",
    "    save_as_msp(dataset, f\"{path}/{name}.msp\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform splitting\n",
    "nist_dict = unique_mapping(canonicalized_dataset)\n",
    "DATASET_LENGTH = len(nist_dict)\n",
    "\n",
    "indices = generate_index(nist_dict, TRAIN_RATIO, VALID_RATIO, TEST_RATIO)\n",
    "train, valid, test = generate_train_test_dataset(nist_dict, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save splits to .msp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is commented on to avoid unintentional rewriting of the created dataset.\n",
    "\n",
    "save_dir = PROJECT_ROOT + \"/data/nist\"\n",
    "\n",
    "save_dataset(train, save_dir, \"train\")\n",
    "save_dataset(test, save_dir, \"test\")\n",
    "save_dataset(valid, save_dir, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing = count_all(canonicalized_dataset)\n",
    "\n",
    "unique_smiles = set([obj.get('smiles') for obj in canonicalized_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of new unique smiles is {len(set(unique_smiles))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df[\"Update count\"] = [all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing]\n",
    "\n",
    "missing_df[\"Update average\"] = missing_df.apply(lambda row: row[\"Update count\"] / all_data, axis = 1)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no overlap test\n",
    "len(train), len(valid), len(test), len(set(train) & set(valid)), len(set(train) & set(test)), len(set(valid) & set(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) NIST dataleaks elimination\n",
    "In this subsection we ensure that there are no dataleaks between our NIST split and NEIMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) NIST to .jsonl\n",
    "In this part of the notebook we create a `.jsonl` containing only SMILES, m/z values and intensities for each molecule. This file will be used in the training process (the finetuning part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from spectra_process_utils import msp2jsonl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_type in [\"train\", \"valid\", \"test\"]:\n",
    "    dataset_path = Path(f\"{PROJECT_ROOT}/data/nist\")\n",
    "    msp2jsonl(path_msp=dataset_path / f\"{dataset_type}.msp\",\n",
    "                      tokenizer = None,\n",
    "                      path_jsonl=dataset_path / f\"{dataset_type}.jsonl\",\n",
    "                      keep_spectra=True,\n",
    "                      do_preprocess=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SMILES collection from ZINC\n",
    "\n",
    "In this section we download a subset of ZINC20 library (query *2d-standard-annotated-druglike*) that contains SMILES strings but no m/z nor intensities. Finally we want to end up with about 4M to 5M molecules in the Synthetic dataset (empirically a good balance between computational intensity and coverage). We observed that the RASSP filter (section 3) allows about 1/6 of the molecules to pass through, therefore we need to sample 30M SMILES strings from ZINC. The steps to replicate our process are:\n",
    "\n",
    "#### 2.1) Download the ZINC library\n",
    "\n",
    "With our specification query *2d-standard-annotated-druglike* you download about 1.8B SMILES strings (101GB). It is necessary to download all of them and sample them afterwards so we cover the whole chemical space. For the download you can use the `download_script.sh` in `PROJECT_ROOT/data/zinc/scripts` that we got [here](https://zinc20.docking.org/tranches/home). The ZINC20 database is being continuously updated - though it's just small bits, it makes the sampling process nondeterministic. If you want exactly the same Synthetic dataset as we used, you can download it from us.\n",
    "\n",
    "#### 2.2) Sample 40M SMILES strings\n",
    "We further sample the SMILES strings randomly to ~40M, which creates a base for synthetic generation. This step also includes stripping csv header and removing zinc_id column fro the tranches.\n",
    "\n",
    "#### 2.3) SMILES cleaning\n",
    "Perform the following steps: corrupted smiles filtering, destereochemicalization, canonicalization, long smiles filtering (over 100). At the end concat the clean SMILES strings into a single file.\n",
    "\n",
    "#### 2.4) Deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings\n",
    "Now we can finally deduplicate the SMILES strings, to avoid any direct dataleaks we remove all NIST20 molecules and sample the final 30M dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "#paths\n",
    "TRANCHES_PATH = f\"{PROJECT_ROOT}/data/zinc/tranches\"\n",
    "DOWNLOAD_SCRIPT_PATH = f\"{PROJECT_ROOT}/data/zinc/scripts/download_script.sh\"\n",
    "\n",
    "TRANCHES_40M_PATH = f\"{TRANCHES_PATH}_40M\"\n",
    "TRANCHES_40M_CLEAN_PATH = f\"{TRANCHES_40M_PATH}_clean\"\n",
    "ALL_40M_CLEAN_SMILES_PATH = f\"{TRANCHES_40M_CLEAN_PATH}/all_smiles.txt\"\n",
    "ALL_30M_CLEAN_SMILES_PATH = f\"{PROJECT_ROOT}/data/zinc/30M/30M.smi\"\n",
    "\n",
    "\n",
    "# macro variables\n",
    "SEED = 42\n",
    "NUM_WORKERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Download the ZINC library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TRANCHES_PATH}\n",
    "!cd {TRANCHES_PATH} && bash {DOWNLOAD_SCRIPT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Sample 40M SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first script: make it 40M and only SMILES\n",
    "!python ../data/zinc/scripts/zinc_to_slice_of_smiles.py --input-dir {TRANCHES_PATH} --output-dir {TRANCHES_40M_PATH} --sample-ratio 0.0222 --num-workers {NUM_WORKERS} --seed {SEED}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) SMILES cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted smiles filtering, destereochemicalization, SMILES canonicalization, long smiles filtering\n",
    "!python ../data/zinc/scripts/clean_smiless.py --input-dir {TRANCHES_40M_PATH} --output-dir {TRANCHES_40M_CLEAN_PATH} --num-workers {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all files in the 40M_clean folder to one file\n",
    "os.environ['TRANCHES_40M_CLEAN_PATH'] = TRANCHES_40M_CLEAN_PATH\n",
    "os.environ['ALL_40M_CLEAN_SMILES_PATH'] = ALL_40M_CLEAN_SMILES_PATH\n",
    "\n",
    "!echo ${TRANCHES_40M_CLEAN_PATH}/* | xargs -I {} sh -c 'cat {} >> ${ALL_40M_CLEAN_SMILES_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare unique NIST SMILES\n",
    "NIST_FOLDER = f\"{PROJECT_ROOT}/data/nist\"\n",
    "\n",
    "nist_train = pd.read_json(f\"{NIST_FOLDER}/train.jsonl\", lines=True)\n",
    "nist_valid = pd.read_json(f\"{NIST_FOLDER}/valid.jsonl\", lines=True)\n",
    "nist_test = pd.read_json(f\"{NIST_FOLDER}/test.jsonl\", lines=True)\n",
    "\n",
    "nist_unique_smiles = set(nist_train[\"smiles\"]) | set(nist_valid[\"smiles\"]) | set(nist_test[\"smiles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings\n",
    "\n",
    "SAMPLE_SIZE = 30000000\n",
    "\n",
    "Path(ALL_30M_CLEAN_SMILES_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load NIST\n",
    "\n",
    "np.random.seed(SEED)\n",
    "with open(ALL_40M_CLEAN_SMILES_PATH, \"r\") as inputf, open(f\"{ALL_30M_CLEAN_SMILES_PATH}\", \"w\") as outputf:\n",
    "    \n",
    "    print(\"## READING FILE\")\n",
    "    unique_smi = set(inputf.read().splitlines()) # read and deduplicate\n",
    "    print(f\"Length of deduplicated: {len(unique_smi)})\")\n",
    "\n",
    "    unique_smi.difference_update(nist_unique_smiles) # remove NIST20\n",
    "    print(f\"Length of deduplicated without NIST20: {len(unique_smi)})\")\n",
    "\n",
    "    print(\"## SAMPLING\")\n",
    "    unique_smi = np.array(list(unique_smi))\n",
    "    sample = np.random.choice(unique_smi, SAMPLE_SIZE, replace=False)\n",
    "    \n",
    "    print(f\"## WRITING (length: {len(sample)})\")\n",
    "    for smi in sample:\n",
    "        outputf.write(smi + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Remove all the temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN ONLY IF YOU ARE SURE YOU HAVE A CORRECT DATASET IN THE 30M FOLDER\n",
    "\n",
    "shutil.rmtree(TRANCHES_PATH)\n",
    "shutil.rmtree(TRANCHES_40M_PATH)\n",
    "shutil.rmtree(TRANCHES_40M_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "```text\n",
    "downloaded ZINC subset:                               1820667950      (3 hrs)\n",
    "40M sample:                                             40418852      (3 min)\n",
    "40M sample cleaned:                                     40418750      (12 min)\n",
    "40M sample cleaned concatenated:                        40418750      (5 sec)\n",
    "40M sample cleaned deduplicated:                        39577841\n",
    "40M sample cleaned deduplicated without NIST20:         39575106      \n",
    "30M final sample:                                       30000000      (2 min)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RASSP synthetic spectra generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) NEIMS synthetic spectra generation\n",
    "\n",
    "UCESAT, DAT DO PUCU PRO ALESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf preparation\n",
    "PandasTools.AddMoleculeColumnToFrame(\n",
    "    df, smilesCol='smiles', molCol='ROMol')\n",
    "\n",
    "# exporting to SDF\n",
    "df[\"id\"] = df.index\n",
    "PandasTools.WriteSDF(df, str(after_phase2_file), idName=\"id\", properties=list(\n",
    "    df.columns))  # might be trouble with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectra generation\n",
    "subp.check_call(f\"python {config['neims_dir']}/make_spectra_prediction.py \\\n",
    "                    --input_file={after_phase2_file} \\\n",
    "                    --output_file={after_phase3_sdf} \\\n",
    "                    --weights_dir={config['neims_dir']}/NEIMS_weights/massspec_weights\", shell=True) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectra filtering\n",
    "df = PandasTools.LoadSDF(str(after_phase3_sdf), idName=\"id\", molColName='Molecule')\n",
    "    \n",
    "# processing spectra \n",
    "df = oneD_spectra_to_mz_int(df)\n",
    "\n",
    "# filtering high MZs\n",
    "df = df.loc[[x[-1] <= config[\"max_mz\"] for x in tqdm(df[\"mz\"])]]\n",
    "\n",
    "# filtering long spectra\n",
    "df = df.loc[[len(x) <= config[\"max_peaks\"] for x in tqdm(df[\"mz\"])]]\n",
    "\n",
    "# drop unnecessary columns\n",
    "df = df[[\"smiles\", \"mz\", \"intensity\"]]\n",
    "\n",
    "# strip potential whitespace from smiles\n",
    "df[\"smiles\"] = df[\"smiles\"].progress_apply(lambda x: x.strip()) \n",
    "\n",
    "# save the df\n",
    "df.to_json(after_phase4_json, orient=\"records\", lines=True)\n",
    "\n",
    "return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test and valid\n",
    "    df_train, df_test, df_valid = data_split(\n",
    "        df, config, logging, process_id, lock)\n",
    "\n",
    "    path_wo_ext = after_phase5_json.with_suffix('')\n",
    "    path_ext = after_phase5_json.suffix\n",
    "    parent_dir = after_phase5_json.parents[0]\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "    df_train.to_json(str(path_wo_ext) + \"_train\" + path_ext, orient=\"records\", lines=True)\n",
    "    df_test.to_json(str(path_wo_ext) + \"_test\" + path_ext, orient=\"records\", lines=True)\n",
    "    df_valid.to_json(str(path_wo_ext) + \"_valid\" + path_ext, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "    return df_train, df_test, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Dataleaks elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BARTtrainH100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
