{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "In this notebook we describe and demonstrate the process of data acquisition, synthetic spectra generation and dataset composition. Training of our model requires three datasets - experimentally measured commercial NIST GS-EI-MS library (we use NIST20) and two much bigger synthetic datasets generated by open source models NEIMS and RASSP. The NIST dataset can be purchased from distributors listed [here](https://chemdata.nist.gov/dokuwiki/doku.php?id=chemdata:distributors), the synthetic datasets can be either generated with this notebook or downloaded from us (see **Precomputed datasets** below).\n",
    "\n",
    "The only information about each molecule we need for training our model is:\n",
    "    \n",
    "```text\n",
    "- SMILES:              \"CCCCCCCCC(CCCC)O[Si]1(C)CCCCC1\"\n",
    "- list of m/z values:  [26, 27, 28, 31, ..., 200, 201, 255, 256, 257]\n",
    "- list of intensities: [21.98, 335.7, 49.95, 737.34, ..., 23.98, 5.99, 78.93, 20.98, 5.0]\n",
    "```\n",
    "\n",
    "### 1) NIST dataset\n",
    "This notebook first creates training, test and validation splits for NIST and converts them into `.jsonl` files. Each line of these files is a `json` with SMILES, m/z and intensities as keys representing a single molecule.\n",
    "\n",
    "### 2) SMILES acquisition from ZINC\n",
    "Secondly we download a subset of ZINC library (query *2d-standard-annotated-druglike*) that contains SMILES strings but no m/z nor intensities. We further filter the SMILES randomly to 30M, which creates a base for synthetic generation. \n",
    "\n",
    "### 3) RASSP synthetic spectra generation\n",
    "Thirdly, we generate synthetic spectra using RASSP model. This model's molecular restrictions reduce the number of generated spectra to ???4.8M???. These spectra are split into training, validation and test sets.\n",
    "\n",
    "### 4) NEIMS synthetic spectra generation\n",
    "In the fourth step we use NEIMS model to generate synthetic spectra from the ???4.8M??? molecules permitted by RASSP (NEIMS does not have as strict molecular restrictions). These spectra are divided into **the same** training, validation and test splits as RASSP.\n",
    "\n",
    "### 5) Dataleaks elimination (TODO upresni, nebo vyhod - dataleaky se resi v 3) uz)\n",
    "Finally we ensure there are no data leaks between NIST valid + test set (which will serve as the primary evaluation sets) and all the training data (RASSP train set, NEIMS train set and synthetic train sets). \n",
    "\n",
    "### Precomputed datasets\n",
    "Synthetic spectra generation in full scale is very computationally intensive and requires a lot of resources. Therefore, we provide the prepared synthetic dataset in the form of a zip file that can be downloaded from the following link: ....\n",
    "\n",
    "If the user chooses to use the precomputed datasets for training, he can perform step 1 with his o, skip the steps 2-4 and proceed directly to the dataleaks elimination step 5. \n",
    "If the user chooses to generate the synthetic datasets himself, he can go through all the steps and choose to process the whole dataset instead of just the toy example presented.\n",
    "\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) NIST dataset\n",
    "\n",
    "### 1.1) NIST cleaning and splitting\n",
    "The core part of this section is altered from a [notebook](https://github.com/Jozefov/mass-spectra-prediction-GCN/blob/master/Notebooks/data_preprocessing.ipynb) created by Filip Jozefov.\n",
    "\n",
    "- in this subbsection we inspect the missing identifiers in NISt dataset\n",
    "- we dorp ~60k of NIST spectra that don't have any form of a proper identifier (smiles, inchikey)\n",
    "- we canonize the smiles strings and remove stereochemistry information\n",
    "- we split the remaining data in the 0.8:0.1:0.1 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from matchms.importing import load_from_msp\n",
    "from matchms.exporting import save_as_msp\n",
    "from matchms import Spectrum\n",
    "import matchms\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spectra_process_utils import remove_stereochemistry_and_canonicalize\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/xhajek9/gc-ms_bart/clean_paper' # TODO: change this to the path of your project root\n",
    "NIST_PATH = '../data/datasets/NIST/NIST20/20210925_NIST_EI_MS_cleaned.msp' # TODO: change this to the path of your NIST20 dataset\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_dataset = list(load_from_msp(NIST_PATH, metadata_harmonization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of missing identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count examined occurrences of specific data missing in our dataset\n",
    "def count_all(dataset):\n",
    "    \n",
    "    all_data = 0\n",
    "\n",
    "    no_smiles = 0\n",
    "    no_inchikey = 0\n",
    "    no_inchi = 0\n",
    "\n",
    "    no_smile_only = 0\n",
    "    no_inchikey_only = 0\n",
    "    both_missing_counter = 0\n",
    "\n",
    "    all_identifier_missing = 0\n",
    "\n",
    "    for obj in dataset:\n",
    "        if obj.get('smiles') == None:\n",
    "            no_smiles += 1\n",
    "        if obj.get('inchikey') == None:\n",
    "            no_inchikey += 1\n",
    "        if obj.get('inchi') == None:\n",
    "            no_inchi += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchi') != None:\n",
    "            no_smile_only += 1\n",
    "        if obj.get('smiles') != None and obj.get('inchi') == None:\n",
    "            no_inchikey_only += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchikey') != None:\n",
    "            both_missing_counter += 1\n",
    "        if obj.get('smiles') == None and obj.get('inchikey') == None and obj.get('inchi') == None:\n",
    "            all_identifier_missing += 1\n",
    "        all_data += 1\n",
    "    return (all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\n",
    "            both_missing_counter, all_identifier_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing = count_all(nist_dataset)\n",
    "\n",
    "unique_smiles = set([obj.get('smiles') for obj in nist_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We are currently working with {all_data - no_smiles} smiles, from which {len(unique_smiles)} are unique\\n\")\n",
    "print(f\"We are currently working with {all_data - no_inchikey} inchikeys\\n\")\n",
    "print(f\"We are currently working with {all_data - no_inchi} inchi\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS\n",
    "data_missing = {\n",
    "    'All data': [all_data],\n",
    "    'No smiles': [no_smiles],\n",
    "    'No inchikey': [no_inchikey],     \n",
    "    'No inchi': [no_inchi],\n",
    "    'Smiles Only Missing': [no_smile_only],\n",
    "    'Inchi Only Missing': [no_inchikey_only],\n",
    "    'Both Missing': [both_missing_counter],\n",
    "    'All tree missing': [all_identifier_missing],    \n",
    "}\n",
    "missing_df = pd.DataFrame(data_missing)\n",
    "\n",
    "missing_df = missing_df.T\n",
    "missing_df.columns = [\"Count\"]\n",
    "missing_df[\"average\"] = missing_df.apply(lambda row: row.Count / all_data, axis = 1)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATISTICS VISUALIZATION\n",
    "\n",
    "# x-coordinates of left sides of bars \n",
    "parameters_missing = [i for i in range(len(missing_df))]\n",
    "  \n",
    "# heights of bars\n",
    "height = [i for i in missing_df.Count]\n",
    "  \n",
    "# labels for bars\n",
    "tick_label = ['All data', 'No smiles', 'No inchikey', 'No inchi', 'Smiles Only Missing',\n",
    "              'Inchi Only Missing', 'Both Missing', 'All tree missing']\n",
    "  \n",
    "\n",
    "plt.bar(parameters_missing, height, tick_label = tick_label,\n",
    "        width = 0.8)\n",
    "  \n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Number of missing data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifier reconstruction\n",
    "\n",
    "We have approximately 60k data that we are unable to work with. They do not include any identifiers (inchi, inchikey, or SMILES) so we are dropping them. SMILES is crucial for us, so for molecules that don't include a valid SMILES string but include another identifier we will try to reconstruct it. If that fails, we will drop such molecule too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and try to restore corupted smiles\n",
    "# both tools are used, with help of rdkit and matchms as well\n",
    "# e add the smiles destereo and canonization\n",
    "\n",
    "def reconstruct_information(dataset):\n",
    "    updated_dataset = []\n",
    "    for spectrum in tqdm(dataset):\n",
    "        smiles = spectrum.get('smiles')\n",
    "        # all missing\n",
    "        if smiles == None and spectrum.get('inchikey') == None and spectrum.get('inchi') == None:\n",
    "            continue\n",
    "            \n",
    "        #check weather smiles is syntactically valid or molecule is chemically reasonable\n",
    "        if (smiles == None or \\\n",
    "            Chem.MolFromSmiles(smiles, sanitize=False) == None or\\\n",
    "            Chem.MolFromSmiles(smiles) == None) and\\\n",
    "            spectrum.get('inchi') != None:\n",
    "            \n",
    "            # try to convert from inchi\n",
    "            tmp = Chem.inchi.MolFromInchi(spectrum.get('inchi'))\n",
    "            if tmp != None:\n",
    "                spectrum.set('smiles', Chem.MolToSmiles(tmp))\n",
    "                smiles = spectrum.get('smiles')\n",
    "        \n",
    "        # try with matchms\n",
    "        if smiles == None and spectrum.get('inchi') != None:\n",
    "            spectrum = matchms.filtering.derive_smiles_from_inchi(spectrum)\n",
    "            smiles = spectrum.get('smiles')\n",
    "\n",
    "        if smiles == None:\n",
    "            continue\n",
    "            \n",
    "        updated_dataset.append(spectrum)\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_dataset = reconstruct_information(nist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In the dataset there remains {len(reconstructed_dataset)} / {len(nist_dataset)} molecules and all have now SMILES strings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stereochemistry and canonicalize smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stereochemistry_and_canonicalize_whole_dataset(dataset):\n",
    "    updated_dataset = []\n",
    "    counter_smiles_changed = 0\n",
    "    for i, spectrum in enumerate(dataset):\n",
    "        smiles = spectrum.get('smiles')\n",
    "        if smiles is None:\n",
    "            raise ValueError(\"Smiles is None, reconstruction and filtering poorly done.\")\n",
    "        new_smiles = remove_stereochemistry_and_canonicalize(smiles)\n",
    "        if new_smiles is None:\n",
    "            continue\n",
    "        spectrum.set('smiles', new_smiles)\n",
    "        if new_smiles != smiles:\n",
    "            counter_smiles_changed += 1\n",
    "        updated_dataset.append(spectrum)\n",
    "    print(f\"Number of smiles canonicalized or destereochemicalized: {counter_smiles_changed}\")\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalized_dataset = remove_stereochemistry_and_canonicalize_whole_dataset(reconstructed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_smiles = set([obj.get('smiles') for obj in canonicalized_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In the dataset there remains {len(canonicalized_dataset)} / {len(nist_dataset)} molecules and all have now canonical SMILES strings\")\n",
    "print(f\"\\nFrom the remaining there are {len(unique_smiles)} unique SMILES strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "TRAIN_INDEX = 0\n",
    "VALID_INDEX = 1\n",
    "TEST_INDEX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each spectrum to its smiles\n",
    "def unique_mapping(dataset):\n",
    "    \n",
    "    smiles_dict = dict()\n",
    "    counter_none = 0\n",
    "    \n",
    "    for spectrum in dataset:\n",
    "        if \"smiles\" not in spectrum.metadata or spectrum.get(\"smiles\") == None:\n",
    "            counter_none += 1\n",
    "            continue\n",
    "        if spectrum.get(\"smiles\") not in smiles_dict:\n",
    "            smiles_dict[spectrum.get(\"smiles\")] = [spectrum]\n",
    "        else:\n",
    "            smiles_dict[spectrum.get(\"smiles\")].append(spectrum)\n",
    "\n",
    "    print(f\"Missing smiles identifier in {counter_none} cases\")\n",
    "    return smiles_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate shuffled indices for train, valid and test \n",
    "def generate_index(dataset, train_ratio, valid_ratio, test_ratio):\n",
    "    dataset_length = len(dataset)\n",
    "    \n",
    "    train_idx = np.full(int(dataset_length * train_ratio), 0, dtype=int)\n",
    "    valid_idx = np.full(int(dataset_length * valid_ratio), 1, dtype=int)\n",
    "    test_idx = np.full(int(dataset_length * test_ratio), 2, dtype=int)\n",
    "    \n",
    "    concatenate_array = np.concatenate((train_idx, valid_idx, test_idx))\n",
    "    \n",
    "    np.random.shuffle(concatenate_array)\n",
    "    \n",
    "    return concatenate_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build list dateset for training, valid and test\n",
    "# we iterate over all cases with same value and append them to final list in way\n",
    "# that all train, valid and test does not overlap with duplicities \n",
    "# at the end, lists are shuffled to avoid continuous stream of same data\n",
    "def generate_train_test_dataset(dataset, indices):\n",
    "    \n",
    "    train = []\n",
    "    valid = []\n",
    "    test = []\n",
    " \n",
    "    for i, spectrums in zip(indices, dataset):\n",
    "        if i == TRAIN_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                train.append(spectrum)\n",
    "        elif i == VALID_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                valid.append(spectrum)\n",
    "        elif i == TEST_INDEX:\n",
    "            for spectrum in dataset[spectrums]:\n",
    "                test.append(spectrum)\n",
    "                \n",
    "    random.shuffle(train)\n",
    "    random.shuffle(valid)\n",
    "    random.shuffle(test)\n",
    "    return (train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving list in msp format\n",
    "def save_dataset(dataset, path, name):            \n",
    "    # makes all intermediate-level directories needed to contain the leaf directory\n",
    "    os.makedirs(path, mode=0o777, exist_ok=True)\n",
    "    save_as_msp(dataset, f\"{path}/{name}.msp\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform splitting\n",
    "nist_dict = unique_mapping(canonicalized_dataset)\n",
    "DATASET_LENGTH = len(nist_dict)\n",
    "\n",
    "indices = generate_index(nist_dict, TRAIN_RATIO, VALID_RATIO, TEST_RATIO)\n",
    "train, valid, test = generate_train_test_dataset(nist_dict, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save splits to .msp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is commented on to avoid unintentional rewriting of the created dataset.\n",
    "\n",
    "save_dir = PROJECT_ROOT + \"/data/nist\"\n",
    "\n",
    "save_dataset(train, save_dir, \"train\")\n",
    "save_dataset(test, save_dir, \"test\")\n",
    "save_dataset(valid, save_dir, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing = count_all(canonicalized_dataset)\n",
    "\n",
    "unique_smiles = set([obj.get('smiles') for obj in canonicalized_dataset if obj.get('smiles') != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of new unique smiles is {len(set(unique_smiles))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df[\"Update count\"] = [all_data, no_smiles, no_inchikey, no_inchi, no_smile_only, no_inchikey_only,\\\n",
    "            both_missing_counter, all_identifier_missing]\n",
    "\n",
    "missing_df[\"Update average\"] = missing_df.apply(lambda row: row[\"Update count\"] / all_data, axis = 1)\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no overlap test\n",
    "len(train), len(valid), len(test), len(set(train) & set(valid)), len(set(train) & set(test)), len(set(valid) & set(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) NIST dataleaks elimination\n",
    "In this subsection we ensure that there are no dataleaks between our NIST split and NEIMS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) NIST to .jsonl\n",
    "In this part of the notebook we create a `.jsonl` containing only SMILES, m/z values and intensities for each molecule. This file will be used in the training process (the finetuning part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from spectra_process_utils import msp2jsonl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_type in [\"train\", \"valid\", \"test\"]:\n",
    "    dataset_path = Path(f\"{PROJECT_ROOT}/data/nist\")\n",
    "    msp2jsonl(path_msp=dataset_path / f\"{dataset_type}.msp\",\n",
    "                      tokenizer = None,\n",
    "                      path_jsonl=dataset_path / f\"{dataset_type}.jsonl\",\n",
    "                      keep_spectra=True,\n",
    "                      do_preprocess=False\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) SMILES collection from ZINC\n",
    "\n",
    "In this section we download a subset of ZINC20 library (query *2d-standard-annotated-druglike*) that contains SMILES strings but no m/z nor intensities. Finally we want to end up with about 4M to 5M molecules in the Synthetic dataset (empirically a good balance between computational intensity and coverage). We observed that the RASSP filter (section 3) allows about 1/6 of the molecules to pass through, therefore we need to sample 30M SMILES strings from ZINC. The steps to replicate our process are:\n",
    "\n",
    "#### 2.1) Download the ZINC library\n",
    "\n",
    "With our specification query *2d-standard-annotated-druglike* you download about 1.8B SMILES strings (101GB). It is necessary to download all of them and sample them afterwards so we cover the whole chemical space. For the download you can use the `download_script.sh` in `PROJECT_ROOT/data/zinc/scripts` that we got [here](https://zinc20.docking.org/tranches/home). The ZINC20 database is being continuously updated - though it's just small bits, it makes the sampling process nondeterministic. If you want exactly the same Synthetic dataset as we used, you can download it from us.\n",
    "\n",
    "#### 2.2) Sample 40M SMILES strings\n",
    "We further sample the SMILES strings randomly to ~40M, which creates a base for synthetic generation. This step also includes stripping csv header and removing zinc_id column fro the tranches.\n",
    "\n",
    "#### 2.3) SMILES cleaning\n",
    "Perform the following steps: corrupted smiles filtering, destereochemicalization, canonicalization, long smiles filtering (over 100). At the end concat the clean SMILES strings into a single file.\n",
    "\n",
    "#### 2.4) Deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings\n",
    "Now we can finally deduplicate the SMILES strings, to avoid any direct dataleaks we remove all NIST20 molecules and sample the final 30M dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "#paths\n",
    "TRANCHES_PATH = f\"{PROJECT_ROOT}/data/zinc/tranches\"\n",
    "DOWNLOAD_SCRIPT_PATH = f\"{PROJECT_ROOT}/data/zinc/scripts/download_script.sh\"\n",
    "\n",
    "TRANCHES_40M_PATH = f\"{TRANCHES_PATH}_40M\"\n",
    "TRANCHES_40M_CLEAN_PATH = f\"{TRANCHES_40M_PATH}_clean\"\n",
    "ALL_40M_CLEAN_SMILES_PATH = f\"{TRANCHES_40M_CLEAN_PATH}/all_smiles.txt\"\n",
    "ALL_30M_CLEAN_SMILES_PATH = f\"{PROJECT_ROOT}/data/zinc/30M/30M.smi\"\n",
    "\n",
    "\n",
    "# macro variables\n",
    "SEED = 42\n",
    "NUM_WORKERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Download the ZINC library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {TRANCHES_PATH}\n",
    "!cd {TRANCHES_PATH} && bash {DOWNLOAD_SCRIPT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Sample 40M SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first script: make it 40M and only SMILES\n",
    "!python ../data/zinc/scripts/zinc_to_slice_of_smiles.py --input-dir {TRANCHES_PATH} --output-dir {TRANCHES_40M_PATH} --sample-ratio 0.0222 --num-workers {NUM_WORKERS} --seed {SEED}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) SMILES cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted smiles filtering, destereochemicalization, SMILES canonicalization, long smiles filtering\n",
    "!python ../data/zinc/scripts/clean_smiless.py --input-dir {TRANCHES_40M_PATH} --output-dir {TRANCHES_40M_CLEAN_PATH} --num-workers {NUM_WORKERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all files in the 40M_clean folder to one file\n",
    "os.environ['TRANCHES_40M_CLEAN_PATH'] = TRANCHES_40M_CLEAN_PATH\n",
    "os.environ['ALL_40M_CLEAN_SMILES_PATH'] = ALL_40M_CLEAN_SMILES_PATH\n",
    "\n",
    "!echo ${TRANCHES_40M_CLEAN_PATH}/* | xargs -I {} sh -c 'cat {} >> ${ALL_40M_CLEAN_SMILES_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare unique NIST SMILES\n",
    "NIST_FOLDER = f\"{PROJECT_ROOT}/data/nist\"\n",
    "\n",
    "nist_train = pd.read_json(f\"{NIST_FOLDER}/train.jsonl\", lines=True)\n",
    "nist_valid = pd.read_json(f\"{NIST_FOLDER}/valid.jsonl\", lines=True)\n",
    "nist_test = pd.read_json(f\"{NIST_FOLDER}/test.jsonl\", lines=True)\n",
    "\n",
    "nist_unique_smiles = set(nist_train[\"smiles\"]) | set(nist_valid[\"smiles\"]) | set(nist_test[\"smiles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplicate, remove all NIST20 molecules and sample final 30M SMILES strings\n",
    "\n",
    "SAMPLE_SIZE = 30000000\n",
    "\n",
    "Path(ALL_30M_CLEAN_SMILES_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "with open(ALL_40M_CLEAN_SMILES_PATH, \"r\") as inputf, open(f\"{ALL_30M_CLEAN_SMILES_PATH}\", \"w\") as outputf:\n",
    "    \n",
    "    print(\"## READING FILE\")\n",
    "    unique_smi = set(inputf.read().splitlines()) # read and deduplicate\n",
    "    print(f\"Length of deduplicated: {len(unique_smi)})\")\n",
    "\n",
    "    unique_smi.difference_update(nist_unique_smiles) # remove NIST20\n",
    "    print(f\"Length of deduplicated without NIST20: {len(unique_smi)})\")\n",
    "\n",
    "    print(\"## SAMPLING\")\n",
    "    unique_smi = np.array(list(unique_smi))\n",
    "    sample = np.random.choice(unique_smi, SAMPLE_SIZE, replace=False)\n",
    "    \n",
    "    print(f\"## WRITING (length: {len(sample)})\")\n",
    "    for smi in sample:\n",
    "        outputf.write(smi + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Remove all the temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN ONLY IF YOU ARE SURE YOU HAVE A CORRECT DATASET IN THE 30M FOLDER\n",
    "\n",
    "shutil.rmtree(TRANCHES_PATH)\n",
    "shutil.rmtree(TRANCHES_40M_PATH)\n",
    "shutil.rmtree(TRANCHES_40M_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats\n",
    "```text\n",
    "downloaded ZINC subset:                               1820667950      (3 hrs)\n",
    "40M sample:                                             40418852      (3 min)\n",
    "40M sample cleaned:                                     40418750      (12 min)\n",
    "40M sample cleaned concatenated:                        40418750      (5 sec)\n",
    "40M sample cleaned deduplicated:                        39577841\n",
    "40M sample cleaned deduplicated without NIST20:         39575106      \n",
    "30M final sample:                                       30000000      (2 min)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) RASSP synthetic spectra generation\n",
    "\n",
    "This step requires quite a lot of computing time. Therefore it is desirable to offload it to a computational cluster and to run as many parallel, independent jobs. Here, we just prepare the data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/home/ljocha/work/Recetox/gc-ms_bart/clean_paper'\n",
    "ALL_30M_CLEAN_SMILES_DIR = f\"{PROJECT_ROOT}/data/zinc/30M/\"\n",
    "ALL_30M_CLEAN_SMILES_PATH = f\"{ALL_30M_CLEAN_SMILES_DIR}/30M.smi\"\n",
    "RASSP_OUTPUT_DIR = f\"{PROJECT_ROOT}/data/synth/rassp_gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p {RASSP_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! split -a 3 -d -l 100000 {ALL_30M_CLEAN_SMILES_PATH} {ALL_30M_CLEAN_SMILES_PATH}_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {ALL_30M_CLEAN_SMILES_DIR} && zip 30M.zip 30M.smi_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grab the file `{ALL_30M_CLEAN_SMILES_DIR}/30M.zip` (approx. 300 MB), copy it to a suitable folder at the computational cluster, and unzip it there. It yields 300 chunks of 100,000 lines each.\n",
    "\n",
    "We provide an example implementation which runs at our [Metacentrum PBS cluster](https://metacentrum.cz) smoothly. It needs:\n",
    "- [rassp-pbs.sh](../forward/rassp-pbs.sh) : shell wrapper designed to be submitted as PBS job, taking a single argument -- the SMILES file; unless specified otherwise with options, it downloads our RASSP docker image `cerit.io/ljocha/rassp:CURRENT_STABLE`, converts it for singularity, downloads the published RASSP models, splits the input into 1000 lines chunks, and runs predictions on the chunks sequentially.\n",
    "- [rassp-predict.py](../forward/rassp-predict.py) : the actual RASSP spectra prediction, it takes a file with SMILES as input and produces spectra in JSONL format; it calls the original RASSP library, and it is expected to be run in our docker/singularity image. SMILES which cannot be processed by RASSP (too many atoms, unknown atom types, too many subformulae, ...) are filtered out.\n",
    "\n",
    "Copy both these files to the same folder as `30M.zip` and submit a PBS job on each of them:\n",
    "\n",
    "    \n",
    "    singularity pull rassp.sif docker://cerit.io/ljocha/rassp:nvidia-2023-6\n",
    "    \n",
    "    for s in 30M.smi_*; do\n",
    "      qsub -N $s -q gpu -l walltime=4:00:00 -l select=1:ncpus=8:ngpus=1:mem=8gb:scratch_local=50gb -- $PWD/rassp-pbs.sh -i $PWD/rassp.sif $s\n",
    "    done\n",
    "\n",
    "Use of 8 or even more CPUs per job makes sense, GPU is effectively required (virtually any current NVidia will work; the published RASSP model usually fits into 8 GB GPU memory), with current Intel/AMD server CPUs \n",
    "the prediction can give upto 1000 molecules/minute, 4 hours is a safe upper bound for the job.\n",
    "\n",
    "The first line could be omitted together with `-i` option of`rassp-pbs.sh` (it pulls the image, then); however, it takes quite long and it is better to recycle\n",
    "\n",
    "After the jobs are finished, corresponding `*.jsonl` files are copied to the same folder. Copy them back to {RASSP_OUTPUT_DIR}.\n",
    "\n",
    "The docker container was built with Dockerfile available in [our fork of original RASSP repository](https://github.com/ljocha/rassp-public/tree/ljocha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) NEIMS synthetic spectra generation\n",
    "In this section we take a .smi file with molecules filtered according to RASSP restrictions and generate synthetic spectra using NEIMS model. The generated spectra are split into training, validation and test sets and saved as .jsonl files, where each line is a json dictionary with smiles, mz and intensity as keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit.Chem import PandasTools\n",
    "from pathlib import Path\n",
    "import subprocess as subp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# macro variables\n",
    "SEED = 42\n",
    "PROJECT_ROOT = '/home/ljocha/work/Recetox/gc-ms_bart/clean_paper' # TODO: change this to the path of your project root\n",
    "RASSP_OUTPUT_DIR = f\"{PROJECT_ROOT}/data/synth/rassp_gen\"\n",
    "\n",
    "BASE='after_rassp'\n",
    "SMI_FILE_PATH = f\"{PROJECT_ROOT}/data/zinc/10K_debug/{BASE}.smi\"                 #### OPRAV! DEBUG!\n",
    "SYNTH_NEIMS_DATASET_FOLDER = f\"{PROJECT_ROOT}/data/synth/neims_gen/\" #### OPRAV! DEBUG!\n",
    "\n",
    "# NEIMS inference makes a single batch from the whole input file; 50k SMILES use approx. 7GB GPU memory, \n",
    "# which is generally acceptable; make it smaller if you run out\n",
    "# \n",
    "NEIMS_CHUNK=50000\n",
    "\n",
    "Path(SDF_PLAIN_FILE_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NEIMS_REPO = PROJECT_ROOT + \"/deep-molecular-massspec\"\n",
    "NEIMS_WEIGHTS = NEIMS_REPO + \"/NEIMS_weights/massspec_weights\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Gather SMILES from RASSP output and convert to SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smi = []\n",
    "for f in glob.glob(f\"{RASSP_OUTPUT_DIR}/*.jsonl\"):\n",
    "    with open(f) as j:\n",
    "        smi1 = map(lambda l: json.loads(l)['smiles'],j)\n",
    "        smi += smi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {SYNTH_NEIMS_DATASET_FOLDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "while smi:\n",
    "    smi1 = smi[:NEIMS_CHUNK]\n",
    "    smi = smi[NEIMS_CHUNK:]\n",
    "    df_smi = pd.DataFrame(smi1,columns=['smiles'])\n",
    "\n",
    "# alternatively, load .smi file\n",
    "# df_smi = pd.read_csv(SMI_FILE_PATH, header=None, names=[\"smiles\"])\n",
    "\n",
    "    PandasTools.AddMoleculeColumnToFrame(df_smi, smilesCol='smiles', molCol='ROMol')\n",
    "\n",
    "# exporting to SDF\n",
    "    df_smi[\"id\"] = df_smi.index\n",
    "    PandasTools.WriteSDF(df_smi, f'{SYNTH_NEIMS_DATASET_FOLDER}/{BASE}_{i:03}.sdf', idName=\"id\", properties=list(df_smi.columns))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cd {SYNTH_NEIMS_DATASET_FOLDER} && zip {BASE}.zip {BASE}*.sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to RASSP prediction, copy the file {SYNTH_NEIMS_DATASET_FOLDER}/{BASE}.zip to a computational cluster and unzip it there. Copy also the PBS script [neims-pbs.sh](../forward/neims-pbs.sh) and submit it to a suitable queue after downloading our container image (created in [our NEIMS repository clone](https://github.com/ljocha/deep-molecular-massspec/tree/ljocha))\n",
    "\n",
    "    singularity pull neims.sif docker://cerit.io/ljocha/neims\n",
    "    \n",
    "    qsub -q gpu -l walltime=4:00:00 -l select=1:ncpus=1:ngpus=1:mem=8gb:scratch_local=50gb -- $PWD/neims-pbs.sh *.sdf\n",
    "    \n",
    "Unlike RASSP, the NEIMS prediction is quite fast and makes little sense to split it into multiple jobs, the script loops over all input files.\n",
    "\n",
    "After it finishes, grab all {BASE}-out.sdf files and copy them back to {SYNTH_NEIMS_DATASET_FOLDER}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) NEIMS synthetic spectra generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectra generation: NEIMSpy3 conda environment needed!! (29s for 10K)\n",
    "subp.check_call(f\"python {NEIMS_REPO}/make_spectra_prediction.py \\\n",
    "                    --input_file={SDF_PLAIN_FILE_PATH} \\\n",
    "                    --output_file={SDF_ENRIHCED_FILE_PATH} \\\n",
    "                    --weights_dir={NEIMS_WEIGHTS}\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Spectra processing\n",
    "\n",
    "TECHNICKA: chceme data filtrovat uz ted?\n",
    "\n",
    "plus: \n",
    "- vime, na kolika datech budeme trenovat\n",
    "\n",
    "minus: \n",
    "- delame to zbytecne - datapipelina to pa stejne dela znovu onthefly pri trenovani\n",
    "- omezujeme si tim moznost vyuziti dynamickyho pristupu, se kterej muzeme filtry nastavovat az tesne predtrennikem\n",
    "\n",
    "muj nazor - nefiltrovat, ale pak to kvuli cislum v clanku projet a zapsat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from spectro_process_utils (env import problems)\n",
    "def oneD_spectra_to_mz_int(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function that takes a DF and splits the one-array-representation of spectra into mz and intensity parts\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "         dataframe containing 'PREDICTED SPECTRUM' column with sdf spectra representation\n",
    "         -> is used after loading enriched sdf file with PandasTools.LoadSDF\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df2 : pd.DataFrame\n",
    "          dataframe containing columns 'mz' and 'intensity' that contain decomposed spectra representation, two arrays of the same length\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    all_i = []\n",
    "    all_mz = []\n",
    "    for row in tqdm(range(len(df2))):\n",
    "        spec = df2[\"PREDICTED SPECTRUM\"][row].split(\"\\n\")\n",
    "        mz = []\n",
    "        i = []\n",
    "        spec_max = 0\n",
    "        for t in spec:\n",
    "            j,d = t.split()\n",
    "            j,d = int(j), float(d)\n",
    "            if spec_max < d:\n",
    "                spec_max = d\n",
    "            mz.append(j)\n",
    "            i.append(d)\n",
    "        all_mz.append(mz)\n",
    "        all_i.append(np.around(np.array(i)/spec_max, 2))\n",
    "    new_df = pd.DataFrame.from_dict({\"mz\": all_mz, \"intensity\": all_i})\n",
    "    df2 = pd.concat([df2, new_df], axis=1)\n",
    "    df2 = df2.drop([\"PREDICTED SPECTRUM\"], axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = PandasTools.LoadSDF(str(SDF_ENRIHCED_FILE_PATH), idName=\"id\", molColName='Molecule')\n",
    "    \n",
    "# processing spectra (get prefered mz and intensity format)\n",
    "print(\"Spectra format conversion\")\n",
    "df_enriched = oneD_spectra_to_mz_int(df_enriched)\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_enriched = df_enriched[[\"smiles\", \"mz\", \"intensity\"]]\n",
    "\n",
    "# strip potential whitespace from smiles (just to be absolutely sure)\n",
    "print(\"Whitespace stripping\")\n",
    "df_enriched[\"smiles\"] = df_enriched[\"smiles\"].progress_apply(lambda x: x.strip()) \n",
    "\n",
    "# save the df (partial result, can be saved and loaded later)\n",
    "# df_enriched.to_json(????, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, train_val_test_ratio, seed):\n",
    "    \"\"\"split the df into train, test and valid sets\"\"\"\n",
    "\n",
    "    if sum(train_val_test_ratio) != 1:\n",
    "        print(\"The sum of the ratios is not equal to 1.\")\n",
    "        train_val_test_ratio = list(np.array(train_val_test_ratio) / sum(train_val_test_ratio))\n",
    "        print(f\"Ratios were normalized to {train_val_test_ratio}\")\n",
    "\n",
    "    train_set = df.sample(frac=train_val_test_ratio[0], random_state=seed)\n",
    "    rest = df.drop(train_set.index)\n",
    "\n",
    "    valid_set = rest.sample(frac=train_val_test_ratio[1] / (train_val_test_ratio[1] + train_val_test_ratio[2]), \n",
    "                            random_state=seed)\n",
    "    test_set = rest.drop(valid_set.index)\n",
    "\n",
    "    print(f\"SPLITTING STATS\\n\" +\n",
    "          f\"train len: {len(train_set)}\\ntest len: {len(test_set)}\\nvalid len: {len(valid_set)}\\n\" +\n",
    "          f\"{len(train_set)} + {len(test_set)} + {len(valid_set)} == {len(df)} : the sum matches len of the df\\n\")\n",
    "\n",
    "    return train_set, test_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test and valid\n",
    "train_val_test_ratio = [0.9, 0.05, 0.05]\n",
    "df_train, df_test, df_valid = data_split(df_enriched, train_val_test_ratio, SEED)\n",
    "\n",
    "df_train.to_json(SYNTH_NEIMS_DATASET_FOLDER + \"/train.jsonl\", orient=\"records\", lines=True)\n",
    "df_valid.to_json(SYNTH_NEIMS_DATASET_FOLDER + \"/valid.jsonl\", orient=\"records\", lines=True)\n",
    "df_test.to_json(SYNTH_NEIMS_DATASET_FOLDER + \"/test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
