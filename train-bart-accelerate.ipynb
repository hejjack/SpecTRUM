{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "technological-dressing",
   "metadata": {},
   "source": [
    "# Training BART using the Accelerate API\n",
    "Training BART in multi-GPU / multi-NODE manner with the Accelerate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "round-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, BartConfig, BartForConditionalGeneration\n",
    "from transformers.file_utils import logging\n",
    "\n",
    "# custom veci\n",
    "from dataset import SpectroDataset, SpectroDataCollator\n",
    "sys.path.append('data')\n",
    "sys.path.append('bart_spektro')\n",
    "from modeling_bart_spektro import BartSpektoForConditionalGeneration\n",
    "from configuration_bart_spektro import BartSpektroConfig\n",
    "from data_preprocess1 import print_args\n",
    "from bart_spektro_tokenizer import BartSpektroTokenizer\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "russian-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initinal variables (shouldn't be necessary to change anything after thi cell)\n",
    "bs = 32 #4\n",
    "gas = 16 # gradient accumulation steps\n",
    "which_bart = \"spektro\" #\"original\" # \"spektro\"\n",
    "data_type = \"1K\"\n",
    "\n",
    "# TOKENIZER\n",
    "tokenizer_type = \"_bbpe_1M\" # for my custom spektro tokenizer use \"\"\n",
    "tokenizer = Tokenizer.from_file(f\"./tokenizer/bbpe_tokenizer/bart{tokenizer_type}_tokenizer.model\")\n",
    "\n",
    "SEQ_LEN = 200\n",
    "num_epochs = 10 # 10 (BARTY se trenovaly 10 epoch celkem) # int(os.environ[\"TOTAL_EPOCHS\"]) #21 + 8 = BP\n",
    "resume_training = False # bool(int(os.environ[\"RESUME_TRAINING\"]))\n",
    "resume_wandb_id = \"\" # \"1l7305qk\" #pass # \"\"\n",
    "\n",
    "model = None # aby nebyl nedefinovany\n",
    "\n",
    "# find the last checkpoint automatically\n",
    "# models_pth = \"/storage/projects/msml/mg_neims_branch/models/bart_trial\"\n",
    "# runs = glob.glob(models_pth)\n",
    "# checkpoints =  glob.glob(runs[-1]+\"/checkpoint-*\")\n",
    "# checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "# load_checkpoint = checkpoints[-1]\n",
    "# print(f\"last checkpoint: {load_checkpoint}\")\n",
    "\n",
    "# load_checkpoint = \"./models/bart_2022-06-01-04_30_20/checkpoint-4152/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "municipal-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of gpus \n",
    "os.environ[\"PBS_NGPUS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "roman-county",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:\n",
      "  lr ........................... 5e-05\n",
      "  seed ......................... 42\n",
      "  gradient_accumulation_steps .. 16\n",
      "  batch_size ................... 32\n",
      "  warmup ....................... 500\n",
      "  weight_decay ................. 0.01\n",
      "  num_workers .................. 16\n",
      "  device ....................... cuda\n",
      "  num_train_epochs ............. 10\n",
      "  save_dir ..................... /storage/projects/msml/mg_neims_branch/MassGenie/models\n",
      "  save_name .................... bart_2022-06-21-17_22_59\n",
      "  load_checkpoint .............. \n",
      "  fp16 ......................... True\n",
      "  train_data_path .............. /storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/1K_bbpe_1M_bart_prepared_data_train.pkl\n",
      "  valid_data_path .............. /storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/1K_bbpe_1M_bart_prepared_data_valid.pkl\n",
      "  log_steps .................... 50\n",
      "  eval_steps ................... 7142\n",
      "  wandb ........................ True\n",
      "  wandb_resume ................. False\n",
      "  wandb_id ..................... f4eambid\n"
     ]
    }
   ],
   "source": [
    "# arguments for the training (fix ones)\n",
    "now = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "now = now.replace(\":\",\"_\").replace(\" \", \"-\")\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--gradient-accumulation-steps\",default=gas, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch-size\",default=bs, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--warmup\",default=500, type=int,  help=\"warmup steps for learning rate\")\n",
    "parser.add_argument(\"--weight-decay\",default=0.01, type=float,  help=\"weight decay rate parameter\")\n",
    "parser.add_argument(\"--num-workers\",default=os.environ[\"PBS_NCPUS\"], type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--num-train-epochs\",default=num_epochs, type=int,  help=\"number of training epochs\")\n",
    "parser.add_argument(\"--save-dir\",default='/storage/projects/msml/mg_neims_branch/MassGenie/models', type=str,  help=\"Path to save trained model\")\n",
    "parser.add_argument(\"--save-name\", type=str, default=f'bart_{now}', help=\"Name of the model, used for saves\")\n",
    "parser.add_argument(\"--load-checkpoint\", type=str, default='', help=\"Path to the checkpoint to resume training\")\n",
    "parser.add_argument(\"--fp16\",default=True, type=bool, required=False, help=\"whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument(\"--train-data-path\",default=f'/storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/{data_type}{tokenizer_type}_bart_prepared_data_train.pkl', type=str, help=\"Path to jsonl train dataset\")\n",
    "parser.add_argument(\"--valid-data-path\",default=f'/storage/projects/msml/mg_neims_branch/MassGenie/data/trial_set/{data_type}{tokenizer_type}_bart_prepared_data_valid.pkl', type=str, help=\"Path to jsonl validation dataset\")\n",
    "parser.add_argument(\"--log-steps\",default=50, type=int,  help=\"number of steps between logs\")\n",
    "parser.add_argument(\"--eval-steps\",default=7142, type=int,  help=\"number of steps between evaluations\")\n",
    "parser.add_argument(\"--wandb\", action='store_true', default=True, help=\"optinal logging via Weights&Biases\")\n",
    "parser.add_argument(\"--wandb-resume\", action='store_true', default=resume_training, help=\"resume logging via wandb, needs an valid run ID set in args.wandb-id\")\n",
    "parser.add_argument(\"--wandb-id\", type=str, default=wandb.util.generate_id(), help=\"Process unique wandb ID used for resumin the training process\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "arg_log = print_args(args)\n",
    "\n",
    "# extended outputs\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "smoking-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART CONIGURATION\n",
    "if which_bart == \"spektro\":\n",
    "    config = BartSpektroConfig(vocab_size = len(tokenizer.get_vocab()),\n",
    "                                 max_position_embeddings = SEQ_LEN,\n",
    "                                 max_length = SEQ_LEN,\n",
    "                                 min_len = 0,\n",
    "                                 encoder_layers = 12,\n",
    "                                 encoder_ffn_dim = 4096,\n",
    "                                 encoder_attention_heads = 16,\n",
    "                                 decoder_layers = 12,\n",
    "                                 decoder_ffn_dim = 4096,\n",
    "                                 decoder_attention_heads = 16,\n",
    "                                 encoder_layerdrop = 0.0,\n",
    "                                 decoder_layerdrop = 0.0,\n",
    "                                 activation_function = 'gelu',\n",
    "                                 d_model = 1024,\n",
    "                                 dropout = 0.2,\n",
    "                                 attention_dropout = 0.0,\n",
    "                                 activation_dropout = 0.0,\n",
    "                                 init_std = 0.02,\n",
    "                                 classifier_dropout = 0.0,\n",
    "                                 scale_embedding = False,\n",
    "                                 use_cache = True,\n",
    "                                 pad_token_id = 2,\n",
    "                                 bos_token_id = 3,\n",
    "                                 eos_token_id = 0,\n",
    "                                 is_encoder_decoder = True,\n",
    "                                 decoder_start_token_id = 3,\n",
    "                                 forced_eos_token_id = 0,\n",
    "                                 max_log_id=9)\n",
    "\n",
    "if which_bart == \"original\":\n",
    "    config = BartConfig(vocab_size = len(tokenizer.get_vocab()),\n",
    "                                 max_position_embeddings = SEQ_LEN,\n",
    "                                 max_length = SEQ_LEN,\n",
    "                                 min_len = 0,\n",
    "                                 encoder_layers = 12,\n",
    "                                 encoder_ffn_dim = 4096,\n",
    "                                 encoder_attention_heads = 16,\n",
    "                                 decoder_layers = 12,\n",
    "                                 decoder_ffn_dim = 4096,\n",
    "                                 decoder_attention_heads = 16,\n",
    "                                 encoder_layerdrop = 0.0,\n",
    "                                 decoder_layerdrop = 0.0,\n",
    "                                 activation_function = 'gelu',\n",
    "                                 d_model = 1024,\n",
    "                                 dropout = 0.2,\n",
    "                                 attention_dropout = 0.0,\n",
    "                                 activation_dropout = 0.0,\n",
    "                                 init_std = 0.02,\n",
    "                                 classifier_dropout = 0.0,\n",
    "                                 scale_embedding = False,\n",
    "                                 use_cache = True,\n",
    "                                 pad_token_id = 2,\n",
    "                                 bos_token_id = 3,\n",
    "                                 eos_token_id = 0,\n",
    "                                 is_encoder_decoder = True,\n",
    "                                 decoder_start_token_id = 3,\n",
    "                                 forced_eos_token_id = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "modified-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "train_data = SpectroDataset(args.train_data_path, original=which_bart==\"original\")\n",
    "valid_data = SpectroDataset(args.valid_data_path, original=which_bart==\"original\")\n",
    "\n",
    "# drops \n",
    "train_data.data.drop(columns=[\"decoder_input_ids\"], inplace=True)\n",
    "valid_data.data.drop(columns=[\"decoder_input_ids\"], inplace=True)\n",
    "\n",
    "if which_bart == \"original\":\n",
    "    try:\n",
    "        train_data.data.drop(columns=[\"position_ids\"], inplace=True)\n",
    "        valid_data.data.drop(columns=[\"position_ids\"], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=bs, shuffle=True)    \n",
    "\n",
    "# clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# MODEL\n",
    "if which_bart == \"original\":\n",
    "    model = BartForConditionalGeneration(config)\n",
    "elif which_bart == \"spektro\":\n",
    "    model = BartSpektoForConditionalGeneration(config)\n",
    "else:\n",
    "    raise AttributeError('Wrong \\'which_bart\\' attribute. Assign \\'original\\' or \\'spektro\\'.')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "advance-slide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destereo_smiles</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>encoder_attention_mask</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>position_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...</td>\n",
       "      <td>[15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...</td>\n",
       "      <td>[3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1</td>\n",
       "      <td>[17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...</td>\n",
       "      <td>[1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...</td>\n",
       "      <td>[30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...</td>\n",
       "      <td>[2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C</td>\n",
       "      <td>[33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...</td>\n",
       "      <td>[2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C</td>\n",
       "      <td>[33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...</td>\n",
       "      <td>[4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       destereo_smiles  \\\n",
       "629  COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...   \n",
       "338       O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1   \n",
       "620  CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...   \n",
       "396   Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C   \n",
       "251            O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C   \n",
       "\n",
       "                                             input_ids  \\\n",
       "629  [15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...   \n",
       "338  [17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   \n",
       "620  [30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
       "396  [33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "251  [33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...   \n",
       "\n",
       "                                encoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                decoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \\\n",
       "629  [3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...   \n",
       "338  [3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...   \n",
       "620  [3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...   \n",
       "396  [3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...   \n",
       "251  [3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...   \n",
       "\n",
       "                                          position_ids  \n",
       "629  [3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...  \n",
       "338  [1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...  \n",
       "620  [2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...  \n",
       "396  [2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...  \n",
       "251  [4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "african-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f4eambid) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3491714<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba92c7df47442f18251f539a4358ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/auto/brno6/home/ahajek/Spektro/MassGenie/wandb/run-20220621_172428-f4eambid/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/auto/brno6/home/ahajek/Spektro/MassGenie/wandb/run-20220621_172428-f4eambid/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">bart_2022-06-21-17_22_59</strong>: <a href=\"https://wandb.ai/hajekad/BART_for_gcms/runs/f4eambid\" target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms/runs/f4eambid</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:f4eambid). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Resuming run <strong style=\"color:#cdcd00\">bart_2022-06-21-17_22_59accelerate</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hajekad/BART_for_gcms\" target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hajekad/BART_for_gcms/runs/f4eambid\" target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms/runs/f4eambid</a><br/>\n",
       "                Run data is saved locally in <code>/auto/brno6/home/ahajek/Spektro/MassGenie/wandb/run-20220621_172522-f4eambid</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resume training\n",
    "if resume_training:\n",
    "    args.load_checkpoint = load_checkpoint\n",
    "    if args.wandb_resume:\n",
    "        args.wandb_id = resume_wandb_id\n",
    "\n",
    "# Init wandb\n",
    "if args.wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(id=args.wandb_id, resume=\"allow\", entity=\"hajekad\", project=\"BART_for_gcms\")\n",
    "    wandb.run.name = args.save_name + \"_accelerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-stockholm",
   "metadata": {},
   "source": [
    "## Now the ***Accelerate*** stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "amateur-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "  )\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "personalized-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerate preparation\n",
    "train_dataloader, valid_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, valid_dataloader, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-light",
   "metadata": {},
   "source": [
    "### Training loop (basic)\n",
    "pridelat minimalne: \n",
    "- wandb logging\n",
    "- evaluace po x krocich\n",
    "- gas? \n",
    "- saving\n",
    "- fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "instructional-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [01:30<00:00,  1.82it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-integrity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
