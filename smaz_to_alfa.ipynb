{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import transformers\n",
    "from bart_spektro.modeling_bart_spektro import BartSpektroForConditionalGeneration\n",
    "from bart_spektro.configuration_bart_spektro import BartSpektroConfig\n",
    "from data_utils import SpectroDataCollator, SpectroDataset\n",
    "import torch\n",
    "from metrics import compute_cos_simils, SpectroMetrics\n",
    "from train_bart import build_tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(tokenizer_path: str) -> transformers.PreTrainedTokenizerFast:\n",
    "    bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=bpe_tokenizer,\n",
    "                                        bos_token=\"<bos>\",\n",
    "                                        eos_token=\"<eos>\",\n",
    "                                        unk_token=\"<ukn>\",\n",
    "                                        pad_token=\"<pad>\",\n",
    "                                        is_split_into_words=False)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model\"\n",
    "bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "tokenizer = build_tokenizer(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1234, 224, 276, 11, 70, 20, 280, 11, 266, 20, 286, 12, 286, 11, 38, 289, 38, 12, 38, 12, 50, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer.encode(\"<bos><nist> CCC(c1ccc(cc1OC)OC(C)(C)C)O<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Clc1ccc(c(c1)Cl)c1ccc(cc1)OC(=O)c1sccc1S(=O)(=O)N(C)C',\n",
       " ' Cc1cc2c(c(=S)[nH]1)C(=O)OC2=Cc1ccc(Cl)c(Cl)c1')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_smiles = \"<bos><nist> Clc1ccc(c(c1)Cl)c1ccc(cc1)OC(=O)c1sccc1S(=O)(=O)N(C)C<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"\n",
    "gt_smiles = \"<nist> Cc1cc2c(c(=S)[nH]1)C(=O)OC2=Cc1ccc(Cl)c(Cl)c1<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos>\"\n",
    "\n",
    "pred_ids = tokenizer.encode(pred_smiles)\n",
    "gt_ids = tokenizer.encode(gt_smiles)\n",
    "pred_smiles, gt_smiles = tokenizer.decode(pred_ids, skip_special_tokens=True), tokenizer.decode(gt_ids, skip_special_tokens=True)\n",
    "pred_smiles, gt_smiles\n",
    "\n",
    "\n",
    "# pred_ids = torch.tensor(pred_ids).unsqueeze(0)\n",
    "# gt_ids = torch.tensor(gt_ids).unsqueeze(0)\n",
    "\n",
    "# pred_ids, gt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = [' CC1CCN(CC1N)Cc1ccns1)CC(C)C']\n",
    "pred_ids = [' CC1CCN(CC1N)Cc1ccns1)CC(C)C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:20:13] SMILES Parse Error: extra close parentheses while parsing: CC1CCN(CC1N)Cc1ccns1)CC(C)C\n",
      "[22:20:13] SMILES Parse Error: Failed parsing SMILES ' CC1CCN(CC1N)Cc1ccns1)CC(C)C' for input: ' CC1CCN(CC1N)Cc1ccns1)CC(C)C'\n",
      "[22:20:13] SMILES Parse Error: extra close parentheses while parsing: CC1CCN(CC1N)Cc1ccns1)CC(C)C\n",
      "[22:20:13] SMILES Parse Error: Failed parsing SMILES ' CC1CCN(CC1N)Cc1ccns1)CC(C)C' for input: ' CC1CCN(CC1N)Cc1ccns1)CC(C)C'\n",
      "ic| pred: ' CC1CCN(CC1N)Cc1ccns1)CC(C)C'\n",
      "    true: ' CC1CCN(CC1N)Cc1ccns1)CC(C)C'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = SpectroMetrics(tokenizer)\n",
    "prediction = transformers.EvalPrediction(predictions=pred_ids, label_ids=gt_ids)\n",
    "compute_cos_simils(label_ids, pred_ids)\n",
    "# metric(prediction, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:29:48] SMILES Parse Error: syntax error while parsing: <neims>\n",
      "[22:29:48] SMILES Parse Error: Failed parsing SMILES '<neims>' for input: '<neims>'\n",
      "ic| pred: ' Clc1ccc(c(c1)Cl)c1ccc(cc1)OC(=O)c1sccc1S(=O)(=O)N(C)C'\n",
      "    true: '<neims> Cc1cc2c(c(=S)[nH]1)C(=O)OC2=Cc1ccc(Cl)c(Cl)c1'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "simil1 = \" Clc1ccc(c(c1)Cl)c1ccc(cc1)OC(=O)c1sccc1S(=O)(=O)N(C)C\"\n",
    "simil2 = \"Cc1cc2c(c(=S)[nH]1)C(=O)OC2=Cc1ccc(Cl)c(Cl)c1\"\n",
    "a,b,c = compute_cos_simils([simil1], [simil2], tokenizer)\n",
    "display(b[0])\n",
    "display(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m BartSpektroConfig(vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocab()), max_log_id\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m BartSpektroForConditionalGeneration(config)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mcheckpoints/bart_2023-04-07-18_27_23_30Mneims/checkpoint-1670000/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gc-ms_bart/bart_spektro/modeling_bart_spektro.py:386\u001b[0m, in \u001b[0;36mBartSpektroForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: BartSpektroConfig):\n\u001b[1;32m    385\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m BartSpektroModel(config)\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mfinal_logits_bias\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mnum_embeddings)))\n\u001b[1;32m    388\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mnum_embeddings, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/gc-ms_bart/bart_spektro/modeling_bart_spektro.py:270\u001b[0m, in \u001b[0;36mBartSpektroModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, config\u001b[39m.\u001b[39md_model, padding_idx)\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BartSpektroEncoder(config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared)\n\u001b[0;32m--> 270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m BartDecoder(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:911\u001b[0m, in \u001b[0;36mBartDecoder.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m embed_tokens\u001b[39m.\u001b[39mweight\n\u001b[1;32m    907\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions \u001b[39m=\u001b[39m BartLearnedPositionalEmbedding(\n\u001b[1;32m    908\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m    909\u001b[0m     config\u001b[39m.\u001b[39md_model,\n\u001b[1;32m    910\u001b[0m )\n\u001b[0;32m--> 911\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BartDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39md_model)\n\u001b[1;32m    914\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:911\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m embed_tokens\u001b[39m.\u001b[39mweight\n\u001b[1;32m    907\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions \u001b[39m=\u001b[39m BartLearnedPositionalEmbedding(\n\u001b[1;32m    908\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m    909\u001b[0m     config\u001b[39m.\u001b[39md_model,\n\u001b[1;32m    910\u001b[0m )\n\u001b[0;32m--> 911\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BartDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    912\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39md_model)\n\u001b[1;32m    914\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/transformers/models/bart/modeling_bart.py:388\u001b[0m, in \u001b[0;36mBartDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_attn_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    387\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, config\u001b[39m.\u001b[39mdecoder_ffn_dim)\n\u001b[0;32m--> 388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mdecoder_ffn_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim)\n\u001b[1;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = BartSpektroConfig(vocab_size=len(tokenizer.get_vocab()), max_log_id=9)\n",
    "model = BartSpektroForConditionalGeneration(config)\n",
    "model.from_pretrained(\"checkpoints/bart_2023-04-07-18_27_23_30Mneims/checkpoint-1670000/\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpectroDataset(\"data/datasets/DEBUG/DEBUG_valid.jsonl\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=SpectroDataCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader:\n",
    "    preds = model.generate(input_ids=batch[\"input_ids\"],\n",
    "                           position_ids=batch[\"position_ids\"],\n",
    "                           attention_mask=batch[\"attention_mask\"],\n",
    "                           max_length=100,\n",
    "                           num_gen)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>NCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCCOCOCCOCCOCOCCOCCOCOCCOCCOCOCCOCCOCOCCOCCOCOCCOCCOCOCCOCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCCNCCCOCC<pad>',\n",
       " '<pad>FCCOCCNC23232323232323232323\\x1aFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNCFCCOCCNC<pad>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_batch(preds.tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos>',\n",
       " '<eos>',\n",
       " '<source3>',\n",
       " '<pad>',\n",
       " '<trafo>',\n",
       " '<nist>',\n",
       " '<ukn>',\n",
       " '<source2>',\n",
       " '<',\n",
       " '<rassp>',\n",
       " '<source1>',\n",
       " '<neims>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x.startswith(\"<\"), list(tokenizer.get_vocab().keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens([\"<trafo>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([1, 2]),\n",
       "  'position_ids': tensor([1, 2]),\n",
       "  'attention_mask': tensor([1, 2]),\n",
       "  'decoder_attention_mask': tensor([1, 2]),\n",
       "  'labels': tensor([1, 2])},\n",
       " {'input_ids': tensor([3, 4]),\n",
       "  'position_ids': tensor([3, 4]),\n",
       "  'attention_mask': tensor([3, 4]),\n",
       "  'decoder_attention_mask': tensor([3, 4]),\n",
       "  'labels': tensor([3, 4])},\n",
       " {'input_ids': tensor([5, 6]),\n",
       "  'position_ids': tensor([5, 6]),\n",
       "  'attention_mask': tensor([5, 6]),\n",
       "  'decoder_attention_mask': tensor([5, 6]),\n",
       "  'labels': tensor([5, 6])},\n",
       " {'input_ids': tensor([7, 8]),\n",
       "  'position_ids': tensor([7, 8]),\n",
       "  'attention_mask': tensor([7, 8]),\n",
       "  'decoder_attention_mask': tensor([7, 8]),\n",
       "  'labels': tensor([7, 8])},\n",
       " {'input_ids': tensor([ 9, 10]),\n",
       "  'position_ids': tensor([ 9, 10]),\n",
       "  'attention_mask': tensor([ 9, 10]),\n",
       "  'decoder_attention_mask': tensor([ 9, 10]),\n",
       "  'labels': tensor([ 9, 10])},\n",
       " {'input_ids': tensor([11, 12]),\n",
       "  'position_ids': tensor([11, 12]),\n",
       "  'attention_mask': tensor([11, 12]),\n",
       "  'decoder_attention_mask': tensor([11, 12]),\n",
       "  'labels': tensor([11, 12])},\n",
       " {'input_ids': tensor([13, 14]),\n",
       "  'position_ids': tensor([13, 14]),\n",
       "  'attention_mask': tensor([13, 14]),\n",
       "  'decoder_attention_mask': tensor([13, 14]),\n",
       "  'labels': tensor([13, 14])},\n",
       " {'input_ids': tensor([15, 16]),\n",
       "  'position_ids': tensor([15, 16]),\n",
       "  'attention_mask': tensor([15, 16]),\n",
       "  'decoder_attention_mask': tensor([15, 16]),\n",
       "  'labels': tensor([15, 16])}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper, FileOpener, JsonParser\n",
    "from data_utils import build_single_datapipe\n",
    "pipe = build_single_datapipe(\"data/datasets/DEBUG_dummy/1.jsonl\")\n",
    "loader = torch.utils.data.DataLoader(pipe, batch_size=2, collate_fn=SpectroDataCollator())\n",
    "list(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = IterableWrapper(range(10))\n",
    "list(dp)\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data/datasets/DEBUG/DEBUG_train.jsonl',\n",
       "  StreamWrapper<<_io.BufferedReader name='data/datasets/DEBUG/DEBUG_train.jsonl'>>),\n",
       " ('data/datasets/DEBUG/DEBUG_valid.jsonl',\n",
       "  StreamWrapper<<_io.BufferedReader name='data/datasets/DEBUG/DEBUG_valid.jsonl'>>)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(datapipe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAPIPES DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_csv(file_label, num_rows: int = 5000, num_features: int = 20) -> None:\n",
    "    fieldnames = ['label'] + [f'c{i}' for i in range(num_features)]\n",
    "    writer = csv.DictWriter(open(f\"sample_data{file_label}.csv\", \"w\", newline=''), fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for i in range(num_rows):\n",
    "        row_data = {col: random.random() for col in fieldnames}\n",
    "        row_data['label'] = random.randint(0, 9)\n",
    "        writer.writerow(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchdata.datapipes as dp\n",
    "\n",
    "def filter_for_data(filename):\n",
    "    return \"sample_data\" in filename and filename.endswith(\".csv\")\n",
    "\n",
    "def row_processor(row):\n",
    "    return {\"label\": np.array(row[0], np.int32), \"data\": np.array(row[1:], dtype=np.float64)}\n",
    "\n",
    "def build_datapipes(root_dir=\".\"):\n",
    "    datapipe = dp.iter.FileLister(root_dir)\n",
    "    datapipe = datapipe.filter(filter_fn=filter_for_data)\n",
    "    datapipe = datapipe.open_files(mode='rt')\n",
    "    datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1)\n",
    "    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n",
    "    datapipe = datapipe.shuffle()\n",
    "    datapipe = datapipe.sharding_filter()  # !!! AFTER SHUFFLE, BEFORE EXPENSIVE OPERATIONS !!!\n",
    "    datapipe = datapipe.map(row_processor)\n",
    "    return datapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels batch shape: torch.Size([5])\n",
      "Feature batch shape: torch.Size([5, 3])\n",
      "labels = tensor([2, 3, 9, 7, 8], dtype=torch.int32)\n",
      "features = tensor([[0.4408, 0.9905, 0.0471],\n",
      "        [0.5507, 0.3512, 0.5192],\n",
      "        [0.9291, 0.2817, 0.8013],\n",
      "        [0.2765, 0.6562, 0.9340],\n",
      "        [0.6604, 0.9201, 0.0606]], dtype=torch.float64)\n",
      "n_sample = 6\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data_utils importimportimportimportimportimportimportimport DataLoader\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_files_to_generate = 3\n",
    "    for i in range(num_files_to_generate):\n",
    "        generate_csv(file_label=i, num_rows=10, num_features=3)\n",
    "    datapipe = build_datapipes()\n",
    "    dl = DataLoader(dataset=datapipe, batch_size=5, num_workers=2)\n",
    "    first = next(iter(dl))\n",
    "    labels, features = first['label'], first['data']\n",
    "    print(f\"Labels batch shape: {labels.size()}\")\n",
    "    print(f\"Feature batch shape: {features.size()}\")\n",
    "    print(f\"{labels = }\\n{features = }\")\n",
    "    n_sample = 0\n",
    "    for row in iter(dl):\n",
    "        n_sample += 1\n",
    "    print(f\"{n_sample = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My DATAPIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from torch.utils.data_utils importimportimportimportimportimportimport DataLoader\n",
    "from data_utils import SpectroDataCollator, SpectroDataset, load_all_datapipes\n",
    "import yaml\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "10\n",
      "10\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': SampleMultiplexerDataPipe,\n",
       " 'valid': {'debug1': MapperIterDataPipe, 'debug2': MapperIterDataPipe},\n",
       " 'example': {'debug1': MapperIterDataPipe, 'debug2': MapperIterDataPipe}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRY IT OUT\n",
    "with open(\"configs/train_config.yaml\", \"r\") as f:\n",
    "    try:\n",
    "        config = yaml.safe_load(f)\n",
    "    except yaml.YAMLError as exc:\n",
    "        raise ValueError(\"Error in configuration file:\", exc) from exc\n",
    "\n",
    "collate_fn = SpectroDataCollator()\n",
    "datapipes = load_all_datapipes(config[\"data_args\"])\n",
    "datapipes\n",
    "\n",
    "# dl = DataLoader(dataset=datapipe, batch_size=1, num_workers=1, collate_fn=collate_fn)\n",
    "# first = next(iter(dl))\n",
    "# n_sample = 0\n",
    "# for row in iter(dl):\n",
    "#     print(row)\n",
    "#     n_sample += 1\n",
    "#     if n_sample == 20:\n",
    "#         break\n",
    "# print(f\"{n_sample = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data/datasets/DEBUG/DEBUG_valid.jsonl', '{\"smiles\":\"CCNC(c1cnn2c1OCCC2)CC=C=C\",\"input_ids\":[26,27,28,30,38,39,40,41,42,43,44,45,46,50,51,52,53,54,55,56,58,59,60,63,64,65,66,67,68,69,70,72,75,77,78,79,80,81,82,83,87,89,90,91,92,93,94,95,104,105,106,107,108,109,110,116,117,118,119,120,121,122,129,131,132,133,136,145,146,147,148,149,150,159,160,161,162,163,164,165,166,173,174,175,176,177,188,189,190,200,204,205,206,218,219,232,233,234,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"labels\":[1233,224,283,11,70,20,284,21,70,20,378,21,12,261,32,38,32,38,0,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100,-100],\"decoder_attention_mask\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"attention_mask\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"position_ids\":[3,6,5,5,5,9,8,9,9,7,9,8,6,3,7,7,9,8,8,8,8,7,6,6,7,8,8,9,8,1,7,8,5,7,7,7,7,7,8,6,2,5,3,5,5,6,4,5,6,8,9,8,7,5,0,7,7,7,6,6,6,1,6,8,8,8,1,6,6,8,8,7,6,6,7,8,7,4,6,5,6,4,8,6,6,0,9,9,7,6,8,7,3,9,7,4,9,7,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(iter(datapipes[\"example\"][\"debug2\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datapipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m SequentialSampler\n\u001b[0;32m----> 4\u001b[0m pipe \u001b[39m=\u001b[39m dp\u001b[39m.\u001b[39miter\u001b[39m.\u001b[39mSampler(datapipe)\n\u001b[1;32m      5\u001b[0m \u001b[39m# len(list(iter(pipe)))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datapipe' is not defined"
     ]
    }
   ],
   "source": [
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data_utils importimportimportimportimportimportimport SequentialSampler\n",
    "\n",
    "pipe = dp.iter.Sampler(datapipe)\n",
    "# len(list(iter(pipe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_all_datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args ={\n",
    "    \"buffer_size\": 1000, # for shuffling in dataloader\n",
    "    \"datasets\": \n",
    "        {\"30M_rassp\": \n",
    "            {\"train_path\": \"data/datasets/30M_rassp/30M_rassp_train.jsonl\",\n",
    "            \"valid_path\": \"data/datasets/30M_rassp/30M_rassp_valid.jsonl\",\n",
    "            \"weight\": 1.0,\n",
    "            \"limit_val_split\": 1000,\n",
    "            \"limit_example_split\": 100},\n",
    "        \"nist\":\n",
    "            {\"train_path\": \"data/datasets/NIST/NIST_split_filip/train_<nist>.jsonl\",\n",
    "            \"valid_path\": \"data/datasets/NIST/NIST_split_filip/valid_<nist>.jsonl\",\n",
    "            \"weight\": 1.0,\n",
    "            \"limit_val_split\": 1000,\n",
    "            \"limit_example_split\": 100}\n",
    "        }\n",
    "}\n",
    "\n",
    "datapipes = load_all_datapipes(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[17, 18, 26,  ...,  2,  2,  2],\n",
      "        [30, 33, 34,  ...,  2,  2,  2],\n",
      "        [28, 33, 34,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [25, 26, 27,  ...,  2,  2,  2],\n",
      "        [37, 38, 39,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 3,  4,  4,  ..., -1, -1, -1],\n",
      "        [ 4,  2,  2,  ..., -1, -1, -1],\n",
      "        [ 2,  2,  3,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 0,  2,  6,  ..., -1, -1, -1],\n",
      "        [ 0,  5,  8,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  3,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,  495,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,   38,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  299,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[26, 27, 28,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2],\n",
      "        [38, 39, 40,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [30, 33, 39,  ...,  2,  2,  2],\n",
      "        [14, 15, 17,  ...,  2,  2,  2],\n",
      "        [28, 29, 30,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 5,  8,  7,  ..., -1, -1, -1],\n",
      "        [ 3,  1,  5,  ..., -1, -1, -1],\n",
      "        [ 1,  8,  7,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 1,  1,  8,  ..., -1, -1, -1],\n",
      "        [ 0,  4,  2,  ..., -1, -1, -1],\n",
      "        [ 3,  2,  6,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  469,  ..., -100, -100, -100],\n",
      "        [1233,  224,  276,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  285,  ..., -100, -100, -100],\n",
      "        [1233,  224,  286,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[33, 34, 39,  ...,  2,  2,  2],\n",
      "        [17, 27, 28,  ...,  2,  2,  2],\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        [16, 17, 18,  ...,  2,  2,  2],\n",
      "        [27, 29, 33,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 3,  1,  7,  ..., -1, -1, -1],\n",
      "        [ 2,  4,  4,  ..., -1, -1, -1],\n",
      "        [ 0,  3,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 7,  1,  8,  ..., -1, -1, -1],\n",
      "        [ 0,  2,  1,  ..., -1, -1, -1],\n",
      "        [ 1,  2,  4,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  404,  ..., -100, -100, -100],\n",
      "        [1233,  224,  327,  ..., -100, -100, -100],\n",
      "        [1233,  224,  292,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  286,  ..., -100, -100, -100],\n",
      "        [1233,  224,  378,  ..., -100, -100, -100],\n",
      "        [1233,  224,  469,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[15, 26, 28,  ...,  2,  2,  2],\n",
      "        [14, 15, 16,  ...,  2,  2,  2],\n",
      "        [26, 27, 28,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [26, 27, 28,  ...,  2,  2,  2],\n",
      "        [34, 39, 40,  ...,  2,  2,  2],\n",
      "        [27, 29, 30,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 4,  0,  3,  ..., -1, -1, -1],\n",
      "        [ 2,  5,  1,  ..., -1, -1, -1],\n",
      "        [ 2,  5,  4,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 4,  7,  6,  ..., -1, -1, -1],\n",
      "        [ 1,  6,  5,  ..., -1, -1, -1],\n",
      "        [ 1,  0,  0,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  286,  ..., -100, -100, -100],\n",
      "        [1233,  224,  404,  ..., -100, -100, -100],\n",
      "        [1233,  224,  306,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[15, 28, 30,  ...,  2,  2,  2],\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        [30, 33, 36,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [27, 30, 33,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  4,  4,  ..., -1, -1, -1],\n",
      "        [ 1,  2,  6,  ..., -1, -1, -1],\n",
      "        [ 3,  2,  3,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 2,  3,  0,  ..., -1, -1, -1],\n",
      "        [ 6,  5,  9,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  6,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,   38,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  495,  ..., -100, -100, -100],\n",
      "        [1233,  224,  276,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[14, 15, 16,  ...,  2,  2,  2],\n",
      "        [16, 17, 18,  ...,  2,  2,  2],\n",
      "        [26, 27, 29,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [28, 29, 30,  ...,  2,  2,  2],\n",
      "        [28, 33, 37,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  5,  3,  ..., -1, -1, -1],\n",
      "        [ 0,  2,  2,  ..., -1, -1, -1],\n",
      "        [ 0,  0,  4,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 5,  3,  4,  ..., -1, -1, -1],\n",
      "        [ 6,  4,  5,  ..., -1, -1, -1],\n",
      "        [ 7,  5,  9,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  281,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,   50,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[33, 34, 39,  ...,  2,  2,  2],\n",
      "        [28, 29, 30,  ...,  2,  2,  2],\n",
      "        [33, 38, 39,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [15, 17, 18,  ...,  2,  2,  2],\n",
      "        [15, 16, 26,  ...,  2,  2,  2],\n",
      "        [15, 17, 18,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  0,  8,  ..., -1, -1, -1],\n",
      "        [ 2,  3,  4,  ..., -1, -1, -1],\n",
      "        [ 0,  3,  8,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 1,  1,  3,  ..., -1, -1, -1],\n",
      "        [ 3,  1,  1,  ..., -1, -1, -1],\n",
      "        [ 3,  1,  2,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  538,  ..., -100, -100, -100],\n",
      "        [1233,  224,  303,  ..., -100, -100, -100],\n",
      "        [1233,  224,  404,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  471,  ..., -100, -100, -100],\n",
      "        [1233,  224,  327,  ..., -100, -100, -100],\n",
      "        [1233,  224,  407,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[27, 29, 30,  ...,  2,  2,  2],\n",
      "        [33, 34, 37,  ...,  2,  2,  2],\n",
      "        [30, 32, 33,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [29, 32, 33,  ...,  2,  2,  2],\n",
      "        [15, 16, 17,  ...,  2,  2,  2],\n",
      "        [33, 34, 36,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 3,  5,  3,  ..., -1, -1, -1],\n",
      "        [ 5,  3,  4,  ..., -1, -1, -1],\n",
      "        [ 4,  2,  3,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 2,  1,  0,  ..., -1, -1, -1],\n",
      "        [ 1,  2,  3,  ..., -1, -1, -1],\n",
      "        [ 5,  3,  2,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  281,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  404,  ..., -100, -100, -100],\n",
      "        [1233,  224,  281,  ..., -100, -100, -100],\n",
      "        [1233,  224,  285,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[26, 27, 28,  ...,  2,  2,  2],\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [14, 15, 16,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2],\n",
      "        [27, 29, 30,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 4,  7,  6,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  4,  ..., -1, -1, -1],\n",
      "        [ 4,  2,  7,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 5,  6,  3,  ..., -1, -1, -1],\n",
      "        [ 0,  4,  6,  ..., -1, -1, -1],\n",
      "        [ 2,  5,  8,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  275,  ..., -100, -100, -100],\n",
      "        [1233,  224,   38,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  297,  ..., -100, -100, -100],\n",
      "        [1233,  224,  263,  ..., -100, -100, -100],\n",
      "        [1233,  224,  281,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[28, 33, 34,  ...,  2,  2,  2],\n",
      "        [33, 34, 35,  ...,  2,  2,  2],\n",
      "        [26, 27, 28,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        [33, 34, 37,  ...,  2,  2,  2],\n",
      "        [29, 30, 33,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 3,  2,  3,  ..., -1, -1, -1],\n",
      "        [ 3,  5,  4,  ..., -1, -1, -1],\n",
      "        [ 4,  7,  5,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 3,  0,  4,  ..., -1, -1, -1],\n",
      "        [ 1,  1,  3,  ..., -1, -1, -1],\n",
      "        [ 5,  6,  0,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  276,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  295,  ..., -100, -100, -100],\n",
      "        [1233,  224,   50,  ..., -100, -100, -100],\n",
      "        [1233,  224,  275,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[14, 16, 17,  ...,  2,  2,  2],\n",
      "        [33, 34, 35,  ...,  2,  2,  2],\n",
      "        [15, 17, 18,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [28, 29, 30,  ...,  2,  2,  2],\n",
      "        [30, 33, 34,  ...,  2,  2,  2],\n",
      "        [19, 26, 27,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  3,  4,  ..., -1, -1, -1],\n",
      "        [ 2,  1,  0,  ..., -1, -1, -1],\n",
      "        [ 2,  1,  2,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 1,  2,  6,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  0,  ..., -1, -1, -1],\n",
      "        [ 0,  1,  6,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  364,  ..., -100, -100, -100],\n",
      "        [1233,  224,  285,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  285,  ..., -100, -100, -100],\n",
      "        [1233,  224,  265,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[33, 34, 37,  ...,  2,  2,  2],\n",
      "        [29, 33, 34,  ...,  2,  2,  2],\n",
      "        [28, 30, 33,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [27, 29, 33,  ...,  2,  2,  2],\n",
      "        [15, 16, 17,  ...,  2,  2,  2],\n",
      "        [33, 39, 40,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 3,  2,  0,  ..., -1, -1, -1],\n",
      "        [ 5,  2,  0,  ..., -1, -1, -1],\n",
      "        [ 3,  7,  4,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 3,  3,  1,  ..., -1, -1, -1],\n",
      "        [ 1,  0,  2,  ..., -1, -1, -1],\n",
      "        [ 5,  8,  7,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  469,  ..., -100, -100, -100],\n",
      "        [1233,  224,  283,  ..., -100, -100, -100],\n",
      "        [1233,  224,  478,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  278,  ..., -100, -100, -100],\n",
      "        [1233,  224,  295,  ..., -100, -100, -100],\n",
      "        [1233,  224,  445,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[ 26,  27,  28,  ...,   2,   2,   2],\n",
      "        [ 15,  29,  30,  ...,   2,   2,   2],\n",
      "        [ 27,  28,  36,  ...,   2,   2,   2],\n",
      "        ...,\n",
      "        [ 27,  29,  30,  ...,   2,   2,   2],\n",
      "        [ 33,  36,  38,  ..., 297, 298,   2],\n",
      "        [ 26,  27,  28,  ...,   2,   2,   2]]), 'position_ids': tensor([[ 2,  4,  2,  ..., -1, -1, -1],\n",
      "        [ 0,  4,  4,  ..., -1, -1, -1],\n",
      "        [ 5,  6,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 2,  1,  7,  ..., -1, -1, -1],\n",
      "        [ 3,  2,  4,  ...,  6,  2, -1],\n",
      "        [ 3,  7,  2,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  286,  ..., -100, -100, -100],\n",
      "        [1233,  224,  263,  ..., -100, -100, -100],\n",
      "        [1233,  224,  279,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  295,  ..., -100, -100, -100],\n",
      "        [1233,  224,   41,  ..., -100, -100, -100],\n",
      "        [1233,  224,   50,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[39, 40, 41,  ...,  2,  2,  2],\n",
      "        [33, 34, 37,  ...,  2,  2,  2],\n",
      "        [18, 26, 27,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [15, 16, 17,  ...,  2,  2,  2],\n",
      "        [19, 26, 27,  ...,  2,  2,  2],\n",
      "        [14, 15, 26,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 6,  4,  9,  ..., -1, -1, -1],\n",
      "        [ 2,  1,  1,  ..., -1, -1, -1],\n",
      "        [ 1,  1,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 2,  0,  1,  ..., -1, -1, -1],\n",
      "        [ 2,  3,  6,  ..., -1, -1, -1],\n",
      "        [ 0,  2,  1,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  263,  ..., -100, -100, -100],\n",
      "        [1233,  224,   70,  ..., -100, -100, -100],\n",
      "        [1233,  224,  327,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  495,  ..., -100, -100, -100],\n",
      "        [1233,  224,  286,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[28, 29, 30,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        [33, 36, 37,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        [17, 18, 26,  ...,  2,  2,  2],\n",
      "        [14, 15, 26,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  5,  7,  ..., -1, -1, -1],\n",
      "        [ 6,  5,  8,  ..., -1, -1, -1],\n",
      "        [ 1,  1,  3,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 1,  3,  6,  ..., -1, -1, -1],\n",
      "        [ 2,  0,  3,  ..., -1, -1, -1],\n",
      "        [ 0,  4,  5,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  400,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  327,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,  297,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[16, 17, 18,  ...,  2,  2,  2],\n",
      "        [27, 29, 31,  ...,  2,  2,  2],\n",
      "        [28, 31, 33,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [33, 34, 38,  ...,  2,  2,  2],\n",
      "        [33, 36, 37,  ...,  2,  2,  2],\n",
      "        [15, 19, 26,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 0,  3,  3,  ..., -1, -1, -1],\n",
      "        [ 0,  3,  5,  ..., -1, -1, -1],\n",
      "        [ 1,  3,  5,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 4,  2,  3,  ..., -1, -1, -1],\n",
      "        [ 1,  5,  1,  ..., -1, -1, -1],\n",
      "        [ 1,  1,  2,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  438,  ..., -100, -100, -100],\n",
      "        [1233,  224,  295,  ..., -100, -100, -100],\n",
      "        [1233,  224,  495,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  327,  ..., -100, -100, -100],\n",
      "        [1233,  224,  364,  ..., -100, -100, -100],\n",
      "        [1233,  224,  407,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[39, 40, 41,  ...,  2,  2,  2],\n",
      "        [15, 26, 28,  ...,  2,  2,  2],\n",
      "        [15, 26, 27,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [33, 34, 36,  ...,  2,  2,  2],\n",
      "        [33, 34, 35,  ...,  2,  2,  2],\n",
      "        [14, 15, 17,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 7,  6,  8,  ..., -1, -1, -1],\n",
      "        [ 0,  1,  3,  ..., -1, -1, -1],\n",
      "        [ 3,  1,  3,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 1,  0,  5,  ..., -1, -1, -1],\n",
      "        [ 3,  3,  6,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  3,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,   50,  ..., -100, -100, -100],\n",
      "        [1233,  224,  263,  ..., -100, -100, -100],\n",
      "        [1233,  224,  495,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  279,  ..., -100, -100, -100],\n",
      "        [1233,  224,  498,  ..., -100, -100, -100],\n",
      "        [1233,  224,  356,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[15, 17, 18,  ...,  2,  2,  2],\n",
      "        [27, 28, 38,  ...,  2,  2,  2],\n",
      "        [31, 33, 39,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [26, 27, 28,  ...,  2,  2,  2],\n",
      "        [14, 15, 26,  ...,  2,  2,  2],\n",
      "        [16, 17, 18,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 1,  2,  3,  ..., -1, -1, -1],\n",
      "        [ 3,  3,  1,  ..., -1, -1, -1],\n",
      "        [ 4,  3,  7,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 3,  6,  4,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  3,  ..., -1, -1, -1],\n",
      "        [ 0,  4,  4,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,   49,  ..., -100, -100, -100],\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,  306,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,   38,  ..., -100, -100, -100],\n",
      "        [1233,  224,  453,  ..., -100, -100, -100],\n",
      "        [1233,  224,  295,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[33, 34, 38,  ...,  2,  2,  2],\n",
      "        [28, 29, 30,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2],\n",
      "        [26, 27, 28,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 4,  0,  2,  ..., -1, -1, -1],\n",
      "        [ 5,  4,  6,  ..., -1, -1, -1],\n",
      "        [ 6,  2,  7,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 7,  1,  7,  ..., -1, -1, -1],\n",
      "        [ 1,  4,  1,  ..., -1, -1, -1],\n",
      "        [ 4,  7,  5,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,   50,  ..., -100, -100, -100],\n",
      "        [1233,  224,  265,  ..., -100, -100, -100],\n",
      "        [1233,  224,   50,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  285,  ..., -100, -100, -100],\n",
      "        [1233,  224,  278,  ..., -100, -100, -100],\n",
      "        [1233,  224,   50,  ..., -100, -100, -100]])}\n",
      "{'input_ids': tensor([[15, 26, 27,  ...,  2,  2,  2],\n",
      "        [26, 27, 28,  ...,  2,  2,  2],\n",
      "        [27, 28, 29,  ...,  2,  2,  2],\n",
      "        ...,\n",
      "        [33, 36, 37,  ...,  2,  2,  2],\n",
      "        [39, 40, 41,  ...,  2,  2,  2],\n",
      "        [15, 27, 28,  ...,  2,  2,  2]]), 'position_ids': tensor([[ 4,  5,  8,  ..., -1, -1, -1],\n",
      "        [ 0,  3,  4,  ..., -1, -1, -1],\n",
      "        [ 6,  4,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [ 3,  2,  1,  ..., -1, -1, -1],\n",
      "        [ 7,  5,  9,  ..., -1, -1, -1],\n",
      "        [ 3,  6,  6,  ..., -1, -1, -1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'decoder_attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1233,  224,  330,  ..., -100, -100, -100],\n",
      "        [1233,  224,  263,  ..., -100, -100, -100],\n",
      "        [1233,  224,  452,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [1233,  224,  261,  ..., -100, -100, -100],\n",
      "        [1233,  224,   38,  ..., -100, -100, -100],\n",
      "        [1233,  224,  276,  ..., -100, -100, -100]])}\n",
      "n_sample = 20\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(dataset=datapipes[\"valid\"][\"30M_rassp\"], batch_size=8, num_workers=1, collate_fn=collate_fn)\n",
    "first = next(iter(dl))\n",
    "n_sample = 0\n",
    "for row in iter(dl):\n",
    "    print(row)\n",
    "    n_sample += 1\n",
    "    if n_sample == 20:\n",
    "        break\n",
    "print(f\"{n_sample = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datapipes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datapipes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datapipes' is not defined"
     ]
    }
   ],
   "source": [
    "datapipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39mpad_token_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = range(10)[None:4]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "Chem.Draw.MolToImage(Chem.MolFromSmiles(\"C1=CC=CC=C1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xhajek9/miniconda3/envs/BARTtrain/bin/python'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the path of python behind this notebook\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit.Chem.Draw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw, MolFromSmiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import pathlib\n",
    "import shutil\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tokenizers\n",
    "import wandb\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Draw, MolFromSmiles, RDKFingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdkit.Chem.rdchem.Mol"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mol = Chem.MolFromSmiles(\"C1=CC=CC=C1\")\n",
    "type(pred_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m PredictionLogger \u001b[39m# compute_cos_simils\u001b[39;00m\n",
      "File \u001b[0;32m~/gc-ms_bart/callbacks.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39micecream\u001b[39;00m \u001b[39mimport\u001b[39;00m ic\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbart_spektro\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_bart_spektro\u001b[39;00m \u001b[39mimport\u001b[39;00m BartSpektroForConditionalGeneration\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_cos_simils\n\u001b[1;32m     24\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPredictionLogger\u001b[39;00m(transformers\u001b[39m.\u001b[39mTrainerCallback):\n\u001b[1;32m     25\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     26\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m         datasets: \u001b[39mlist\u001b[39m[torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset] \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mIterableDataset],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m         \u001b[39m# super().__init__(**kwargs) # not needed?\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from callbacks import PredictionLogger # compute_cos_simils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"a\":2, \"b\":3}\n",
    "x = a.get(\"c\", None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`Trainer` requires either a `model` or `model_init` argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformers\u001b[39m.\u001b[39;49mTrainer()\n",
      "File \u001b[0;32m~/miniconda3/envs/BARTtrain/lib/python3.8/site-packages/transformers/trainer.py:354\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    352\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_model_init()\n\u001b[1;32m    353\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39mif\u001b[39;00m model_init \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `Trainer` requires either a `model` or `model_init` argument"
     ]
    }
   ],
   "source": [
    "transformers.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:00<00:00, 1197.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 no smiles\n",
      "0 smiles too long\n",
      "0 spectra corrupted\n",
      "0 spectra w/ too high mz\n",
      "13 spectra w/ too many peaks\n",
      "totally 13 issues\n",
      "discarded 13/1000 spectra \n"
     ]
    }
   ],
   "source": [
    "from spectra_process_utils import msp_file_to_jsonl\n",
    "\n",
    "msp_file_to_jsonl(\"data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-35_10.msp\", \n",
    "                  \"tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model\",\n",
    "                  source_token=\"<rassp>\",\n",
    "                  path_jsonl=\"data/datasets/30M_rassp/rassp_gen/TEST.jsonl\", \n",
    "                  max_cumsum=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-17_67.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-36_43.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-47_90.msp'),\n",
       "        ...,\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-28_53.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-21_17.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-18_23.msp')],\n",
       "       [PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-06_54.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-15_11.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-26_82.msp'),\n",
       "        ...,\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-16_89.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-43_49.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-44_73.msp')],\n",
       "       [PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-28_86.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-24_13.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-15_05.msp'),\n",
       "        ...,\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-21_04.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-14_88.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-29_17.msp')],\n",
       "       ...,\n",
       "       [PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-15_17.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-14_87.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-47_01.msp'),\n",
       "        ...,\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-26_40.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-48_11.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-28_19.msp')],\n",
       "       [PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-35_52.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-04_44.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-06_46.msp'),\n",
       "        ...,\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-16_11.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-30_16.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-06_76.msp')],\n",
       "       [PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-24_31.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-21_92.msp'),\n",
       "        PosixPath('data/datasets/30M_rassp/rassp_gen/msps/30M_rassp-04_74.msp'),\n",
       "        ..., None, None, None]], dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    " \n",
    "num_processes = 32\n",
    "dir = \"data/datasets/30M_rassp/rassp_gen/msps/\"\n",
    "files = list(Path(dir).glob(\"*\"))\n",
    "pad_len = num_processes - (len(files)%num_processes)\n",
    "files += [None] * pad_len\n",
    "grouped_files = np.array(files).reshape(32, -1)\n",
    "# indexes = np.arange(len(files)).reshape(10, -1)\n",
    "grouped_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split rassp dataset to train test valid\n",
    "def data_split(df, train_test_valid_ratio: list):\n",
    "    \"\"\"split the df into train, test and valid sets\"\"\"\n",
    "    if sum(train_test_valid_ratio) != 1:\n",
    "        print(\"train_test_valid_ratio does not sum to 1\")\n",
    "        return None, None, None\n",
    "    train_set = df.sample(\n",
    "        frac=train_test_valid_ratio[0], random_state=42)\n",
    "    rest = df.drop(train_set.index)\n",
    "\n",
    "    test_set = rest.sample(frac=train_test_valid_ratio[1]/(train_test_valid_ratio[1]+train_test_valid_ratio[2]),\n",
    "                           random_state=42)\n",
    "    valid_set = rest.drop(test_set.index)\n",
    "    print(f\"train len: {len(train_set)}, test len: {len(test_set)}, valid len: {len(valid_set)}\")\n",
    "    return train_set, test_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data/datasets/30M_rassp/rassp_gen/jsonls/all.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 4322581, test len: 240144, valid len: 240143\n"
     ]
    }
   ],
   "source": [
    "train, test, valid = data_split(df, [0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json(\"data/datasets/30M_rassp/rassp_gen/train.jsonl\", orient=\"records\", lines=True)\n",
    "test.to_json(\"data/datasets/30M_rassp/rassp_gen/test.jsonl\", orient=\"records\", lines=True)\n",
    "valid.to_json(\"data/datasets/30M_rassp/rassp_gen/valid.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIST preprocess again (no overlaps in train - valid now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 2917/26365 [00:01<00:12, 1853.77it/s][17:32:43] Explicit valence for atom # 1 Cl, 7, is greater than permitted\n",
      " 12%|        | 3284/26365 [00:01<00:12, 1792.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26365/26365 [00:14<00:00, 1837.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 no smiles\n",
      "48 smiles too long\n",
      "1 spectra corrupted\n",
      "697 spectra w/ too high mz\n",
      "2267 spectra w/ too many peaks\n",
      "totally 3013 issues\n",
      "discarded 2693/26365 spectra \n"
     ]
    }
   ],
   "source": [
    "from spectra_process_utils import msp_file_to_jsonl\n",
    "\n",
    "dataset_path = Path(\"data/datasets/NIST/NIST_split_filip_no_overlap\")\n",
    "dataset_type = \"test\"\n",
    "source_token = \"<nist>\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                tokenizer_path,\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"train\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                tokenizer_path,\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\"\n",
    "                )\n",
    "dataset_type = \"valid\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                tokenizer_path,\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data' from '/home/xhajek9/gc-ms_bart/data.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/smi_preprocess_neims.py\n"
     ]
    }
   ],
   "source": [
    "!ls data/smi_preprocess_neims.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 2884/26365 [00:01<00:13, 1772.51it/s][23:11:24] Explicit valence for atom # 1 Cl, 7, is greater than permitted\n",
      " 12%|        | 3235/26365 [00:01<00:14, 1613.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26365/26365 [00:15<00:00, 1659.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 no smiles\n",
      "48 smiles too long\n",
      "1 spectra corrupted\n",
      "697 spectra w/ too high mz\n",
      "2267 spectra w/ too many peaks\n",
      "totally 3013 issues\n",
      "discarded 2693/26365 spectra \n"
     ]
    }
   ],
   "source": [
    "from spectra_process_utils import msp_file_to_jsonl\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = Path(\"data/datasets/NIST/NIST_split_filip_no_overlap\")\n",
    "dataset_type = \"test\"\n",
    "source_token = \"<nist>\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                Path(tokenizer_path),\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\",\n",
    "                keep_spectra=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 22784/26493 [00:12<00:01, 1860.79it/s][23:12:31] Explicit valence for atom # 0 B, 6, is greater than permitted\n",
      " 87%| | 22971/26493 [00:12<00:01, 1845.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 26493/26493 [00:14<00:00, 1823.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 no smiles\n",
      "39 smiles too long\n",
      "1 spectra corrupted\n",
      "663 spectra w/ too high mz\n",
      "2205 spectra w/ too many peaks\n",
      "totally 2908 issues\n",
      "discarded 2623/26493 spectra \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 39290/237455 [00:21<01:53, 1752.12it/s][23:18:36] SMILES Parse Error: syntax error while parsing: [CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10\n",
      "[23:18:36] SMILES Parse Error: Failed parsing SMILES '[CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10' for input: '[CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10'\n",
      " 17%|        | 39670/237455 [00:22<01:48, 1820.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 48490/237455 [00:26<01:42, 1848.68it/s][23:18:41] Explicit valence for atom # 2 C, 5, is greater than permitted\n",
      " 21%|        | 48853/237455 [00:27<01:48, 1743.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|       | 50866/237455 [00:28<01:43, 1809.83it/s][23:18:42] Explicit valence for atom # 1 C, 5, is greater than permitted\n",
      " 22%|       | 51235/237455 [00:28<01:43, 1806.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 64199/237455 [00:35<01:36, 1798.23it/s][23:18:50] Explicit valence for atom # 18 Br, 3, is greater than permitted\n",
      " 27%|       | 64577/237455 [00:35<01:33, 1840.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 66413/237455 [00:36<01:35, 1798.52it/s][23:18:51] Explicit valence for atom # 8 Br, 5, is greater than permitted\n",
      " 28%|       | 66772/237455 [00:37<01:37, 1755.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 71030/237455 [00:39<01:32, 1795.33it/s][23:18:53] SMILES Parse Error: syntax error while parsing: [CH-]1|2C|3=C|4C|5=C1|[Co]6789|2|3|4|5[CH-]%10B([CH-]6[CH-]7[CH-]8[CH-]9%10)c%11ccccc%11\n",
      "[23:18:53] SMILES Parse Error: Failed parsing SMILES '[CH-]1|2C|3=C|4C|5=C1|[Co]6789|2|3|4|5[CH-]%10B([CH-]6[CH-]7[CH-]8[CH-]9%10)c%11ccccc%11' for input: '[CH-]1|2C|3=C|4C|5=C1|[Co]6789|2|3|4|5[CH-]%10B([CH-]6[CH-]7[CH-]8[CH-]9%10)c%11ccccc%11'\n",
      " 30%|       | 71389/237455 [00:39<01:35, 1741.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 87064/237455 [00:49<02:11, 1144.59it/s][23:19:04] SMILES Parse Error: syntax error while parsing: [CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10\n",
      "[23:19:04] SMILES Parse Error: Failed parsing SMILES '[CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10' for input: '[CH-]1|2C|3=C|4C|5=C1|[Fe]6789|2|3|4|5|[N-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10'\n",
      " 37%|      | 87426/237455 [00:49<01:47, 1400.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 88170/237455 [00:50<01:26, 1733.38it/s][23:19:04] SMILES Parse Error: syntax error while parsing: [Ni]|1|2(|[S-]C(=C([S-]|1)c3ccccc3)c4ccccc4)|[S-]C(=C([S-]|2)c5ccccc5)c6ccccc6\n",
      "[23:19:04] SMILES Parse Error: Failed parsing SMILES '[Ni]|1|2(|[S-]C(=C([S-]|1)c3ccccc3)c4ccccc4)|[S-]C(=C([S-]|2)c5ccccc5)c6ccccc6' for input: '[Ni]|1|2(|[S-]C(=C([S-]|1)c3ccccc3)c4ccccc4)|[S-]C(=C([S-]|2)c5ccccc5)c6ccccc6'\n",
      " 37%|      | 88544/237455 [00:50<01:23, 1774.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 152240/237455 [01:25<00:45, 1883.34it/s][23:19:39] Explicit valence for atom # 7 Br, 3, is greater than permitted\n",
      " 64%|   | 152614/237455 [01:25<00:46, 1834.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 173723/237455 [01:37<00:35, 1776.69it/s][23:19:51] Explicit valence for atom # 0 B, 5, is greater than permitted\n",
      " 73%|  | 174104/237455 [01:37<00:34, 1841.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 186373/237455 [01:44<00:27, 1848.28it/s][23:19:58] Explicit valence for atom # 0 B, 5, is greater than permitted\n",
      " 79%|  | 186739/237455 [01:44<00:28, 1777.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 191396/237455 [01:47<00:25, 1775.24it/s][23:20:01] Explicit valence for atom # 0 C, 5, is greater than permitted\n",
      " 81%|  | 191781/237455 [01:47<00:24, 1845.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 192714/237455 [01:47<00:24, 1798.63it/s][23:20:02] Explicit valence for atom # 0 B, 5, is greater than permitted\n",
      " 81%| | 193086/237455 [01:48<00:24, 1786.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 217318/237455 [02:01<00:10, 1867.02it/s][23:20:15] SMILES Parse Error: syntax error while parsing: [CH-]1|2C|3=C|4C|5=C1|[Mn]6789|2|3|4|5[CH-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10\n",
      "[23:20:15] SMILES Parse Error: Failed parsing SMILES '[CH-]1|2C|3=C|4C|5=C1|[Mn]6789|2|3|4|5[CH-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10' for input: '[CH-]1|2C|3=C|4C|5=C1|[Mn]6789|2|3|4|5[CH-]%10[CH-]6[CH-]7[CH-]8[CH-]9%10'\n",
      " 92%|| 217688/237455 [02:01<00:10, 1804.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't be canonicalized due to Exception: Python argument types in\n",
      "    rdkit.Chem.rdmolfiles.MolToSmiles(NoneType, bool)\n",
      "did not match C++ signature:\n",
      "    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n",
      "    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 237455/237455 [02:12<00:00, 1791.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 no smiles\n",
      "406 smiles too long\n",
      "14 spectra corrupted\n",
      "6004 spectra w/ too high mz\n",
      "20226 spectra w/ too many peaks\n",
      "totally 26650 issues\n",
      "discarded 24049/237455 spectra \n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"valid\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                Path(tokenizer_path),\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\",\n",
    "                keep_spectra=True)\n",
    "\n",
    "dataset_type = \"train\"\n",
    "msp_file_to_jsonl(dataset_path / f\"{dataset_type}.msp\",\n",
    "                Path(tokenizer_path),\n",
    "                source_token,\n",
    "                path_jsonl=dataset_path / f\"{dataset_type}_{source_token}.jsonl\",\n",
    "                keep_spectra=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>position_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>smiles</th>\n",
       "      <th>labels</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>mz</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[41, 44, 46, 50, 51, 52, 53, 61, 62, 63, 64, 6...</td>\n",
       "      <td>[0, 0, 0, 7, 7, 4, 0, 0, 4, 7, 6, 3, 0, 0, 4, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>O=[N+]([O-])c1cccc(N=Cc2ccccc2)c1</td>\n",
       "      <td>[1234, 224, 50, 565, 49, 419, 50, 365, 70, 20,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[41.0, 44.0, 46.0, 50.0, 51.0, 52.0, 53.0, 61....</td>\n",
       "      <td>[1.0, 2.0, 4.0, 262.76, 326.71, 61.94, 5.0, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 38, 39, 40, 41, 4...</td>\n",
       "      <td>[1, 3, 7, 4, 8, 2, 1, 3, 8, 4, 9, 6, 9, 4, 5, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CCCCCCCCCCCCCC(C)OC(=O)c1cccs1</td>\n",
       "      <td>[1234, 224, 1024, 487, 11, 38, 12, 286, 260, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 38....</td>\n",
       "      <td>[11.99, 26.98, 290.74, 45.96, 474.57, 18.98, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[27, 29, 30, 34, 39, 40, 41, 42, 43, 44, 51, 5...</td>\n",
       "      <td>[6, 7, 5, 0, 6, 0, 7, 5, 1, 6, 3, 0, 4, 1, 7, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CC=CC=CC=CN(CC)CC</td>\n",
       "      <td>[1234, 224, 261, 32, 261, 32, 261, 32, 263, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[27.0, 29.0, 30.0, 34.0, 39.0, 40.0, 41.0, 42....</td>\n",
       "      <td>[144.87, 283.74, 86.92, 7.99, 121.89, 0.5, 206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[69, 75, 76, 77, 78, 98, 101, 102, 103, 104, 1...</td>\n",
       "      <td>[7, 5, 6, 6, 3, 4, 4, 6, 6, 4, 5, 8, 9, 8, 6, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CC(=O)N1c2ccccc2C=CC1C1C=Nc2ccccc2N1</td>\n",
       "      <td>[1234, 224, 261, 260, 50, 12, 49, 20, 70, 21, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[69.0, 75.0, 76.0, 77.0, 78.0, 98.0, 101.0, 10...</td>\n",
       "      <td>[239.78, 79.93, 149.86, 159.86, 39.96, 49.95, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[27, 28, 29, 38, 39, 40, 41, 42, 51, 52, 53, 5...</td>\n",
       "      <td>[4, 6, 3, 2, 6, 5, 3, 2, 3, 4, 4, 6, 7, 2, 3, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CC1=CC(=O)C(N)=C(C)C1=O</td>\n",
       "      <td>[1234, 224, 261, 20, 32, 261, 260, 50, 12, 38,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[27.0, 28.0, 29.0, 38.0, 39.0, 40.0, 41.0, 42....</td>\n",
       "      <td>[49.95, 136.88, 26.98, 23.98, 135.88, 76.93, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23865</th>\n",
       "      <td>[15, 17, 18, 26, 27, 28, 29, 30, 31, 38, 39, 4...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 4, 5, 6, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CCCCCOC(=O)CCCC(=O)OCc1ccc(OC)c([N+](=O)[O-])c1</td>\n",
       "      <td>[1234, 224, 876, 260, 50, 12, 296, 260, 50, 12...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[15.0, 17.0, 18.0, 26.0, 27.0, 28.0, 29.0, 30....</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 11.99, 8.99, 23.98, 2.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23866</th>\n",
       "      <td>[26, 27, 28, 29, 36, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "      <td>[0, 1, 1, 3, 0, 0, 2, 0, 5, 0, 3, 0, 0, 1, 3, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CC(C)(C)C1CCC(OC(=O)CCC(=O)OC2CCC(C(C)(C)C)CC2...</td>\n",
       "      <td>[1234, 224, 261, 11, 38, 289, 38, 12, 38, 20, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[26.0, 27.0, 28.0, 29.0, 36.0, 38.0, 39.0, 40....</td>\n",
       "      <td>[2.0, 10.99, 11.99, 29.97, 7.99, 2.0, 16.98, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23867</th>\n",
       "      <td>[15, 26, 27, 28, 29, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "      <td>[2, 1, 6, 3, 7, 1, 6, 2, 8, 3, 9, 3, 1, 1, 3, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CCC(C)(C)C</td>\n",
       "      <td>[1234, 224, 276, 11, 38, 289, 38, 12, 38, 0, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[15.0, 26.0, 27.0, 28.0, 29.0, 38.0, 39.0, 40....</td>\n",
       "      <td>[14.99, 11.99, 188.83, 24.98, 332.7, 9.99, 154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23868</th>\n",
       "      <td>[2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 24, 25...</td>\n",
       "      <td>[1, 1, 2, 4, 7, 0, 0, 0, 0, 0, 0, 1, 4, 7, 5, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>C=C(C)C(OC)OC(C)(C)C</td>\n",
       "      <td>[1234, 224, 38, 32, 38, 11, 38, 12, 38, 11, 28...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0...</td>\n",
       "      <td>[10.99, 12.99, 18.98, 41.96, 217.8, 5.0, 3.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23869</th>\n",
       "      <td>[67, 69, 71, 79, 81, 83, 91, 93, 95, 97, 105, ...</td>\n",
       "      <td>[5, 7, 4, 4, 6, 5, 5, 4, 6, 4, 4, 5, 6, 4, 4, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>CC(C)CCCC(C)C1CCC2(C)C3=C(C(=O)CC12C)C1(C)CCCC...</td>\n",
       "      <td>[1234, 224, 261, 11, 38, 12, 296, 11, 38, 12, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[67.0, 69.0, 71.0, 79.0, 81.0, 83.0, 91.0, 93....</td>\n",
       "      <td>[79.93, 302.73, 69.94, 49.95, 140.87, 110.9, 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23870 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input_ids  \\\n",
       "0      [41, 44, 46, 50, 51, 52, 53, 61, 62, 63, 64, 6...   \n",
       "1      [25, 26, 27, 28, 29, 30, 31, 38, 39, 40, 41, 4...   \n",
       "2      [27, 29, 30, 34, 39, 40, 41, 42, 43, 44, 51, 5...   \n",
       "3      [69, 75, 76, 77, 78, 98, 101, 102, 103, 104, 1...   \n",
       "4      [27, 28, 29, 38, 39, 40, 41, 42, 51, 52, 53, 5...   \n",
       "...                                                  ...   \n",
       "23865  [15, 17, 18, 26, 27, 28, 29, 30, 31, 38, 39, 4...   \n",
       "23866  [26, 27, 28, 29, 36, 38, 39, 40, 41, 42, 43, 4...   \n",
       "23867  [15, 26, 27, 28, 29, 38, 39, 40, 41, 42, 43, 4...   \n",
       "23868  [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 24, 25...   \n",
       "23869  [67, 69, 71, 79, 81, 83, 91, 93, 95, 97, 105, ...   \n",
       "\n",
       "                                            position_ids  \\\n",
       "0      [0, 0, 0, 7, 7, 4, 0, 0, 4, 7, 6, 3, 0, 0, 4, ...   \n",
       "1      [1, 3, 7, 4, 8, 2, 1, 3, 8, 4, 9, 6, 9, 4, 5, ...   \n",
       "2      [6, 7, 5, 0, 6, 0, 7, 5, 1, 6, 3, 0, 4, 1, 7, ...   \n",
       "3      [7, 5, 6, 6, 3, 4, 4, 6, 6, 4, 5, 8, 9, 8, 6, ...   \n",
       "4      [4, 6, 3, 2, 6, 5, 3, 2, 3, 4, 4, 6, 7, 2, 3, ...   \n",
       "...                                                  ...   \n",
       "23865  [0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 4, 5, 6, ...   \n",
       "23866  [0, 1, 1, 3, 0, 0, 2, 0, 5, 0, 3, 0, 0, 1, 3, ...   \n",
       "23867  [2, 1, 6, 3, 7, 1, 6, 2, 8, 3, 9, 3, 1, 1, 3, ...   \n",
       "23868  [1, 1, 2, 4, 7, 0, 0, 0, 0, 0, 0, 1, 4, 7, 5, ...   \n",
       "23869  [5, 7, 4, 4, 6, 5, 5, 4, 6, 4, 4, 5, 6, 4, 4, ...   \n",
       "\n",
       "                                          attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "23865  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23866  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23867  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23868  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23869  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                  smiles  \\\n",
       "0                      O=[N+]([O-])c1cccc(N=Cc2ccccc2)c1   \n",
       "1                         CCCCCCCCCCCCCC(C)OC(=O)c1cccs1   \n",
       "2                                      CC=CC=CC=CN(CC)CC   \n",
       "3                   CC(=O)N1c2ccccc2C=CC1C1C=Nc2ccccc2N1   \n",
       "4                                CC1=CC(=O)C(N)=C(C)C1=O   \n",
       "...                                                  ...   \n",
       "23865    CCCCCOC(=O)CCCC(=O)OCc1ccc(OC)c([N+](=O)[O-])c1   \n",
       "23866  CC(C)(C)C1CCC(OC(=O)CCC(=O)OC2CCC(C(C)(C)C)CC2...   \n",
       "23867                                         CCC(C)(C)C   \n",
       "23868                               C=C(C)C(OC)OC(C)(C)C   \n",
       "23869  CC(C)CCCC(C)C1CCC2(C)C3=C(C(=O)CC12C)C1(C)CCCC...   \n",
       "\n",
       "                                                  labels  \\\n",
       "0      [1234, 224, 50, 565, 49, 419, 50, 365, 70, 20,...   \n",
       "1      [1234, 224, 1024, 487, 11, 38, 12, 286, 260, 5...   \n",
       "2      [1234, 224, 261, 32, 261, 32, 261, 32, 263, 11...   \n",
       "3      [1234, 224, 261, 260, 50, 12, 49, 20, 70, 21, ...   \n",
       "4      [1234, 224, 261, 20, 32, 261, 260, 50, 12, 38,...   \n",
       "...                                                  ...   \n",
       "23865  [1234, 224, 876, 260, 50, 12, 296, 260, 50, 12...   \n",
       "23866  [1234, 224, 261, 11, 38, 289, 38, 12, 38, 20, ...   \n",
       "23867  [1234, 224, 276, 11, 38, 289, 38, 12, 38, 0, -...   \n",
       "23868  [1234, 224, 38, 32, 38, 11, 38, 12, 38, 11, 28...   \n",
       "23869  [1234, 224, 261, 11, 38, 12, 296, 11, 38, 12, ...   \n",
       "\n",
       "                                  decoder_attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "23865  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23866  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23867  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "23868  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "23869  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                      mz  \\\n",
       "0      [41.0, 44.0, 46.0, 50.0, 51.0, 52.0, 53.0, 61....   \n",
       "1      [25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 38....   \n",
       "2      [27.0, 29.0, 30.0, 34.0, 39.0, 40.0, 41.0, 42....   \n",
       "3      [69.0, 75.0, 76.0, 77.0, 78.0, 98.0, 101.0, 10...   \n",
       "4      [27.0, 28.0, 29.0, 38.0, 39.0, 40.0, 41.0, 42....   \n",
       "...                                                  ...   \n",
       "23865  [15.0, 17.0, 18.0, 26.0, 27.0, 28.0, 29.0, 30....   \n",
       "23866  [26.0, 27.0, 28.0, 29.0, 36.0, 38.0, 39.0, 40....   \n",
       "23867  [15.0, 26.0, 27.0, 28.0, 29.0, 38.0, 39.0, 40....   \n",
       "23868  [2.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0...   \n",
       "23869  [67.0, 69.0, 71.0, 79.0, 81.0, 83.0, 91.0, 93....   \n",
       "\n",
       "                                               intensity  \n",
       "0      [1.0, 2.0, 4.0, 262.76, 326.71, 61.94, 5.0, 5....  \n",
       "1      [11.99, 26.98, 290.74, 45.96, 474.57, 18.98, 1...  \n",
       "2      [144.87, 283.74, 86.92, 7.99, 121.89, 0.5, 206...  \n",
       "3      [239.78, 79.93, 149.86, 159.86, 39.96, 49.95, ...  \n",
       "4      [49.95, 136.88, 26.98, 23.98, 135.88, 76.93, 3...  \n",
       "...                                                  ...  \n",
       "23865  [1.0, 1.0, 1.0, 1.0, 11.99, 8.99, 23.98, 2.0, ...  \n",
       "23866  [2.0, 10.99, 11.99, 29.97, 7.99, 2.0, 16.98, 3...  \n",
       "23867  [14.99, 11.99, 188.83, 24.98, 332.7, 9.99, 154...  \n",
       "23868  [10.99, 12.99, 18.98, 41.96, 217.8, 5.0, 3.0, ...  \n",
       "23869  [79.93, 302.73, 69.94, 49.95, 140.87, 110.9, 7...  \n",
       "\n",
       "[23870 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_json(\"data/datasets/NIST/NIST_split_filip_no_overlap/valid_<nist>.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename a wandb group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "api = wandb.Api()\n",
    "for r in api.runs(\"hajekad/BART_for_gcms\", filters={\"group\": \"finetune\"}):\n",
    "    r.group = \"finetune_dataleak\"\n",
    "    r.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEIMSpy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
