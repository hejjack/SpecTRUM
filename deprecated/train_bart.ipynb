{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6b4bd42",
   "metadata": {},
   "source": [
    "## My train using Learner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1625c436",
   "metadata": {},
   "source": [
    "Training BART with a changed script from the BERT project (adding position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28000744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39021b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/scratch.ssd/ahajek/tmp/.conda/envs/BARTtrain/bin/python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb057c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.21.0\n",
    "# !pip install ipywidgets --user\n",
    "# !pip install pandas==1.4.3\n",
    "# !pip install wandb\n",
    "# !pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0964f74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, BartConfig, BartForConditionalGeneration\n",
    "# from transformers.file_utils import logging\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "# custom veci\n",
    "from data_utils import SpectroDataset, SpectroDataCollator\n",
    "sys.path.append('data')\n",
    "sys.path.append('bart_spektro')\n",
    "from modeling_bart_spektro import BartSpektroForConditionalGeneration\n",
    "from configuration_bart_spektro import BartSpektroConfig\n",
    "# from data_preprocess1 import print_args\n",
    "from bart_spektro_tokenizer import BartSpektroTokenizer\n",
    "from bart_spektro_trainer import BartSpectroTrainer\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# from utils import add_special_tokens, generate_sample, sample_seq, set_seed, top_k_top_p_filtering, print_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59761cef",
   "metadata": {},
   "source": [
    "#### Setting basic training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2d58a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ncpus=16\n",
    "ngpus=4 #4\n",
    "bs = 32 #32 for small model, 16 or 28(slower) for 707M model\n",
    "gas = 1 # int(16/ngpus)\n",
    "print(\"GA:\", gas)\n",
    "which_bart = \"spektro\" #\"original\" # \"spektro\"\n",
    "data_type = \"30M\" # \"30M\"\n",
    "tokenizer_type = \"_bbpe_1M\" # for spektro tokenizer use \"\"\n",
    "\n",
    "\n",
    "SEQ_LEN = 200\n",
    "num_epochs = 200 # 10 (BARTy se trenovaly 10 epoch celkem) # int(os.environ[\"TOTAL_EPOCHS\"])\n",
    "debug_data_len =  None #{\"train\": 4000, \"valid\": 4000} # None for no debug   \n",
    "resume_training = False # True # bool(int(os.environ[\"RESUME_TRAINING\"]))\n",
    "resume_wandb_id = \"\" # 2yqhwcas\" # \"191h62zs\" #pass # \"\"\n",
    "eval_steps = 10000 # 6500 # 9000 # 30M steps per epoch : 87617 => 9000 steps should be roughly 8hours -> eval_steps, save_steps\n",
    "save_steps = eval_steps\n",
    "log_steps = 1 # 5\n",
    "eval_log_predictions_size = 100 # 100\n",
    "eval_subset_size = 200000 # 100000 #100k\n",
    "\n",
    "model = None # aby nebyl nedefinovany\n",
    "\n",
    "# find the last checkpoint\n",
    "# checkpoints_pth = \"./checkpoints/*\"\n",
    "# runs = sorted(glob.glob(checkpoints_pth))\n",
    "# checkpoints =  glob.glob(runs[-1]+\"/checkpoint-*\")\n",
    "# checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "# load_checkpoint = checkpoints[-1]\n",
    "# save_checkpoint = runs[-1]\n",
    "# print(f\"last checkpoint: {load_checkpoint}\")\n",
    "\n",
    "# load_checkpoint = \"./checkpoints/bart_2022-06-28-10_02_31/checkpoint-18128\"\n",
    "\n",
    "# # LOAD CHECKPOINT FROM\n",
    "# prefered_run = \"./checkpoints/bart_2022-06-28-10_02_31_bigdata/\"\n",
    "# checkpoints =  glob.glob(prefered_run+\"checkpoint-*\")\n",
    "# checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "# load_checkpoint = checkpoints[-1]\n",
    "# print(f\"last checkpoint: {load_checkpoint}\") # the newest from particular folder\n",
    "\n",
    "# # SAVE CHECKPOINTS TO  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# save_checkpoint = \"bart_2022-06-28-10_02_31_bigdata/\" # if None -> saves to newly generated folder bart_{now}\n",
    "save_checkpoint = None #\"bart_2022-06-28-10_02_31_bigdata\" # if None -> saves to newly generated folder bart_{now}\n",
    "\n",
    "print(\"save checkpoint to:\", save_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d822c782",
   "metadata": {},
   "source": [
    "#### Setting all training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37fc6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "GPU-0d6541df-b947-e2af-cfe6-d9b6d8fc8d5a\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"PBS_NGPUS\"]\n",
    "!echo $PBS_NGPUS\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0fd1fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--wandb\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptinal logging via Weights&Biases\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--wandb-resume\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39mresume_training, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume logging via wandb, needs an valid run ID set in args.wandb-id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--wandb-id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_id(), help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess unique wandb ID used for resumin the training process\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--tensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptinal logging via TensorBoard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'util'"
     ]
    }
   ],
   "source": [
    "now = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "now = now.replace(\":\",\"_\").replace(\" \", \"-\")\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--gradient-accumulation-steps\",default=gas, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch-size\",default=bs, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--warmup\",default=500, type=int,  help=\"warmup steps for learning rate\")\n",
    "parser.add_argument(\"--weight-decay\",default=0.01, type=float,  help=\"weight decay rate parameter\")\n",
    "parser.add_argument(\"--n-gpu\",default=ngpus, type=int, required=False, help=\"no of gpu available\")\n",
    "parser.add_argument(\"--fairscale\",default=\"\", type=str, required=False, choices=[\"simple\", \"zero_dp_2\", \"zero_dp_3\"], help=\"GPU paralellization via Fairscale, \" +\n",
    "                    \"more info in HuggingFace's Trainer docs\")\n",
    "parser.add_argument(\"--deepspeed\", default=None, type=str, required=False, help=\"GPU paralellization via Deepspeed, the value is the location of DeepSpeed json config file; \" +\n",
    "                    \"more info in HuggingFace's Trainer docs\")\n",
    "parser.add_argument(\"--num-workers\",default=ncpus, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--num-train-epochs\",default=num_epochs, type=int,  help=\"number of training epochs\")\n",
    "parser.add_argument(\"--output-dir\",default='./output', type=str,  help=\"Path to save evaluation results\")\n",
    "parser.add_argument(\"--save-dir\",default='./checkpoints', type=str,  help=\"Path to save trained model\")\n",
    "parser.add_argument(\"--save-name\", type=str, default=f'bart_{now}', help=\"Name of the model, used for saves\")\n",
    "parser.add_argument(\"--load-checkpoint\", type=str, default='', help=\"Path to the checkpoint to resume training\")\n",
    "parser.add_argument(\"--config-dir\",default='./configs', type=str,  help=\"Path to save config files of checkpoints\")\n",
    "parser.add_argument(\"--fp16\",default=True, type=bool, required=False, help=\"whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "parser.add_argument(\"--max-grad-norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--train-data-path\",default=f'./data/trial_set/{data_type}{tokenizer_type}_bart_prepared_data_train.pkl', type=str, help=\"Path to jsonl train dataset\")\n",
    "parser.add_argument(\"--valid-data-path\",default=f'./data/trial_set/{data_type}{tokenizer_type}_bart_prepared_data_valid.pkl', type=str, help=\"Path to jsonl validation dataset\")\n",
    "parser.add_argument(\"--log-steps\",default=log_steps, type=int,  help=\"number of steps between logs\")\n",
    "parser.add_argument(\"--eval-steps\",default=eval_steps, type=int,  help=\"number of steps between evaluations\")\n",
    "parser.add_argument(\"--tokenizer-path\",default='/storage/brno6/home/ahajek/nic', type=str, help=\"location of the desied tokenizer (special sep token will be added))\")\n",
    "parser.add_argument(\"--model-path\",default='./checkpoints/NECO', type=str, help=\"location of the desired model to finetune\")\n",
    "parser.add_argument(\"--wandb\", action='store_true', default=True, help=\"optinal logging via Weights&Biases\")\n",
    "parser.add_argument(\"--wandb-resume\", action='store_true', default=resume_training, help=\"resume logging via wandb, needs an valid run ID set in args.wandb-id\")\n",
    "parser.add_argument(\"--wandb-id\", type=str, default=wandb.util.generate_id(), help=\"Process unique wandb ID used for resumin the training process\")\n",
    "parser.add_argument(\"--tensorboard\", action='store_true', default=False, help=\"optinal logging via TensorBoard\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# extended outputs\n",
    "# logging.set_verbosity_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd49c948",
   "metadata": {},
   "source": [
    "#### Loading tokenizer, data, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61780e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7d5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER\n",
    "tokenizer = Tokenizer.from_file(f\"./tokenizer/bbpe_tokenizer/bart{tokenizer_type}_tokenizer.model\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # surpressing a warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e91242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART CONIGURATION\n",
    "if which_bart == \"spektro\":\n",
    "    config = BartSpektroConfig(vocab_size = len(tokenizer.get_vocab()),\n",
    "                                 max_position_embeddings = SEQ_LEN,\n",
    "                                 max_length = SEQ_LEN,\n",
    "                                 min_len = 0,\n",
    "                                 encoder_layers = 12, # 24\n",
    "                                 encoder_ffn_dim = 4096,\n",
    "                                 encoder_attention_heads = 16,\n",
    "                                 decoder_layers = 12,  # 24\n",
    "                                 decoder_ffn_dim = 4096,\n",
    "                                 decoder_attention_heads = 16,\n",
    "                                 encoder_layerdrop = 0.0,\n",
    "                                 decoder_layerdrop = 0.0,\n",
    "                                 activation_function = 'gelu',\n",
    "                                 d_model = 1024,\n",
    "                                 dropout = 0.2,\n",
    "                                 attention_dropout = 0.0,\n",
    "                                 activation_dropout = 0.0,\n",
    "                                 init_std = 0.02,\n",
    "                                 classifier_dropout = 0.0,\n",
    "                                 scale_embedding = False,\n",
    "                                 use_cache = True,\n",
    "                                 pad_token_id = 2,\n",
    "                                 bos_token_id = 3,\n",
    "                                 eos_token_id = 0,\n",
    "                                 is_encoder_decoder = True,\n",
    "                                 decoder_start_token_id = 3,\n",
    "                                 forced_eos_token_id = 0,\n",
    "                                 max_log_id=9)\n",
    "\n",
    "if which_bart == \"original\":\n",
    "    config = BartConfig(vocab_size = len(tokenizer.get_vocab()),\n",
    "                                 max_position_embeddings = SEQ_LEN,\n",
    "                                 max_length = SEQ_LEN,\n",
    "                                 min_len = 0,\n",
    "                                 encoder_layers = 12,\n",
    "                                 encoder_ffn_dim = 4096,\n",
    "                                 encoder_attention_heads = 16,\n",
    "                                 decoder_layers = 12,\n",
    "                                 decoder_ffn_dim = 4096,\n",
    "                                 decoder_attention_heads = 16,\n",
    "                                 encoder_layerdrop = 0.0,\n",
    "                                 decoder_layerdrop = 0.0,\n",
    "                                 activation_function = 'gelu',\n",
    "                                 d_model = 1024,\n",
    "                                 dropout = 0.2,\n",
    "                                 attention_dropout = 0.0,\n",
    "                                 activation_dropout = 0.0,\n",
    "                                 init_std = 0.02,\n",
    "                                 classifier_dropout = 0.0,\n",
    "                                 scale_embedding = False,\n",
    "                                 use_cache = True,\n",
    "                                 pad_token_id = 2,\n",
    "                                 bos_token_id = 3,\n",
    "                                 eos_token_id = 0,\n",
    "                                 is_encoder_decoder = True,\n",
    "                                 decoder_start_token_id = 3,\n",
    "                                 forced_eos_token_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bef446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE NAME\n",
    "args.save_name = save_checkpoint if save_checkpoint else args.save_name # change args only if user specifies different save name in the initial cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0299d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "args.train_data_path = f\"./data/trial_set/{data_type}_train.pkl\"\n",
    "args.valid_data_path = f\"./data/trial_set/{data_type}_valid.pkl\"\n",
    "\n",
    "train_data = SpectroDataset(args.train_data_path, original=which_bart==\"original\")\n",
    "valid_data = SpectroDataset(args.valid_data_path, original=which_bart==\"original\")\n",
    "\n",
    "if debug_data_len is not None:\n",
    "    train_data.data = train_data.data[:debug_data_len[\"train\"]]\n",
    "    valid_data.data = valid_data.data[:debug_data_len[\"valid\"]]\n",
    "\n",
    "# clean memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "149ce363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kratkej vypis :D\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "if which_bart == \"original\":\n",
    "    model = BartForConditionalGeneration(config)\n",
    "else:\n",
    "    model = BartSpektroForConditionalGeneration(config)\n",
    "# model = BartForConditionalGeneration.from_pretrained(args.model_path)\n",
    "model.to(args.device)\n",
    "\n",
    "print(\"kratkej vypis :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed49097",
   "metadata": {},
   "outputs": [],
   "source": [
    "if which_bart == \"original\":\n",
    "    try:\n",
    "        train_data.data.drop(columns=[\"position_ids\"], inplace=True)\n",
    "        valid_data.data.drop(columns=[\"position_ids\"], inplace=True)\n",
    "        train_data.data.drop(columns=[\"decoder_input_ids\"], inplace=True)\n",
    "        valid_data.data.drop(columns=[\"decoder_input_ids\"], inplace=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "710b2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>encoder_attention_mask</th>\n",
       "      <th>position_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CCNC(c1cnn2c1OCCC2)CC=C=C</td>\n",
       "      <td>[26, 27, 28, 30, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[1233, 224, 283, 11, 70, 20, 284, 21, 70, 20, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 6, 5, 5, 5, 9, 8, 9, 9, 7, 9, 8, 6, 3, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CCC1CN(C(=O)C)C(CN1C(=O)c1nc2ccccc2cc1Cl)CC</td>\n",
       "      <td>[36, 38, 39, 40, 41, 42, 43, 44, 49, 50, 51, 5...</td>\n",
       "      <td>[1233, 224, 276, 20, 263, 11, 38, 260, 50, 12,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 0, 6, 5, 7, 8, 8, 6, 3, 1, 5, 4, 6, 6, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>OC1CNCC1N=c1[nH]c(ncc1[N+](=O)[O-])C(C)(C)C</td>\n",
       "      <td>[30, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[1233, 224, 286, 20, 400, 20, 49, 32, 70, 20, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 1, 1, 3, 3, 7, 6, 9, 9, 8, 9, 6, 5, 4, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>COC(=O)Cc1ccc(c(c1)[N+](=O)[O-])NCCc1cnc2n(c1)...</td>\n",
       "      <td>[33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
       "      <td>[1233, 224, 285, 260, 50, 12, 279, 20, 280, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 7, 0, 6, 8, 6, 6, 6, 8, 7, 5, 4, 8, 9, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>O=C(c1ccc2c(c1)n[nH]n2)Nc1ccc(cc1)NS(=O)(=O)c1...</td>\n",
       "      <td>[33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[1233, 224, 50, 32, 38, 11, 70, 20, 280, 21, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 2, 2, 1, 3, 7, 5, 6, 4, 6, 6, 6, 0, 2, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796</th>\n",
       "      <td>CC(=CCNCC1(CCOCC1)NC(=O)c1c(C)nsc1C)C</td>\n",
       "      <td>[33, 39, 40, 41, 42, 43, 44, 45, 51, 52, 53, 5...</td>\n",
       "      <td>[1233, 224, 261, 260, 374, 20, 11, 288, 20, 12...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 8, 7, 9, 8, 9, 8, 6, 7, 7, 8, 6, 8, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6807</th>\n",
       "      <td>CN(C(C(=O)N1CCC1)C)CC1COCCN1</td>\n",
       "      <td>[15, 18, 27, 28, 32, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "      <td>[1233, 224, 263, 11, 38, 11, 38, 260, 50, 12, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[2, 0, 1, 3, 1, 1, 5, 5, 8, 8, 8, 8, 6, 4, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6830</th>\n",
       "      <td>COCC1(OC)CCCN(CC1)CCC(=O)C</td>\n",
       "      <td>[14, 15, 18, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n",
       "      <td>[1233, 224, 306, 20, 11, 286, 12, 299, 11, 261...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 4, 3, 3, 6, 6, 6, 6, 6, 3, 5, 7, 6, 9, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>O=C(c1cccc(c1)CNC(=O)c1c(C)ccnc1O)N=c1cc[nH]cc1</td>\n",
       "      <td>[33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[1233, 224, 50, 32, 38, 11, 70, 20, 338, 11, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 3, 3, 3, 6, 9, 7, 7, 7, 7, 8, 3, 0, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>CC(C1CC1NC(=O)c1c(Br)nc(n1C)C)C</td>\n",
       "      <td>[30, 38, 39, 40, 41, 42, 43, 44, 45, 50, 51, 5...</td>\n",
       "      <td>[1233, 224, 261, 11, 38, 20, 261, 20, 265, 260...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[5, 3, 8, 7, 9, 9, 9, 8, 6, 6, 7, 7, 7, 7, 8, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  \\\n",
       "16                            CCNC(c1cnn2c1OCCC2)CC=C=C   \n",
       "19          CCC1CN(C(=O)C)C(CN1C(=O)c1nc2ccccc2cc1Cl)CC   \n",
       "50          OC1CNCC1N=c1[nH]c(ncc1[N+](=O)[O-])C(C)(C)C   \n",
       "59    COC(=O)Cc1ccc(c(c1)[N+](=O)[O-])NCCc1cnc2n(c1)...   \n",
       "64    O=C(c1ccc2c(c1)n[nH]n2)Nc1ccc(cc1)NS(=O)(=O)c1...   \n",
       "...                                                 ...   \n",
       "6796              CC(=CCNCC1(CCOCC1)NC(=O)c1c(C)nsc1C)C   \n",
       "6807                       CN(C(C(=O)N1CCC1)C)CC1COCCN1   \n",
       "6830                         COCC1(OC)CCCN(CC1)CCC(=O)C   \n",
       "6837    O=C(c1cccc(c1)CNC(=O)c1c(C)ccnc1O)N=c1cc[nH]cc1   \n",
       "6920                    CC(C1CC1NC(=O)c1c(Br)nc(n1C)C)C   \n",
       "\n",
       "                                              input_ids  \\\n",
       "16    [26, 27, 28, 30, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "19    [36, 38, 39, 40, 41, 42, 43, 44, 49, 50, 51, 5...   \n",
       "50    [30, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "59    [33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
       "64    [33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "...                                                 ...   \n",
       "6796  [33, 39, 40, 41, 42, 43, 44, 45, 51, 52, 53, 5...   \n",
       "6807  [15, 18, 27, 28, 32, 38, 39, 40, 41, 42, 43, 4...   \n",
       "6830  [14, 15, 18, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n",
       "6837  [33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "6920  [30, 38, 39, 40, 41, 42, 43, 44, 45, 50, 51, 5...   \n",
       "\n",
       "                                                 labels  \\\n",
       "16    [1233, 224, 283, 11, 70, 20, 284, 21, 70, 20, ...   \n",
       "19    [1233, 224, 276, 20, 263, 11, 38, 260, 50, 12,...   \n",
       "50    [1233, 224, 286, 20, 400, 20, 49, 32, 70, 20, ...   \n",
       "59    [1233, 224, 285, 260, 50, 12, 279, 20, 280, 11...   \n",
       "64    [1233, 224, 50, 32, 38, 11, 70, 20, 280, 21, 7...   \n",
       "...                                                 ...   \n",
       "6796  [1233, 224, 261, 260, 374, 20, 11, 288, 20, 12...   \n",
       "6807  [1233, 224, 263, 11, 38, 11, 38, 260, 50, 12, ...   \n",
       "6830  [1233, 224, 306, 20, 11, 286, 12, 299, 11, 261...   \n",
       "6837  [1233, 224, 50, 32, 38, 11, 70, 20, 338, 11, 7...   \n",
       "6920  [1233, 224, 261, 11, 38, 20, 261, 20, 265, 260...   \n",
       "\n",
       "                                 decoder_attention_mask  \\\n",
       "16    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "19    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "50    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "59    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "64    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "6796  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6807  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6830  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6837  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6920  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                 encoder_attention_mask  \\\n",
       "16    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "19    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "50    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "59    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "64    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "6796  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6807  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6830  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6837  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6920  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                           position_ids  \n",
       "16    [3, 6, 5, 5, 5, 9, 8, 9, 9, 7, 9, 8, 6, 3, 7, ...  \n",
       "19    [3, 0, 6, 5, 7, 8, 8, 6, 3, 1, 5, 4, 6, 6, 5, ...  \n",
       "50    [5, 1, 1, 3, 3, 7, 6, 9, 9, 8, 9, 6, 5, 4, 7, ...  \n",
       "59    [3, 7, 0, 6, 8, 6, 6, 6, 8, 7, 5, 4, 8, 9, 9, ...  \n",
       "64    [3, 2, 2, 1, 3, 7, 5, 6, 4, 6, 6, 6, 0, 2, 4, ...  \n",
       "...                                                 ...  \n",
       "6796  [5, 8, 7, 9, 8, 9, 8, 6, 7, 7, 8, 6, 8, 8, 8, ...  \n",
       "6807  [2, 0, 1, 3, 1, 1, 5, 5, 8, 8, 8, 8, 6, 4, 1, ...  \n",
       "6830  [0, 4, 3, 3, 6, 6, 6, 6, 6, 3, 5, 7, 6, 9, 9, ...  \n",
       "6837  [5, 3, 3, 3, 6, 9, 7, 7, 7, 7, 8, 3, 0, 8, 8, ...  \n",
       "6920  [5, 3, 8, 7, 9, 9, 9, 8, 6, 6, 7, 7, 7, 7, 8, ...  \n",
       "\n",
       "[300 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_data.data.shape)\n",
    "train_data.data.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "557dc9ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 354206720\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29d0ec81",
   "metadata": {},
   "source": [
    "#### Setting run resuming and WandB logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf091621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhajekad\u001b[0m (\u001b[33mmsgc_boys\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhajekad\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch.ssd/ahajek/tmp/zia_training_stuff/wandb/run-20230407_181742-taha2v2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hajekad/BART_for_gcms/runs/taha2v2e' target=\"_blank\">fine-spaceship-144</a></strong> to <a href='https://wandb.ai/hajekad/BART_for_gcms' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hajekad/BART_for_gcms' target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hajekad/BART_for_gcms/runs/taha2v2e' target=\"_blank\">https://wandb.ai/hajekad/BART_for_gcms/runs/taha2v2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Resume training\n",
    "if resume_training:\n",
    "    args.load_checkpoint = load_checkpoint\n",
    "    if args.wandb_resume:\n",
    "        args.wandb_id = resume_wandb_id\n",
    "\n",
    "# Init wandb\n",
    "if args.wandb:\n",
    "    wandb.login()\n",
    "    run = wandb.init(id=args.wandb_id, resume=\"allow\", entity=\"hajekad\", project=\"BART_for_gcms\")\n",
    "    wandb.run.name = args.save_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f790f74",
   "metadata": {},
   "source": [
    "#### Setting training arguments (according to args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba7dffa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=args.save_dir+\"/\"+args.save_name,         # output directory\n",
    "    num_train_epochs=args.num_train_epochs,              # total # of training epochs\n",
    "    weight_decay=args.weight_decay,                                   # strength of weight decay\n",
    "    logging_dir=args.save_dir + './logs',                # directory for storing logs\n",
    "    run_name=args.save_name,\n",
    ")\n",
    "\n",
    "### custom BartSpectroTrainer arguments\n",
    "# new things: eval_subset_size, generate_kwargs, eval_log_predictions_size, eval_tokenizer\n",
    "# change: evaluation_strategy: , eval_steps, logging_steps, save_strategy, save_steps\n",
    "\n",
    "trainer = BartSpectroTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,            # training dataset\n",
    "    eval_dataset=valid_data,             # evaluation dataset\n",
    "    data_collator = SpectroDataCollator(),\n",
    "    eval_subset_size = eval_subset_size,\n",
    "    generate_kwargs = {\"num_return_sequences\": 1, \"top_p\": 0.8, \"do_sample\": True, \"num_beams\": 1},\n",
    "    eval_log_predictions_size = eval_log_predictions_size,\n",
    "    eval_tokenizer = tokenizer,\n",
    "    w_run = run # wandb run\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09129d48",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b0e8ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments:\n",
      "  lr ........................... 5e-05\n",
      "  seed ......................... 42\n",
      "  gradient_accumulation_steps .. 1\n",
      "  batch_size ................... 32\n",
      "  warmup ....................... 500\n",
      "  weight_decay ................. 0.01\n",
      "  n_gpu ........................ 4\n",
      "  fairscale .................... \n",
      "  deepspeed .................... None\n",
      "  num_workers .................. 16\n",
      "  device ....................... cuda\n",
      "  num_train_epochs ............. 200\n",
      "  output_dir ................... ./output\n",
      "  save_dir ..................... ./checkpoints\n",
      "  save_name .................... bart_2023-04-07-18_17_32\n",
      "  load_checkpoint .............. \n",
      "  config_dir ................... ./configs\n",
      "  fp16 ......................... True\n",
      "  max_grad_norm ................ 1.0\n",
      "  train_data_path .............. ./data/trial_set/DEBUG_train.pkl\n",
      "  valid_data_path .............. ./data/trial_set/DEBUG_valid.pkl\n",
      "  log_steps .................... 1\n",
      "  eval_steps ................... 1000\n",
      "  tokenizer_path ............... /storage/brno6/home/ahajek/nic\n",
      "  model_path ................... ./checkpoints/NECO\n",
      "  wandb ........................ True\n",
      "  wandb_resume ................. False\n",
      "  wandb_id ..................... taha2v2e\n",
      "  tensorboard .................. False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch.ssd/ahajek/tmp/.local-PyTorch:22.04-py3.SIF/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/scratch.ssd/ahajek/tmp/.conda/envs/BARTtrain/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 105/8000 00:47 < 1:01:00, 2.16 it/s, Epoch 2.60/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log args to \"args\" file in args.save_dir (right before the training starts, to be the latest)\n",
    "Path(f'{args.save_dir}/{args.save_name}').mkdir(exist_ok=True)\n",
    "arg_log = print_args(args)\n",
    "with open(f'{args.save_dir}/{args.save_name}/args', 'w+') as output_file:\n",
    "    output_file.write(arg_log)\n",
    "\n",
    "if args.load_checkpoint:\n",
    "    trainer.train(args.load_checkpoint)\n",
    "else:\n",
    "    #trainer.evaluate()\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
