{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the BartSpektro architecture\n",
    "we've done some changes to the BART architecture and need to test it to see if it's working properly.\n",
    "\n",
    "### Curent changes to test\n",
    "The model is now having different encoder and decoder sequence lengths. These are modifiable by the encoder_max_position_embeddings (maybe, bcs it doesn't seem to be used anywhere) and decoder_max_position_embeddings (this is at least used to create the position embedding matrix with the correct shape).\n",
    "\n",
    "Also we deleted the max_position_embeddings from the config. It looks like it's not used anywhere now, but it might pop up somewhere.\n",
    "\n",
    "TASK: \n",
    "- if there is sth that controls the sequence length, FIND IT\n",
    "- if there is no actual parameter controling the sequence length in both encoder and decoder, make sure it's true\n",
    "________________________________\n",
    "### Results\n",
    "\n",
    "#### Important architectural parameters\n",
    "- d_model - embedding size\n",
    "- max_log_id - encoder positional embedding dimension\n",
    "- max_mz => encoder embedding dimension\n",
    "- vocab_size => decoder embedding dimension    AND    modeling head dimension\n",
    "- decoder_max_position_embeddings => decoder positional embedding dimension\n",
    "\n",
    "not needed:\n",
    "- encoder_max_position_embeddings\n",
    "- seq_len\n",
    "- ...\n",
    "!! padding is not needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.data_utils import SpectroDataCollator, build_single_datapipe\n",
    "from bart_spektro.modeling_bart_spektro import BartSpektroForConditionalGeneration\n",
    "from bart_spektro.configuration_bart_spektro import BartSpektroConfig\n",
    "from tokenizers import Tokenizer\n",
    "from train_bart import get_spectro_config, build_tokenizer\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"separate_encoder_decoder_embeds\": True,\n",
    "    \"max_mz\": 500,\n",
    "    \"decoder_seq_len\": 100,\n",
    "    \"encoder_layers\": 12,\n",
    "    \"decoder_layers\": 12,\n",
    "    \"encoder_attention_heads\": 16,\n",
    "    \"decoder_attention_heads\": 16,\n",
    "    \"encoder_ffn_dim\": 4096,\n",
    "    \"decoder_ffn_dim\": 4096,\n",
    "}\n",
    "tokenizer = build_tokenizer(\"../tokenizer/bbpe_tokenizer/bart_bbpe_tokenizer_1M_mf10000000.model\")\n",
    "config = get_spectro_config(model_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model is using   T W O   dictionaries - one for encoder, one for decoder.\n"
     ]
    }
   ],
   "source": [
    "model = BartSpektroForConditionalGeneration.from_pretrained(\"../checkpoints/finetune/fearless-wildflower-490_rassp1_neims1_224kPretrain_148k/checkpoint-147476\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xhajek9/gc-ms_bart/notebooks/../utils.data_utils.py:108: UserWarning: SHUFFLE and its buffer_size are IGNORED if either not specified\n",
      "  warnings.warn(\"SHUFFLE and its buffer_size are IGNORED if either not specified\")\n"
     ]
    }
   ],
   "source": [
    "pipe = build_single_datapipe(\"../deprecated/NIST_split_filip_smiUpTo100/valid.jsonl\", True)\n",
    "loader = torch.utils.data.DataLoader(pipe, batch_size=2, collate_fn=SpectroDataCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids                 torch.Size([2, 2])\n",
      "position_ids              torch.Size([2, 2])\n",
      "decoder_attention_mask    torch.Size([2, 6])\n",
      "labels                    torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "enc_tensor_len = 2\n",
    "dec_tensor_len = 6\n",
    "inp[\"input_ids\"] = (torch.cat((inp[\"input_ids\"], inp[\"input_ids\"], inp[\"input_ids\"]), dim=1)[:, :enc_tensor_len]).contiguous()\n",
    "# inp[\"attention_mask\"] = (torch.cat((inp[\"attention_mask\"], inp[\"attention_mask\"], inp[\"attention_mask\"]), dim=1)[:, :enc_tensor_len]).contiguous()\n",
    "inp.pop(\"attention_mask\")\n",
    "inp[\"position_ids\"] = (torch.cat((inp[\"position_ids\"], inp[\"position_ids\"], inp[\"position_ids\"]), dim=1)[:, :enc_tensor_len]).contiguous()\n",
    "inp[\"decoder_attention_mask\"] = (torch.cat((inp[\"decoder_attention_mask\"], inp[\"decoder_attention_mask\"], inp[\"decoder_attention_mask\"]), dim=1)[:, :dec_tensor_len]).contiguous()\n",
    "inp[\"labels\"] = (torch.cat((inp[\"labels\"], inp[\"labels\"], inp[\"labels\"]), dim=1)[:, :dec_tensor_len]).contiguous()\n",
    "\n",
    "for name, t in inp.items():\n",
    "    print(name, + (24 - len(name))*\" \", t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 1240])\n",
      "torch.Size([2, 2, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutputSpectro(loss=tensor(7.2967, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.5537,  0.4766,  0.2917,  ...,  0.3016, -0.5364, -0.4306],\n",
       "         [ 0.9487, -0.1303, -0.1805,  ...,  0.1474, -0.1853, -0.8019],\n",
       "         [ 0.5662,  0.3621, -0.3568,  ...,  0.0613, -0.3281, -0.0750],\n",
       "         [ 0.7645,  0.4561, -0.1900,  ...,  0.2491, -0.7282, -1.3658],\n",
       "         [ 0.0197,  1.1113, -0.7314,  ..., -0.4131, -0.7679, -1.1542],\n",
       "         [ 0.7056,  0.2304, -0.5058,  ...,  0.4372, -0.3797, -0.7306]],\n",
       "\n",
       "        [[-0.0434,  0.3159, -0.0291,  ...,  0.5430, -0.6446, -0.4710],\n",
       "         [-0.2474,  0.8425, -0.3931,  ...,  0.2944, -0.0368,  0.2165],\n",
       "         [-0.4595,  0.6255,  0.6558,  ...,  0.5852, -0.2850, -0.1359],\n",
       "         [ 0.0050,  0.8261,  0.0548,  ...,  1.2372, -0.1472, -0.0383],\n",
       "         [ 0.4785,  0.6806,  0.2442,  ...,  0.5867, -0.8200, -0.0069],\n",
       "         [ 0.3526,  0.3159,  0.0327,  ...,  0.1641, -0.9132,  0.2976]]],\n",
       "       grad_fn=<AddBackward0>), last_hidden_state=None, past_key_values=None, decoder_hidden_states=None, hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.2474,  0.5889,  0.2418,  ...,  1.2432,  1.9238, -0.5073],\n",
       "         [ 0.2126, -1.5438,  0.7448,  ..., -0.4621, -0.1541, -0.7221]],\n",
       "\n",
       "        [[ 0.4446, -0.1383,  0.1461,  ..., -0.2540,  1.0548, -0.4886],\n",
       "         [-0.3238, -1.0163,  0.4750,  ..., -0.9763,  1.1545,  0.1330]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(**inp)\n",
    "print(out.logits.shape)\n",
    "print(out.encoder_last_hidden_state.shape)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartSpektroForConditionalGeneration(\n",
       "  (model): BartSpektroModel(\n",
       "    (encoder): BartSpektroEncoder(\n",
       "      (embed_tokens): Embedding(501, 1024)\n",
       "      (embed_positions): BartSpektroLearnedPositionalEmbedding(42, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartSpektroDecoder(\n",
       "      (embed_tokens): Embedding(267, 1024, padding_idx=2)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(202, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=267, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BARTtrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
