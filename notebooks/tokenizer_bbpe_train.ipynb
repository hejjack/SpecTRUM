{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37aba19b",
   "metadata": {},
   "source": [
    "# SMILES tokenizer\n",
    "The goal of this notebook is to prepare training data and train a suitable SMILES tokenizer. We also look at already trained WordPiece tokenizer from the `deepchem` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c14a2",
   "metadata": {},
   "source": [
    "## WordPiece tokenizer\n",
    "Result: This tokenizer from the `deepchem` library looks to be better for anorganic molecules, has a lot of structures and groups that are not that present in our SMILES strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a11b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
      "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'O', 'c', '1', 'c', 'c', '2', 'c', 'c', 'c', '(', '=', 'O', ')', 'o', 'c', '2', 'c', '(', 'O', '[C@@H]', '2', 'O', '[C@@H]', '(', 'C', 'O', ')', '[C@H]', '(', 'O', ')', '[C@@H]', '(', 'O', ')', '[C@@H]', '2', 'O', ')', 'c', '1', 'O', '1', '6', '2', '1', '5', '1', '3', '2']\n"
     ]
    }
   ],
   "source": [
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "import os\n",
    "\n",
    "vocab_path = 'wp_tokenizer/vocab.txt'\n",
    "tokenizer = SmilesTokenizer(vocab_path)\n",
    "print(tokenizer.tokenize(df.iloc[90][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff876da",
   "metadata": {},
   "source": [
    "## BBPE Tokenizer\n",
    "Next we get inspired by the generative NLP approaches and try to train a BBPE tokenizer. The main advantage of this tokenization technique is, that since the training starts with a vocabulary with all on-byte ASCII char, we can be sure to tokenize any SMILES string without encountering an unknown token (for compatibility reasons we add it anyway though).\n",
    "\n",
    "### Data preparation\n",
    "As training data we take a 1M random slice from the 30M dataset we scraped from ZINC15 database. The SMILES are already deduplicated and canonical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48fd224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data_path = \"../data/datasets/ZINC15/30M_slice/30M.smi\"\n",
    "slice_save_dir = \"../tokenizer/training_data\"\n",
    "\n",
    "def random_slice(size, data_path, slice_save_path):\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = np.array(f.read().splitlines())\n",
    "        choice = np.random.choice(data, size, replace=False)\n",
    "\n",
    "    with open(slice_save_path, 'w') as f:\n",
    "        for item in choice:\n",
    "            f.write(item + \" \")\n",
    "    \n",
    "random_slice(1000000, data_path, slice_save_dir + \"/1M.txt\")\n",
    "random_slice(1000, data_path, slice_save_dir + \"/1K.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d6698",
   "metadata": {},
   "source": [
    "### BBPE tokenizer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37eb2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self, vocab_size=100000, min_frequency=10):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()   # normalization of unicode characters (technicality)\n",
    "        ])\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, path):\n",
    "        trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, show_progress=True, initial_alphabet=ByteLevel.alphabet(), \n",
    "                             special_tokens=[\"<eos>\",\n",
    "                                             \"<unk>\",\n",
    "                                             \"<pad>\",\n",
    "                                             \"<bos>\",\n",
    "                                             \"<neims>\",\n",
    "                                             \"<nist>\",\n",
    "                                             \"<rassp>\",\n",
    "                                             \"<trafo>\",\n",
    "                                             \"<source1>\",\n",
    "                                             \"<source2>\",\n",
    "                                             \"<source3>\",])\n",
    "        self.tokenizer.train(path, trainer)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "470782a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "path = [\"../tokenizer/training_data/1M.txt\"]\n",
    "mfs = [10000000]\n",
    "\n",
    "for min_frequency in mfs:\n",
    "    tokenizer = BPE_token(min_frequency=min_frequency)\n",
    "\n",
    "    # train the tokenizer model\n",
    "    tokenizer.bpe_train(path)\n",
    "\n",
    "    # saving the tokenized data in our specified folder\n",
    "    save_path = '../tokenizer/bbpe_tokenizer/' ####\n",
    "    tokenizer.save_tokenizer(save_path)\n",
    "    tokenizer.tokenizer.save(save_path + f\"bart_bbpe_tokenizer_1M_mf{min_frequency}.model\")    #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d807955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_bbpe_tokenizer_1M_mf3\n",
      "- max vocab_size 100000\n",
      "- min_frequency 3\n",
      "- final vocab size 1827 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf10\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10\n",
      "- final vocab size 1286 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf30\n",
      "- max vocab_size 100000\n",
      "- min_frequency 30\n",
      "- final vocab size 985 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf50\n",
      "- max vocab_size 100000\n",
      "- min_frequency 50\n",
      "- final vocab size 887 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf100\n",
      "- max vocab_size 100000\n",
      "- min_frequency 100\n",
      "- final vocab size 780 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf500\n",
      "- max vocab_size 100000\n",
      "- min_frequency 500\n",
      "- final vocab size 583 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf1000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 1000\n",
      "- final vocab size 523 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf3000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 3000\n",
      "- final vocab size 427 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf5000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 5000\n",
      "- final vocab size 401 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf6000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 6000\n",
      "- final vocab size 395 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf10000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10000\n",
      "- final vocab size 367 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf50000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 50000\n",
      "- final vocab size 302 (including 11 special tokens)\n",
      "\n",
      "bart_bbpe_tokenizer_1M_mf10000000\n",
      "- max vocab_size 100000\n",
      "- min_frequency 10000000\n",
      "- final vocab size 267 (including 11 special tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "save_path = '../tokenizer/bbpe_tokenizer/'\n",
    "all_mfs = [3, 10, 30, 50, 100, 500, 1000, 3000, 5000, 6000, 10000, 50000, 10000000]\n",
    "\n",
    "for min_frequency in all_mfs:\n",
    "    # loading the saved tokenizer\n",
    "    tokenizer = Tokenizer.from_file(save_path + f\"/bart_bbpe_tokenizer_1M_mf{min_frequency}.model\")\n",
    "\n",
    "    print(f\"bart_bbpe_tokenizer_1M_mf{min_frequency}\\n\" +\n",
    "          \"- max vocab_size 100000\\n\" + \n",
    "          f\"- min_frequency {min_frequency}\\n\" +\n",
    "          f\"- final vocab size {len(tokenizer.get_vocab())} (including 11 special tokens)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e29bb06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1e7'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1e7\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '1e7'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2378752e",
   "metadata": {},
   "source": [
    "### Try the BBPE out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0e31df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# vocab = \"./tokenizer/bbpe_tokenizer/vocab.json\"\n",
    "# merges = \"./tokenizer/bbpe_tokenizer//merges.txt\"\n",
    "tok = \"./tokenizer/bbpe_tokenizer/bart_bbpe_tokenizer_1M_mf3.model\"\n",
    "tokenizer = Tokenizer.from_file(tok)\n",
    "# special_tokens_dict = {\"bos_token\": \"<bos>\", \"unk_token\": \"<unk>\", \"eos_token\": \"<eos>\", \"sep_token\": \"<sep>\"}\n",
    "# special_tokens_dict = [\"<sep>\"]\n",
    "\n",
    "\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# And then encode:\n",
    "encoded = tokenizer.encode(\"Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ac54d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġ', 'CS', '(=', 'O', ')(=', 'O', ')', 'NCC', '(=', 'O', ')', 'N', '[', 'C', '@', 'H', ']', '1', 'COCC', '[', 'C', '@@', 'H', ']', '1', 'Oc', '1', 'ccc', '(', 'C', '(', 'N', ')=', 'O', ')', 'cc', '1']\n"
     ]
    }
   ],
   "source": [
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4276e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62814d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = tokenizer.token_to_id(\"<bos>\")\n",
    "et = tokenizer.token_to_id(\"<eos>\")\n",
    "tok_smiles = [bt] + tokenizer.encode(df.smiles[0]).ids #+ [et] + (200-2-len(tokenizer.encode(df.smiles[0]))) * [pt]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dcd4fb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1nn(C)c(C)c1S(=O)(=O)N1CCN2C(=O)CN(C)C(=O)[C@H]2C1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[224,\n",
       " 278,\n",
       " 20,\n",
       " 282,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 70,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 70,\n",
       " 20,\n",
       " 54,\n",
       " 260,\n",
       " 50,\n",
       " 270,\n",
       " 50,\n",
       " 12,\n",
       " 49,\n",
       " 20,\n",
       " 267,\n",
       " 21,\n",
       " 38,\n",
       " 260,\n",
       " 50,\n",
       " 12,\n",
       " 266,\n",
       " 11,\n",
       " 38,\n",
       " 12,\n",
       " 38,\n",
       " 260,\n",
       " 50,\n",
       " 263,\n",
       " 38,\n",
       " 35,\n",
       " 43,\n",
       " 64,\n",
       " 21,\n",
       " 38,\n",
       " 20]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.smiles[0])\n",
    "tokenizer.encode(df.smiles[0]).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0147c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destereo_smiles</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>decoder_input_ids</th>\n",
       "      <th>encoder_attention_mask</th>\n",
       "      <th>decoder_attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>position_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...</td>\n",
       "      <td>[15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...</td>\n",
       "      <td>[3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1</td>\n",
       "      <td>[17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...</td>\n",
       "      <td>[1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...</td>\n",
       "      <td>[30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...</td>\n",
       "      <td>[3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...</td>\n",
       "      <td>[2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C</td>\n",
       "      <td>[33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...</td>\n",
       "      <td>[2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C</td>\n",
       "      <td>[33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...</td>\n",
       "      <td>[4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>OCCOCCNC(=O)C1CC(C(C1)O)NC(=O)c1nsnc1C</td>\n",
       "      <td>[19, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 4...</td>\n",
       "      <td>[3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...</td>\n",
       "      <td>[3, 4, 3, 3, 5, 3, 4, 7, 6, 9, 9, 9, 9, 9, 7, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>COc1ccc(cc1C(=O)NC1COC2C1OCC2O)S(=O)(=O)N</td>\n",
       "      <td>[26, 31, 33, 36, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...</td>\n",
       "      <td>[0, 7, 2, 4, 1, 7, 4, 8, 8, 9, 8, 8, 7, 7, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>O=C(Cn1ncn(c1=O)C)OCCCOC(=O)Cn1ncn(c1=O)C</td>\n",
       "      <td>[29, 30, 31, 39, 40, 41, 42, 43, 44, 45, 52, 5...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...</td>\n",
       "      <td>[3, 1, 3, 4, 6, 4, 8, 5, 6, 0, 2, 6, 6, 8, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>OC(=O)CN(C(=O)C)CC1OCCN(C1)C(=O)CN1CSCC1=O</td>\n",
       "      <td>[30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 4...</td>\n",
       "      <td>[3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...</td>\n",
       "      <td>[3, 0, 0, 3, 2, 1, 4, 1, 6, 6, 8, 9, 9, 9, 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>O=C(N1CCN(C2C1CS(=O)(=O)C2)S(=O)(=O)C)Cc1cccnc1</td>\n",
       "      <td>[27, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...</td>\n",
       "      <td>[4, 3, 1, 1, 3, 8, 6, 8, 8, 7, 5, 1, 4, 6, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       destereo_smiles  \\\n",
       "629  COCCN1C(=O)C(=O)N(C1=O)CC(=O)c1c(N)n(C)c(=O)n(...   \n",
       "338       O=C(NC1CCS(=O)(=O)C1)CCC(=O)NC1CCS(=O)(=O)C1   \n",
       "620  CN(C(=O)c1c(C)nc2n1CCN(C2)C(=O)c1cc(=O)n(c(=O)...   \n",
       "396   Cn1ncc(c1)S(=O)(=O)N1CCN(CC1)S(=O)(=O)c1cnn(c1)C   \n",
       "251            O=C(C1CNCC(C1)C(=O)N1CCOCC1)NCCc1nncn1C   \n",
       "..                                                 ...   \n",
       "308             OCCOCCNC(=O)C1CC(C(C1)O)NC(=O)c1nsnc1C   \n",
       "674          COc1ccc(cc1C(=O)NC1COC2C1OCC2O)S(=O)(=O)N   \n",
       "631          O=C(Cn1ncn(c1=O)C)OCCCOC(=O)Cn1ncn(c1=O)C   \n",
       "526         OC(=O)CN(C(=O)C)CC1OCCN(C1)C(=O)CN1CSCC1=O   \n",
       "121    O=C(N1CCN(C2C1CS(=O)(=O)C2)S(=O)(=O)C)Cc1cccnc1   \n",
       "\n",
       "                                             input_ids  \\\n",
       "629  [15, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 4...   \n",
       "338  [17, 18, 26, 27, 28, 29, 30, 31, 32, 33, 34, 3...   \n",
       "620  [30, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 4...   \n",
       "396  [33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "251  [33, 36, 39, 40, 41, 42, 43, 44, 45, 51, 52, 5...   \n",
       "..                                                 ...   \n",
       "308  [19, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 4...   \n",
       "674  [26, 31, 33, 36, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "631  [29, 30, 31, 39, 40, 41, 42, 43, 44, 45, 52, 5...   \n",
       "526  [30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 4...   \n",
       "121  [27, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "\n",
       "                                     decoder_input_ids  \\\n",
       "629  [3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...   \n",
       "338  [3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...   \n",
       "620  [3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...   \n",
       "396  [3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...   \n",
       "251  [3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...   \n",
       "..                                                 ...   \n",
       "308  [3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...   \n",
       "674  [3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...   \n",
       "631  [3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...   \n",
       "526  [3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...   \n",
       "121  [3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...   \n",
       "\n",
       "                                encoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "308  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "674  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "631  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "526  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                decoder_attention_mask  \\\n",
       "629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "338  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "620  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "396  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "251  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "308  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "674  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "631  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "526  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "121  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                labels  \\\n",
       "629  [3, 224, 325, 20, 38, 260, 50, 12, 38, 260, 50...   \n",
       "338  [3, 224, 50, 32, 38, 11, 272, 20, 290, 260, 50...   \n",
       "620  [3, 224, 266, 11, 38, 260, 50, 12, 70, 20, 70,...   \n",
       "396  [3, 224, 275, 20, 310, 11, 70, 20, 12, 54, 260...   \n",
       "251  [3, 224, 50, 32, 38, 11, 38, 20, 266, 261, 11,...   \n",
       "..                                                 ...   \n",
       "308  [3, 224, 372, 291, 260, 50, 12, 38, 20, 261, 1...   \n",
       "674  [3, 224, 331, 20, 274, 11, 264, 20, 38, 260, 5...   \n",
       "631  [3, 224, 50, 32, 38, 11, 275, 20, 303, 11, 70,...   \n",
       "526  [3, 224, 280, 260, 50, 12, 266, 11, 38, 260, 5...   \n",
       "121  [3, 224, 50, 32, 38, 11, 49, 20, 267, 11, 38, ...   \n",
       "\n",
       "                                          position_ids  \n",
       "629  [3, 1, 2, 5, 5, 4, 4, 5, 6, 7, 8, 6, 7, 9, 6, ...  \n",
       "338  [1, 1, 3, 6, 6, 7, 4, 5, 2, 1, 1, 2, 4, 7, 6, ...  \n",
       "620  [2, 4, 0, 4, 7, 7, 7, 9, 7, 8, 6, 4, 4, 6, 6, ...  \n",
       "396  [2, 3, 3, 1, 5, 6, 6, 6, 8, 6, 5, 3, 4, 4, 4, ...  \n",
       "251  [4, 1, 8, 7, 9, 9, 8, 9, 2, 6, 8, 8, 8, 9, 9, ...  \n",
       "..                                                 ...  \n",
       "308  [3, 4, 3, 3, 5, 3, 4, 7, 6, 9, 9, 9, 9, 9, 7, ...  \n",
       "674  [0, 7, 2, 4, 1, 7, 4, 8, 8, 9, 8, 8, 7, 7, 6, ...  \n",
       "631  [3, 1, 3, 4, 6, 4, 8, 5, 6, 0, 2, 6, 6, 8, 8, ...  \n",
       "526  [3, 0, 0, 3, 2, 1, 4, 1, 6, 6, 8, 9, 9, 9, 8, ...  \n",
       "121  [4, 3, 1, 1, 3, 8, 6, 8, 8, 7, 5, 1, 4, 6, 2, ...  \n",
       "\n",
       "[469 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"data/trial_set/1K_bbpe_bart_prepared_data_train.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
