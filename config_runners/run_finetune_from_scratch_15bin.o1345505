/storage/brno2/home/ahajek/miniconda3/envs/BARTtrainH100
CUDA_VISIBLE_DEVICES set to: GPU-fcdeab69-98e1-0e6c-8a32-86db99078d6f
device: cuda
_CudaDeviceProperties(name='NVIDIA H100 80GB HBM3', major=9, minor=0, total_memory=81041MB, multi_processor_count=132)

Using   A U T O M A T I C   batch size
relies heavily on POSSIBLE_TO_FIT_ON_GPU hardcoded constant
> it works well for BART base
> it is computed from the effective BS)
> per device BS and GAS are overwritten
> it automatically distinguishes between 40GB and 80GB GPUs

AUTO BS
> GAS: 1
> effective train batch size: 128
> effective eval batch size: 128
> train batch size per GPU: 128
> eval batch size per GPU: 64
> gradient accumulation steps: 1
> eval accumulation steps: 1
> num of GPUs: 1
> GPU RAM: 84978434048
Metric for choosing best model: eval_nist_morgan_tanimoto_simil
TOKENIZER vocab size: 267
Using  O N - T H E - F L Y  PREPROCESSING
shuffling ../data/datasets/NIST/NIST_split_filip/train.jsonl with buffer_size=100000
shuffling ../data/datasets/4_8M/neims_gen/train.jsonl with buffer_size=100000
shuffling ../data/datasets/4_8M/rassp_gen/train.jsonl with buffer_size=100000
Number of non-zero weight datasets:  1
Loading model...
WARNING: This model is using   T W O   dictionaries - one for encoder, one for decoder.
Number of trained parameters: 353997824/353997824 = 100.00%
Run name: cosmic-eon-452_from_scratch_lb1_6_15shift
save path: ../checkpoints/finetune/cosmic-eon-452_from_scratch_lb1_6_15shift
