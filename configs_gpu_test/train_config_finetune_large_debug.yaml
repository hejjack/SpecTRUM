data_args:
  show_raw_preds: True
  buffer_size: 100000 # for shuffling in dataloader
  data_seed: 42
  shuffle_train: True
  dataset_for_choosing_best_model: "nist"  # !!!!!!!!!!!!
  datasets: 
    nist:    # 208908 + 26000
      train_path: "../data/datasets/NIST/NIST_split_filip/train.jsonl"
      valid_path: "../data/datasets/NIST/NIST_split_filip/valid.jsonl"
      weight: 1.0
      limit_train_split: null  # null for full; limit of head from valid set chosen for validation split
      limit_val_split: 128     # null for full; limit of head from valid set chosen for validation split
      limit_example_split: 128  # null for full; limit of head chosen for visualized (generated)
      source_token: "<nist>"
    NEIMS: # NEIMS only for evaluation tracking purposes
      train_path: "../data/datasets/4_8M/neims_gen/train.jsonl"
      valid_path: "../data/datasets/4_8M/neims_gen/valid.jsonl"
      weight: 0.0
      limit_train_split: null  # null for full; limit of head from valid set chosen for validation split
      limit_val_split: 0     # null for full; limit of head from valid set chosen for validation split
      limit_example_split: 0  # null for full; limit of head chosen for visualized (generated)
      source_token: "<neims>"
    RASSP: # only for evaluation tracking purposes
      train_path: "../data/datasets/4_8M/rassp_gen/train.jsonl"
      valid_path: "../data/datasets/4_8M/rassp_gen/valid.jsonl"
      weight: 0.0
      limit_train_split: null  # null for full; limit of head from valid set chosen for validation split
      limit_val_split: 0     # null for full; limit of head from valid set chosen for validation split
      limit_example_split: 0  # null for full; limit of head chosen for visualized (generated)
      source_token: "<rassp>"


model_args:
  tokenizer_path: "../tokenizer/bbpe_tokenizer/bart_bbpe_tokenizer_1M_mf3.model"
  separate_encoder_decoder_embeds: True
  max_mz: 500 # NO,BACHA (jaka je ta max value? uz jsou ty indexy opraveny?)...    # controls vocab size for encoder, if separate_encoder_decoder_embeds=True
  decoder_seq_len: 200
  encoder_layers: 30 # 24
  encoder_ffn_dim: 8192
  encoder_attention_heads: 32
  decoder_layers: 30 # 24
  decoder_ffn_dim: 8192
  decoder_attention_heads: 32


example_generation_args:
  "top_k": null
  "top_p": null
  "do_sample": True
  "num_beams": 5
  "temperature": null
  "penalty_alpha": null
  "num_return_sequences": 1  # NEVER TOUCH THIS
  "length_penalty": 1.0 # (use for beam search otherwise set to 0)  


hf_training_args:
  do_train: True
  do_eval: True

  max_steps: 74_000
  optim: "adamw_torch"
  warmup_steps: 10

  learning_rate: 0.00005
  auto_bs: False   # works well for BART base, computes GPU bs and gas based on effective bs, num GPUs and card memory
  effective_train_batch_size: 256
  effective_eval_batch_size: 256
  per_device_train_batch_size: 32  # max for large model
  per_device_eval_batch_size: 32  # max for large model?
  gradient_accumulation_steps: 8
  eval_accumulation_steps: 8

  dataloader_num_workers: 6
  dataloader_drop_last: False

  report_to: "wandb" # "wandb"
  remove_unused_columns: False # NEVER TOUCH THIS
  logging_steps: 5

  fp16: True
  seed: 42

  predict_with_generate: True # NEVER TOUCH THIS
  generation_num_beams: 1
  generation_max_length: 200  # NEVER TOUCH THIS
  evaluation_strategy: "steps"
  eval_steps: 100 ### make it match (4896 ~ approx 3*full NIST dataset)

  save_steps: 5500 ### make it match (4896 ~ approx 3*full NIST dataset)
  save_strategy: "steps"
  save_total_limit: 2

  load_best_model_at_end: True
  metric_for_best_model: "eval_morgan_tanimoto_simil"
  greater_is_better: True

