# this script is used to preprocess the msp data (e.g. generated by RASSP) in parallel
# the input is a folder containing the msp files
# the output is a folder containing the preprocessed jsonl files (optionaly concatenated into one file)
# the jsonl can be used directly to feed to BartSpektro
from __future__ import annotations

import sys
sys.path.append("..")

import typer
import numpy as np
import multiprocessing as mp
from tqdm import tqdm
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from pathlib import Path
from spectra_process_utils import msp_file_to_jsonl
from bart_spektro.selfies_tokenizer import SelfiesTokenizer, hardcode_build_selfies_tokenizer
from train_bart import build_tokenizer

app = typer.Typer()

def msp_files_to_jsonl_files(process_id, files, 
                             output_dir, 
                             tokenizer: PreTrainedTokenizerFast | SelfiesTokenizer, 
                             source_token, 
                             max_cumsum, 
                             keep_spectra):
    print(f"process {process_id} STARTED")
    for file in tqdm(files): 
        jsonl_file = output_dir / f"{file.stem}.jsonl"
        msp_file_to_jsonl(file,
                          tokenizer=tokenizer,
                          source_token=source_token,
                          path_jsonl=jsonl_file,
                          max_cumsum=max_cumsum,
                          keep_spectra=keep_spectra)
    print(f"process {process_id} DONE")


@app.command()
def main(input_dir: Path = typer.Option(..., help="input directory containing the msp files"), 
         output_dir: Path = typer.Option(..., help="output directory to store the preprocessed jsonl files"), 
         source_token: str = typer.Option("<rassp>", help="source token to use for the jsonl files"),
         max_cumsum: float = typer.Option(0.995, help="maximum number of tokens in the summary"),
         keep_spectra: bool = typer.Option(False, help="keep the mz/intensity values in the jsonl files (for evaluation)"),
         mol_representation: str = typer.Option("smiles", help="molecular representation to use for the jsonl files (smiles/selfies)"),
         num_processes: int = typer.Option(1, help="number of processes to use for parallelization"),
         concat: bool = typer.Option(False, help="concatenate the preprocessed jsonl files into one file"),
         clean: bool = typer.Option(False, help="delete the preprocessed jsonl files after concatenation")):
    """
    Preprocess the msp data (e.g. generated by RASSP) in parallel.
    The input is a folder containing the msp files.
    The output is a folder containing the preprocessed jsonl files (optionaly concatenated into one file).
    The jsonl can be used directly to feed to BartSpektro.
    """

    if mol_representation == "smiles":
        tokenizer = build_tokenizer(tokenizer_path="tokenizer/bbpe_tokenizer/bart_bbpe_1M_tokenizer.model")
    elif mol_representation == "selfies":
        tokenizer = hardcode_build_selfies_tokenizer()
    else:
        raise ValueError("mol_representation must be either smiles or selfies")

    print("Number of processes: ", num_processes)
    if not output_dir.exists():
        output_dir.mkdir(parents=True)
    if concat:
        output_file = output_dir / "concat.jsonl"
        if output_file.exists():
            output_file.unlink()
    files = list(input_dir.glob("*.msp"))
    if len(files)%num_processes != 0:
        # pad_with None to divisable length
        print("files len is not divisable by num_processes. Padding with None to divisable length")
        pad_len = num_processes - (len(files)%num_processes)
        files += [None] * pad_len
    
    grouped_files = np.array(files).reshape(num_processes, -1)
    processes = {}
    for i in range(num_processes):
        processes[f"process{i}"] = mp.Process(target=msp_files_to_jsonl_files, args=(i, 
                                                                                     grouped_files[i], 
                                                                                     output_dir, 
                                                                                     tokenizer,
                                                                                     source_token, 
                                                                                     max_cumsum, 
                                                                                     keep_spectra))
    for process in processes.values():
        process.start()
    for process in processes.values():
        process.join()
    print("All DONE")

    if concat:
        print("Concatenating the jsonl files into one file")
        jsonl_files = list(output_dir.glob("*.jsonl"))
        with open(output_dir / "all.jsonl", "w+", encoding="utf-8") as outfile:
            for jsonl_file in jsonl_files:
                with open(jsonl_file, "r+", encoding="utf-8") as infile:
                    for line in infile:
                        outfile.write(line)
        if clean:
            for jsonl_file in jsonl_files:
                jsonl_file.unlink()
        print("Concatenation DONE")


if __name__ == "__main__":
    app()